% \chapter{关于本模板}

% 本模板根据浙江大学研究生院编写的《浙江大学研究生学位论文编写规则》~\cite{zjugradthesisrules}，
% 在原有的 zjuthesis 模板~\cite{zjuthesis}基础上开发而来。

% 本模板的本科生版本\cite{zjuthesisrules}得到了浙江大学本科生院老师的支持与审核，
% 已经在本科生院网上公示。
% 但当前的研究生版本并未经过研究生院老师的审核，
% 同学们使用时要注意对照模板与要求，
% 切不可盲目使用。

% 作者本人并未编写过浙江大学研究生毕业论文，
% 所以不清楚具体要求。
% 如果有热心同学愿意帮忙，
% 可以替我联系相关老师，我会配合审核并修改代码。

% \section{Overleaf 使用注意事项}

% 如果你在Overleaf上编译本模板，请注意如下事项：

% % \begin{itemize}
% %     \item 删除根目录的 ``.latexmkrc'' 文件，否则编译失败且不报任何错误
% %     \item 字体有产权所以本模板不能附带字体，请务必手动上传字体文件，并在各个专业模板下手动指定字体。
% %         具体方法参照 GitHub 主页的说明。
% %     \item 当前的Overleaf默认使用TexLive 2017进行编译，但一些伪粗体复制乱码的问题需要TexLive 2019版本来解决。
% %         所以各位同学可以在Overleaf上编写论文时务必切换到TexLive 2019或更新版本来编译，以免产生查重相关问题。
% %         具体说明参照 GitHub 主页。
% % \end{itemize}


% \section{节标题}

% 我们可以用includegraphics来插入现有的jpg等格式的图片，
% 如\autoref{fig:zju-logo}所示。

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=.3\linewidth]{logo/zju}
%     \caption{\label{fig:zju-logo}浙江大学LOGO}
% \end{figure}


% \subsection{小节标题}


% \par 如\autoref{tab:sample}所示，这是一张自动调节列宽的表格。

% \begin{table}[htbp]
%     \caption{\label{tab:sample}自动调节列宽的表格}
%     \begin{tabularx}{\linewidth}{c|X<{\centering}}
%         \hline
%         第一列 & 第二列 \\ \hline
%         xxx & xxx \\ \hline
%         xxx & xxx \\ \hline
%         xxx & xxx \\ \hline
%     \end{tabularx}
% \end{table}


% \par 如\autoref{equ:sample}，这是一个公式

% \begin{equation}
%     \label{equ:sample}
%     A=\overbrace{(a+b+c)+\underbrace{i(d+e+f)}_{\text{虚数}}}^{\text{复数}}
% \end{equation}

% \chapter{另一章}


% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=.3\linewidth]{example-image-a}
%     \caption{\label{fig:fig-placeholder}图片占位符}
% \end{figure}

\chapter{第一章~~绪论}
\section{研究背景}

\subsection{数据流通场景}
伴随着物联网、移动互联网等信息技术的飞速发展，数据的规模和种类呈现出爆炸性增长，
涵盖了人类社会各个领域的方方面面，包括社交媒体、金融、医疗、能源、交通等。
根据国际数据公司（International Data Corporation，IDC）的调查，每天有超过 50 亿人参与生成数据。
预计到2025年，这一数字将上升到60亿，相当于全球人口的75\%，全球将产生超过135ZB的数据。
随着网络基础设施的建设和提速降费政策的实施，中国正逐渐成为全球数据量最高的国家，并有望在2025年将达到全球第一\cite{IDC2018-1}。
% 如今，基于数据分析提供的服务已经渗透到社会生产的方方面面，在降低生产成本的同时也便利了人们的生活。
随着数据经济不断发展和人工智能技术深度应用，数据日益成为数据经济时代下新的重要战略资源和生产要素，
已快速融入生产、分配、流通、消费和社会服务管理等各个环节，
深刻改变着生产方式、生活方式和社会治理方式\cite{庄子银2022充分发挥数据新型生产要素作用}。
例如，基于数据驱动的“城市大脑”已经在杭州，衢州，苏州，北京等城市先后落地，在当地交通治理，环境保护，城市精细化管理等创新实践提供了有力支撑\cite{叶晓楠2019}；
全球能源互联网美国研究院等机构基于深度强化学习技术提出了具有在线学习功能的电网自主优化控制和决第框架，即“电网脑”系统。
该系统可通过离线和在线数据不断积累经验，从而在亚秒时间内根据电网实时量测数据给出调度控制指令及预期控制效果\cite{王之伟2020基于深度强化学习的电网自主控制与决策技术}。

% 数据天性就是流通的，在安全条件下的开放、共享和利用，能够极大地提高资本、技术、知识等其他生产要素的利用效率和结合对接，有效地推动管理、组织、制度和技术的不断创新。
% 而数据泄露的风险是阻滞数据流通最主要的障碍之一，其所造成的后果导致各方对数据的上传流通采取过于保守的态度，因噎废食。
数据要素是根据特定生产需求汇聚、整理、加工而成的计算机数据及其衍生形态\cite{信通院2023数据要素白皮书}。
在确保安全的前提下开放、共享和利用数据，可以大幅提高资本、技术、知识等其他生产要素的利用效率和相互配合，有效地推动管理、组织、制度和技术的不断创新。
然而，数据泄露的风险是阻碍数据流通的主要障碍之一，
往往使数据流通场景中的参与方对数据的共享变得过于保守，
因此无法充分利用数据\cite{陈晓建2019大数据安全流通机制的研究与应用}。
文献{\rm\parencite{信通院2023数据要素白皮书}}通过分析数据流通过程，将数据要素的主要表现形态总结如\autoref{fig:数据要素主要表现形态}所示。
\begin{figure}[h]
    \centering
    \includegraphics[width=0.70\hsize]{figure/others/数据要素主要表现形态.pdf}
    \caption{数据要素主要表现形态} 
    \label{fig:数据要素主要表现形态}
\end{figure}
原始数据一般是指用户记录的规模较小并且价值密度也比较低的数据。
接着，用户上传的数据被收集、整理之后形成标准化数据集，可以为后续业务提供支撑的能力。
最后，数据服务商基于标准化数据集进行分析、处理、加工等操作，形成具有商品和服务属性的数据模型。

% 人工智能（Artificial Intelligence，AI）相关理论和技术不断取得新的突破，使得大规模数据的价值被进一步开发并成为新型生产要素，
% 已快速融入生产、分配、流通、消费和社会服务管理等各个环节，
% 深刻改变着生产方式、生活方式和社会治理方式\cite{庄子银2022充分发挥数据新型生产要素作用}。
% 例如，在图像处理领域，1990年发布的MINIST数据集只包含了6万张图片，机器学习模型只能完成基础的手写数字识别任务\cite{DBLP:journals/pieee/LeCunBBH98}。
% 而到了2009年，随着李飞飞等发布了含有150万张图片的ImageNet数据集\cite{DBLP:conf/cvpr/DengDSLL009}，图像处理相关的研究便出现了井喷式发展，相关应用覆盖人脸追踪\cite{DBLP:conf/cvpr/AndrilukaRS08}、图像语义分割\cite{DBLP:conf/cvpr/LongSD15, DBLP:conf/cvpr/ZhangQY0M18}、图像增强\cite{DBLP:conf/iccv/IgnatovKTVG17, DBLP:journals/tip/RenLMXXCDY19}等领域。
% 近日，人工智能研究公司OpenAI发布了新一代GPT-4模型\cite{2023arXiv230308774O}，其在训练阶段所使用的文本已经超过10万亿个单词。
% 凭借着其出色的语言流畅度与逻辑完整度，GPT-4模型被视为未来主流的AI应用之一。
%首先说模型需要的数据量越来越大，数据在参与生产过程中需要在多个主体之间流通，在本段的最后引出数据的主体
% 党的十九届四中全会《决定》明确提出：“健全劳动、资本、土地、知识、技术、管理、数据等生产要素由市场评价贡献、按贡献决定报酬的机制”，首次将数据增列为生产要素。

% 数据要素主要表现形态如\autoref{}所示。 图2
% 其中，结合图7把对象和数据主要表现形态结合
% \subsection{数据权利主体}
% 在上述数据流通的过程中，参与的主要数据权利主体可以归纳为数据生产者、数据加工者和数据消费者。

% \subsubsection{数据生产者}
% \subsubsection{数据加工者}
% \subsubsection{数据消费者}


\subsection{数据要素安全}
结合上述三种数据要素主要形态，我们按照数据流通过程依次介绍本文关注的数据安全问题的背景和内容。

\mypara{原始数据}
数据不能凭空产生，
其产生过程一般直接或者间接与人相关，
所以原始数据中包含着大量的个体敏感数据。
例如，很多智能手机服务会不断地采集个人的位置信息，
医疗系统会记录病人的症状以及诊断数据。
这些数据在收集、分析过程中如果不加以保护，
将不可避免的泄露个体的敏感数据，
影响个体参与数据流通的积极性并可能造成恶劣社会影响。
近年来，个体敏感数据大批量泄露的事件在世界范围内多次发生。
例如，在2006年，美国在线（America Online，AOL）公布了650,000个用户的搜索日志，
采用了随机替换用户身份的方法来保护用户隐私。
然而，通过与网上已公开的用户信息进行对照，两名记者仍然能够推断出一些用户的真实身份。
该事件引起了广泛关注和用户诉讼，导致AOL首席技术官辞职并向受影响的用户道歉。
2014年，纽约市出租车和私人车辆委员会公布了一份删去乘客身份的数据集，
其中包含了超过1.7亿次出租车行程的记录。
后来，研究人员利用该数据集中的信息推断出特定人员的出行记录和行踪，引发了个人隐私保护的争议和讨论。
2021年4月，超过5.8亿LinkedIn用户的个人数据被泄露，包括电子邮件地址、电话号码、个人和职业资料等。这是目前为止LinkedIn历史上最大规模的数据泄露事件。

为此，各国政府陆续出台隐私保护法案保护民众的数据隐私。
美国国会于2012年通过了《消费者隐私权利法案》，
该法案主要用于保护消费者的个人信息不被滥用、泄露或共享。
该法案强化了消费者对自己信息的控制权，
要求企业必须向消费者透露它们如何收集和使用个人信息，
并且提供消费者选择不让其个人信息被共享的选项。
而欧洲于2018年开始施行的《通用数据保护条例》（General Data Protection Regulation，GDPR）被称为史上最严格的数据保护法案之一。
该法案规定企业必须获得用户明确的同意才能收集、处理和使用用户的个人数据。
用户有权随时访问自己的个人数据，并有权要求企业删除这些数据。
如果企业违反了该法案，可能会被处以高达全球营收的4\%的罚款。
我国于2021年正式实施了《中华人民共和国个人信息保护法》，
该法旨在加强对个人信息的保护，
规范个人信息处理的行为，
要求个人信息处理主体应当建立健全个人信息安全管理制度，
采取必要的技术措施和管理措施保障个人信息安全。

\mypara{标准化数据集}
随着机器学习算法、数据处理能力和云计算基础设施的不断提升，
人工智能在软件领域的采用和商业化显著增加\cite{Gartner2022}。
% 随着机器学习算法、数据处理能力和云计算基础设施的不断提升，人工智能已经变得更加可行和实惠，适用于各种规模的企业。
% 企业正在使用人工智能来自动化和优化各种业务流程，例如客户服务、销售、营销、供应链管理和产品开发。人工智能驱动的工具和应用程序也正在为特定行业开发，例如医疗保健、金融和制造业，以提高效率、准确性和决策能力。
为特定的机器学习应用创建出色的训练集是一项耗时耗财的工作。
例如，在图片分类领域，开源数据集ImageNet包含1000个类别的1400多万张图像\cite{}。
在场景理解领域，谷歌公司开源了Open Images数据集。
该数据集包含900万张图像和多种语义注释，其中包括图像级别标签、物体边界框、物体分割掩码、视觉关系和本地化叙述等。
除了这些公开数据集，还有许多机构收集整理的大规模私有数据集。
% GPT-3使用的训练数据集也十分庞大，基于包含近1万亿单词量的CommonCrawl数据集、网络文本、数据、维基百科等数据，数据量达到了45TB。
例如，OpenAI在2022年发布大语言模型ChatGPT，
其训练阶段所使用的私有数据集就包含近1万亿单词量的CommonCrawl数据集、网络文本、数据、维基百科等数据，
数据量达到了45TB\cite{DBLP:conf/nips/BrownMRSKDNSSAA20}。
尽管在典型环境中，侵权人无法直接访问这些私有训练集，
但是如果通过某些方式获得了访问权限，便可以轻而易举地恶意复制、分发训练集。
% 标准化数据作为一种虚拟物品，低成本复制特点导致其面临着较大的产权侵犯的风险。

为了保护数据处理者的数据产权，我国在2022年对外公布了构建数据基础制度体系的《数据二十条》，
提出建立保障权益、合规使用的数据产权制度，
现阶段以数据处理者为保护主体，
以经过一定规则处理且处于未公开状态的数据集作为保护对象。
% 国家知识产权局副局长胡文辉16日在国新办举行的新闻发布会上介绍，
% 国家知识产权局针对基础制度内容开展了研究，
除私有数据集外，确保公开可用数据集的使用合法，特别是用于商业目的，同样十分重要。
目前，公开数据集的产权保护主要依靠与数据集相关联的许可证。
与开源软件许可证类似，数据集许可证概述了用户享有的权利以及用户必须履行的义务。
从人工智能软件开发的角度来看，特定数据集许可证决定了数据集是否可以用于构建商业应用程序并提供盈利服务。
与在商业软件中使用开源软件类似，为确保数据集的合法商业使用，需要满足数据集许可证合规性。
因此，英国和加拿大政府规定除非产权持有人明确许可，否则受产权保护的数据只能用于非商业用途\cite{Infringement2022}。
欧盟于2019年发布了新版的《数字化单一市场产权指令》，明确规定仅在非商业用途下才能不经产权持有人的明确许可使用受产权保护的材料\cite{triaille2014study}。

\mypara{数据模型}
目前人工智能技术供应商一般通过机器学习即服务（Machine Learning as a Service，MLaaS）的方式向用户提供服务。
MLaaS是一种云计算服务模型，为客户提供基于云端的机器学习模型。
人工智能技术供应商在云端的服务器部署了一些预训练的机器学习模型，
客户可以通过应用程序编程接口（Application Programming Interface，API）调用这些模型并使用它们的预测能力。
客户不需要拥有自己的计算资源和优质数据集，可以快速、低成本地使用现成的模型来解决自己的问题。 MLaaS已应用于智能语音客服、人脸识别、城市交通预测等场景。

训练一个机器学习模型往往需要消耗大量的金钱和人力成本。
例如，训练1750亿参数的GPT-3模型的计算量为$3.14\times10^{23}$浮点运算次数（Floating Point Operations，FLOPs），约需要1024张80G显存的A100卡连续计算34天\cite{DBLP:conf/nips/BrownMRSKDNSSAA20}。
人工智能技术供应商凭借性能优异的机器学习模型可以获得巨大的利润，是其重要知识产权，
所以，攻击者一旦窃取了模型的信息，就能逃避付费或者开辟第三方服务从中获取商业利益，损害人工智能技术供应商的利益。
另一方面，泄露的模型信息会增加模型对于外界攻击的脆弱性。
攻击者可以将模型信息与其他攻击结合发动更具威胁性的攻击。
例如，在针对亚马逊和谷歌的在线人工智能分类任务进行黑盒对抗攻击的时候，
攻击者首先借助少量的查询窃取模型信息，借助这些信息生成对抗样本。
这些基于模型信息产生的对抗样本使得亚马逊和谷歌的在线分类模型分别出现96.19\%和88.94\%的误判率\cite{DBLP:conf/ccs/PapernotMGJCS17}。
模型窃取攻击（Model Extraction Attack，MEA）是一种主流的通过API进行模型信息推断的技术\cite{DBLP:conf/uss/TramerZJRR16, DBLP:conf/sp/WangG18, DBLP:conf/nips/CravenS95, DBLP:conf/uss/JagielskiCBKP20, DBLP:conf/cvpr/TruongMWP21, DBLP:conf/crypto/CarliniJM20}。
攻击者通过构造一系列模型输入并采集模型对应的输出信息来窥探模型的超参数，例如模型激活函数和模型结构。
MEA可以应用于多种机器学习任务，例如图像分类、语音识别、自然语言处理等。

综上所述，对于包含大量个体敏感信息的原始数据，本文主要关注在数据聚合分析过程中保护个体数据隐私；
对于经过预处理之后的高质量标准化数据集，本文主要关注在数据分发使用过程中保护数据集所有者的知识产权；
对于完成训练即将部署的数据模型，本文主要考虑评估模型部署过程中的超参数泄漏风险。

% 因此，使用包含受产权保护的数据的公开可用数据集构建商业AI软件可能会导致潜在的产权侵权。
% 同时，国家知识产权局提出构建登记程序，通过登记方式赋予数据处理者一定的权利。“这将有利于强化对他人不正当获取和使用数据行为的规制，也有利于激励市场相关主体投入更多的资源发掘数据的价值和促进数据要素的交易流通。”胡文辉说。
% 相比其他生产要素，数据的部分特性使它难以参照传统方式进行管理和利用，但其可复制、可共享、无限增长和供给的禀赋，打破了传统要素有限供给对增长的制约，
% 再加上数据主体多元、权利内容多样、场景丰富多变，数据与数据之间，
% 数据涉及的多元主体之间关联交织难以分割，
% 与传统要素主体的确定性、要素关联关系的稳定性和固定性之间也存在矛盾，使得数据权属界定尤为复杂。
\section{研究现状}
如前所述，本文主要探讨在数据流通场景中，
原始数据保隐私聚合分析技术、标准化数据集产权保护技术和数据模型超参数泄露风险评估技术。
接下来，我们将围绕这三个方面对现有的研究工作进行逐一介绍，并总结现有工作在效率、功能性以及安全上的优点和存在的缺陷。

\subsection{原始数据保隐私分析技术}
现有文献已经提出了大量面向不同的场景需求的原始数据保隐私分析技术。
按照技术所依据的数学原理不同，常见的保隐私数据分析方法可以分为三种：密码学方法、匿名化方法和数据扰动方法。
本文将分别讲解三种技术的基础概念和代表性事例。

\mypara{密码学方法}
基于加密的数据保隐私分析方法主要是构建在1976年由Diffie等提出的公钥加密框架\cite{DBLP:journals/tit/DiffieH76}。
之后，Rivest等提出了非对称加密算法Rivest–Shamir–Adleman（RSA），
解决了之前对称加密算法秘钥安全传输的困难\cite{DBLP:journals/cacm/RivestSA78}，
使得在不可信的信道上进行数据保隐私传输变得可行。
在非对称加密传输场景中，数据收集者首先生成一对公钥和私钥，并将公钥发送给需要上传数据的用户。
接着用户使用公钥加密自己的敏感数据，并将加密后的密文数据传输给数据收集者。
最后，数据收集者通过私钥解密密文得到用户的敏感数据。
% 同年，Rivest等提出了同态加密问题以及满足同态运算要求的算法\cite{rivest1978data}。
% 同态加密技术可以在密文状态下进行数据的计算分析，并在运算结束后解密密文得到最终结果，保证了参与方敏感数据的安全性。
% 同态加密技术的提出标志着加密方法从处理静态数据（密态传输）到处理动态数据（密态运算）的跨越。
1982年，姚期智院士通过提出了“百万富翁问题”，首次提出了安全多方计算的概念\cite{DBLP:conf/focs/Yao82b}，成为现代密码学数据保隐私分析的主要技术手段。
安全多方计算(Secure Multi-Party Computation，SMPC)是指在无可信第三方情况下，
通过多方共同参与，安全地完成某种协同计算。
即在一个分布式的网络中，每个参与者都各自持有秘密输入，
希望共同完成对某个函数的计算，但要求每个参与者除计算结果外均不能得到其他参与实体的任何输入信息。
也就是参与者各自完成运算的一部份，
最后的计算结果由部分参与者掌握或公开共享。
安全多方计算主要基于密码学的一些隐私技术，包括有同态加密(Homomorpgic Encryption)\cite{rivest1978data}，不经意传输(Oblivious Transfer)\cite{DBLP:journals/iacr/Rabin05}，混淆电路(Garbled Circuit)\cite{DBLP:conf/focs/Yao86}，秘密共享(Secret Sharing)\cite{DBLP:journals/cacm/Shamir79, DBLP:conf/mark2/Blakley79}等。

基于密码学的数据保隐私分析方法的优势是能够实现高数据可用性，理论上可以保证解密后的计算结果与明文结果无偏差。
然而，在大规模数据收集分析场景下，加解密和验证环节会带来巨大的计算开销。
例如，全同态加密（Fully Homomorphic Encryption）同时满足同态加法运算和同态乘法运算~\cite{DBLP:conf/stoc/Gentry09}，这意味着同态加密方案支持任意给定的计算函数。
但由于全同态计算开销极大，导致其无法在实际中使用。

\mypara{匿名化方法}
该方法的核心思想是通过删去原始数据贡献者的身份标签，
或者使用生成的虚假身份替代真实的身份标签，
从而破坏发布数据和数据贡献者之间的关联性\cite{DBLP:conf/icde/LiLZ09}。
$k$-匿名方法（$k$-anonymity）和$\ell$-多样性方法（$\ell$-diversity）是匿名化思想的两种代表算法。
其中，Sweeney等\cite{DBLP:journals/ijufks/Sweene02}提出的$k$-匿名方法通过删除原始的用户身份标签以及模糊化具有可区分性的数据信息（例如住址、年龄、性别等），保证每个数据贡献者的上传记录与至少$k-1$个其他贡献者无法区分，从而达到保护数据贡献者敏感数据隐私的目的。
为了弥补$k$-匿名方法无法抵御同质化攻击（Homogeneity Attack）以及背景知识攻击（Background Knowledge Attack）的不足，
Machanavajjhala等\cite{DBLP:conf/icde/MachanavajjhalaGKV06}提出了$\ell$-多样性方法。
该方法在经过$k$-匿名方法保护后的数据需要进一步满足$\ell$-多样性原则，即经模糊化后属性相同的数据贡献者其原始真实的属性至少有$\ell$个不同值。
但是，$\ell$-多样性方法依然受到相似性攻击（Similarity Attack）的威胁，即攻击者可以借助数据的全局分布推断具体数据贡献者的敏感信息。
虽然，Li等\cite{DBLP:conf/icde/LiLV07}提出了$t$-近似方法（$t$-closeness）来改进$\ell$-多样性方法，
但是，只要攻击者掌握了某一敏感属性的几条真实数据，攻击者便具有推断剩余敏感属性的能力\cite{DBLP:journals/kbs/Domingo-FerrerS15}。

基于匿名化思想的数据保隐私分析方法的优势是运算开销较小且数据保真度较好，
十分适合部署在计算资源有限且数据规模庞大的场景。
然而，匿名化方法由于没有定义出严格、统一的攻击者模型，因此常常陷入到攻击和保护方法的循环竞争中。
提出的保护方法容易受到不断涌现的新型攻击的威胁。
% 是一种简单直观的隐私保护技术，其基本思想是在发布数据之前，将数据中表示数据贡献者身份的片段删去，或者利用假名代替原有的身份标识，使得隐私攻击者无法根据发布的数据与具体的数据贡献者直接关联[。基于该匿名化思想，Sweeney提出了人匿名(k-anonymity算法，通过删去标识符信息（唯一标识个休身份）与合并抽象 准标识符信息（如年龄、邮政编码和人种等)，保证发布数据中单个个体的信息与

\mypara{数据扰动方法}
该方法通过适量地扰动敏感数据，以损失部分分析结果精度为代价，削弱敏感数据和数据贡献者身份之间的映射关系。
现有的数据扰动方法可以根据隐私定义的不同进行分类，
主要包括基于互信息的扰动技术（Mutual Information）\cite{cuff2016differential}、
基于最小熵的扰动技术（Minimum Entropy）\cite{alvim2011differential}、
基于差分可识别的扰动技术（Differential Identifiability）\cite{lee2012differential}和基于差分隐私的扰动技术（Differential Privacy）\cite{dwork2008differential}。
其中，基于差分隐私的扰动技术具有严格的数学定义以及良好的性质，例如后处理免疫和组合性原理，
已成为学术界和工业界主流的非加密数据保隐私分析标准\cite{wang2016relation}。
基于是否具需要可信数据聚合中心，差分隐私技术（Differential Privacy，DP）可以细分为集中式差分隐私技术（Centralized Differential Privacy，CDP）和本地差分隐私技术（Local Differential Privacy，LDP）。

Dwork等\cite{DBLP:conf/icalp/Dwork06}明确了集中式差分隐私的定义。
基于邻居数据集的概念，集中式差分隐私技术限制了增加、删除数据集中的任意一条记录对于统计结果的影响。
集中式差分隐私可以通过高斯机制\cite{nikolov2013geometry}、拉普拉斯机制\cite{dwork2006calibrating, dwork2016calibrating}和指数机制\cite{mcsherry2007mechanism}等扰动机制实现。
通过分析数据记录对于统计结果的影响，即全局敏感度（Global Sensitivity），
结合隐私保护强度量化指标隐私预算（Privacy Budget）可以计算出对应的扰动强度。
目前，集中式差分隐私已成功应用于保隐私的数据分析任务，例如频率分布估计\cite{hay2010boosting, zhang2014towards, acs2012differentially}、基于位置的服务\cite{andres2013geo, xiao2015protecting, hardt2010geometry}等。
另外，集中式差分隐私在保隐私的机器学习场景中也有很多应用案例\cite{abadi2016deep, 刘俊旭2020机器学习的隐私保护研究综述}，例如Meta公司开源的满足差分隐私的机器学习框架Opacus\cite{yousefpour2021opacus}。

然而，集中式差分隐私所假设的可信数据聚合中心在现实中往往难以实现。
一方面，需要引入复杂的安全保护措施才有可能保证数据聚合中心在数据收集、分析和发布的全过程中可信\cite{chen2017constrained, zhou2021node}。
另一方面，近年来用户敏感记录被数据收集分析公司泄漏的事件频发\cite{FacebookCohen2018, TribuneindiaKhaira2018, ConfidentialGreenbone2019}，使用户对敏感数据集中式分析和管理逐渐失去信任。
为此，Kasiviswanathan等\cite{kasiviswanathan2011can}提出了本地差分隐私的概念，即用户在本地先对敏感数据进行扰动之后再上传至数据聚合中心。
这种情况下，即使数据聚合中心是不可信的，用户的敏感数据依旧得到保护。
近几年，本地差分隐私技术受到了工业界和学术界的重视\cite{DBLP:conf/wsdm/KenthapadiMT19}。
例如，微软公司使用本地差分隐私技术分析用户使用移动应用的时间特征\cite{DBLP:conf/nips/DingKY17}，
苹果公司将本地差分隐私技术内嵌至移动端操作系统iOS和macOS中，用于保隐私地收集用户输入习惯等信息\cite{LearningDifferential2017}，
谷歌公司的研究人员提出满足本地差分隐私的RAPPOR算法，用以保隐私地分析谷歌浏览器用户的最喜爱主页\cite{DBLP:conf/ccs/ErlingssonPK14}。

% 与密码学方法相比，差分隐私技术不需要依赖密钥的安全性，
% 同时在计算和传输代价也较小，所以更适合大规模数据保隐私收集分析场景。
% 另外，相比于匿名化方法，差分隐私技术具有严格的理论保障和可量化的隐私保护强度（即隐私预算）。
综上所述，差分隐私技术不需要依赖密钥的安全性，
同时在计算和传输代价也较小，所以更适合大规模数据保隐私收集分析场景。
另外，差分隐私技术具有严格的理论保障和可量化的隐私保护强度（即隐私预算）。
特别是数据记录之间相互独立的情况，差分隐私技术可以保证即使攻击者掌握了数据集除目标用户之外的其他用户的敏感记录，
也无法准确推断出目标用户的敏感记录\cite{张啸剑2014面向数据发布和分析的差分隐私保护, 熊平2014差分隐私保护及其应用, 李杨2012差分隐私保护研究综述}。
然而，我们应同样意识到，差分隐私技术是一种基于数据扰动的策略，在保护用户敏感数据的同时会降低数据的可用性。
所以，在相同的隐私保护强度下，提高保隐私聚合分析结果的准确性是目前差分隐私领域研究的重点。


\subsection{标准化数据集产权保护技术}
现有的产权保护技术按照是否需要改变原始数据集，可以分侵入式方法和非侵入式方法。
侵入式方法通过生成特定分布的标签数据（即数据集水印），并混入原始数据集一起发布。
这些标签数据可以作为数据集产权界定的依据。
非侵入式方法通过挖掘数据集自身特征作为数据标签，在不影响原始数据集分布的情况下，实现数据集的产权审计。
接下来，本文将分别介绍这两种方法的基础概念和代表性事例。

\mypara{侵入式方法}
该方法一般包含两个主要功能模块，
一是水印生成模块，即生成数据集水印并注入到原始数据集;
二是水印检测模块，即通过从嫌疑机器学习模型中提取水印信息并做出审计论断。
生成的数据集水印需要满足无害性、有效性、隐蔽性。
具体来说，数据集水印应不影响机器学习模型在正常任务上的表现。
其次，水印需要稳定地嵌入使用注入水印的数据集训练的模型，并可用于验证数据集是否被用于模型训练。
最后，水印应具有隐蔽性，不能被数据集窃取者轻易检测出来。
Li等\cite{DBLP:journals/corr/abs-2010-05821}提出了一种基于模型后门攻击\cite{DBLP:journals/corr/abs-1712-05526, DBLP:journals/access/GuLDG19}的数据集水印生成方法。
在水印生成阶段，该方法通过在一部分原始数据上加上特定补丁并改变其对应标签形成数据集水印。
在水印检测阶段，由于特定补丁和改变后的标签（验证目标类）存在强相关性，
所以当嫌疑模型对带有补丁的数据分类为验证目标类的概率显著高于未带有补丁的原始数据时，
审计者可以依此判定嫌疑模型盗用了数据集。
但是，上述方法不仅在样本上叠加了可察觉的特定补丁，而且需要改变水印样本的原始分类标签。
由于特定补丁和正常数据的分布具有明显差异，所以在实际使用中容易被检测出来，从而导致水印失效\cite{DBLP:journals/compsec/AikenKWR21, DBLP:conf/sp/WangYSLVZZ19}。
为了提高注入水印的隐蔽性，Tekgul等\cite{DBLP:conf/codaspy/TekgulA22}将文献{\rm\parencite{DBLP:conf/icml/SablayrollesDSJ20}}提出的放射性数据（Radioactive Data）生成方法应用于数据集产权审计。
基于该方法注入的水印是弥散在整个图片像素空间中，所以更加隐蔽。
在检测阶段，该方法将水印数据和其对应的原始数据输入到嫌疑模型中，并分别记录嫌疑模型最后一层的输出。
接着，该方法采用假设检验方法判定两者是否有显著差异，如果有显著差异，则可判定嫌疑模型在训练阶段使用了水印数据。
Li等\cite{DBLP:journals/corr/abs-2210-00875}指出现有基于模型后门的水印技术在保护数据集产权的同时，往往将注入补丁的样本原有标签改变为特定目标标签。
攻击者可能借此向正常模型发动攻击，扰乱正常模型的将样本分类为特定的类别，从而威胁正常模型在部署场景中的安全。
为了缓解这个问题，Li等\cite{DBLP:journals/corr/abs-2210-00875}探索了非目标模型后门水印方案，
其中被后门触发的模型异常分类结果是不确定的。
具体来说，该文献定义了两个分散性指标并证明了它们的相关性，
在此基础上分别设计了针对改变标签和不改变标签设置下的非目标后门水印方法，
明显降低了攻击者利用模型后门发动攻击的威胁性。
% 衡量与预设的载体向量（carries vector）之间的余弦相似度来判定嫌疑模型在训练阶段是否使用了水印数据
% ，该方法基于对原始数据和被加上特定补丁的数据的预测结果进行假设检验来判定是否发生了数据集盗用。
% 所提出的保护方法不仅可以验证开源数据集是否被用于训练模型，而且不会影响数据集的正常使用。
% 通过损失数据集部分质量在原始数据集中隐藏一个秘密信号。
% 审计者尝试从嫌疑模型的输出中提取该信号，
% 从而验证该模型是否在训练过程中使用了被嵌入水印的数据集。
%% 段尾用

为了保证水印检测结果的准确性，生成的数据集水印分布需要和原始数据集有所不同。
但是，数据集水印分布如果偏离原始数据集太多，那么数据集的可用性将会降低。
由于不同场景下数据的分布特征千差万别，侵入式方法很难平衡水印检测准确性和数据集可用性。
另一方面，注入的模型后门可能带来新的模型脆弱性\cite{DBLP:journals/corr/abs-2210-00875}，
给模型使用者带来安全威胁。

\mypara{非侵入式方法}
为了解决上述侵入式方法存在的不足，
非侵入式方法将数据集自身特征作为审计依据，
从而避免影响原始数据的分布特征。
Maini等\cite{DBLP:conf/iclr/MainiYP21}发现经过训练的样本和模型决策边界的距离较远，
因此可以通过检测嫌疑模型对于数据集样本的输出来判断是否存在数据盗用现象。
具体来说，作者基于对抗样本生成思想\cite{DBLP:journals/corr/SzegedyZSBEGF13}估计数据集样本和嫌疑模型决策边界之间的距离，
接着将距离信息输入到一个回归预测模型得到审计结果，最后基于假设检验方法判定审计结果的显著性并输出最终的判定结果。
Dziedzic等\cite{DBLP:journals/corr/abs-2209-09024}将非侵入式方法推广到自监督学习场景。
由于自监督场景中样本数据没有标签，所以不能像文献{\rm\parencite{DBLP:journals/corr/SzegedyZSBEGF13}}依赖于样本数据点和模型决策边界之间的距离进行判定。
为此，作者采用高斯混合模型（Gaussian Mixture Model，GMM）拟合自监督模型的输入-输出分布特征，并预测嫌疑模型输入、输出之间的对数似然。
对于嫌疑模型使用过的数据样本，其对应的对数似然结果高于未被使用过的数据样本。
所以，作者通过统计多个样本点的对数似然结果并使用假设检验方法以确定嫌疑模型是否盗用数据集。
Dong等\cite{DBLP:conf/ndss/DongLCXZ023}借鉴现行软件产权保护方法，提出了一种新的数据集产权保护框架$\text{RAI}^2$。
由于机器学习模型对于训练数据的良性过拟合\cite{DBLP:conf/iclr/SanyalDKT21, DBLP:journals/jmlr/ChatterjiL21, bartlett2020benign}，模型对训练数据的判定置信度明显高于非训练数据。
所以，在审计阶段该框架通过对比本地模型（在数据集上直接训练的模型）和嫌疑模型对于数据集的样本点预测结果的相似性来判定是否发生数据集盗用。
相比现有工作，该框架通过引入了第三方产权注册机制克服了攻击者恶意注册被盗用数据集所引起的产权纠纷问题。

综上所述，侵入式方法通过向待保护的数据集中添加特定分布的数据水印以构成审计依据。
生成的数据水印需要综合考虑无害性、有效性、隐蔽性，
这增加了侵入式方法的在现实场景中部署的复杂度。
非侵入式方法通过挖掘待保护数据集自身特征形成审计依据，
相比于侵入式方法具有更好的普适性，特别是可以应用于已经发布的数据集。
但是，现有方法往往是预先假定数据集分布和数据类型，并以此设定算法参数。
当数据集分布发生变化时，现有方法需要重新对待保护的数据集进行人工分析后调整算法参数，
否则方法性能将会衰退甚至完全无效。
另外，现有文献主要集中在监督学习和非监督学习场景，缺乏对强化学习场景的数据集产权保护的研究工作。
% 与密码学方法相比，差分隐私技术不需要依赖密钥的安全性，
% 同时在计算和传输代价也较小，所以更适合大规模数据保隐私收集分析场景。
% 另外，相比于匿名化方法，差分隐私技术具有严格的理论保障和可量化的隐私保护强度（即隐私预算）。
% 特别是数据记录之间相互独立的情况，差分隐私技术可以保证即使攻击者掌握了数据集除目标用户之外的其他用户的敏感记录，
% 也无法准确推断出目标用户的敏感记录\cite{张啸剑2014面向数据发布和分析的差分隐私保护, 熊平2014差分隐私保护及其应用, 李杨2012差分隐私保护研究综述}。
% 然而，我们应同样意识到，差分隐私技术是一种基于数据扰动的策略，在保护用户敏感数据的同时会降低数据的可用性。
% 所以，在相同的隐私保护强度下，提高保隐私统计分析结果的准确性是目前差分隐私领域研究的重点。


\subsection{数据模型超参数泄漏风险评估技术}
不同的超参数设置会导致机器学习模型的性能具有明显的差异。
在机器学习模型构建阶段，
模型训练者需要消耗大量的人力、物力来寻找一组超参数以最大程度优化模型性能。
因此，模型超参数往往被视为模型训练者的知识产权和机器学习模型的机密信息，
需要在模型部署之前进行超参数泄漏风险的评估。
按照攻击者进行超参数推断的攻击路径不同，
超参数泄漏风险评估的方法可以分为基于侧信道的评估方法和基于查询的评估方法。
接下来，我将分别介绍这两种方法的基础概念和代表性事例。

\mypara{基于侧信道的评估方法}
该方法主要是通过分析待评估模型所部署的服务器的运行特征（如CPU和GPU的通信数据）进行机器学习模型的超参数推断，一般是针对待评估模型部署在不可信的服务器场景。
侧信道攻击最初用于密码学中用于密钥恢复，例如对RSA的攻击~\cite{Kocher1996TimingAO}。
最近，侧信道攻击的思想被扩展到了模型窃取领域，攻击者利用硬件或软件特性来推断模型超参数信息。
% 其中，最常见的提取信息包括模型超参数或训练后的模型权重。
文献{\rm\parencite{DBLP:journals/corr/abs-1810-03487}}详细分析了缓存侧信道（Cache Side-Channel）存在泄露模型超参数信息的安全性问题。作者借助模型运行主机上的一个共存进程，被动地监视共享框架中模型所用函数的访问情况，通过使用Flush+Reload缓存侧信道技术~\cite{Yarom2014FLUSHRELOADAH}，利用提取的信息重构受害模型的结构信息。在评估中，作者证明攻击者只需要观察到一次模型的前向传播，即可准确重构模型的结构网络。
文献{\rm\parencite{DSRB18}}提出了一种利用时序侧信道（Timing Side-Channel）进行黑盒神经网络模型网络层数推断攻击的方法，并使用强化学习与知识蒸馏有效地减小搜索空间以提高推断效率。另外，所提出的方法具有可扩展性，不受神经网络架构类型的限制。
文献{\rm\parencite{DBLP:conf/uss/YanFT20}}提出了一种使用缓存侧信道快速、准确地窃取深度神经网络（Deep Neuronal Network，DNN）架构的机制（Cache Telepathy）。DNN在进行推理时很大程度上依赖于广义矩阵乘法（Generalized Matrix Multiply, GEMM），而DNN的结构超参数决定了GEMM调用的次数和GEMM函数中使用的矩阵维度。
基于上述事实，文献{\rm\parencite{DBLP:conf/uss/YanFT20}}所提推断方法能够通过大幅减小目标DNN架构的搜索空间，有效地帮助获取模型结构信息。
文献{\rm\parencite{Liu2020GANREDGR}}提出一种基于生成对抗网络(GAN)框架的攻击方法GANRED，它利用缓存时间侧信道（Cache Timing Side-Channel）信息来准确地恢复DNN的结构超参数。相比于之前的工作，GANRED不需要进行DNN库代码分析受害者和攻击者之间不需要共享主内存段。另外GANRED能够准确分析受害者模型的结构信息，而不仅仅是缩小了模型结构搜索空间。
文献{\rm\parencite{Hu2020DeepSnifferAD}}提出了一种鲁棒的基于学习的方法来提取DNN架构（DeepSniffer）。
DeepSniffer通过从服务器系统总线窃听中获取内存访问事件，
使用长短时记忆网络链接时序分类（Long Short Term Memory-Connectionist Temporal Classification，LSTM-CTC）模型进行模型层信息识别，
根据内存访问模式进行模型层拓扑连接，并在数据量约束下进行层维度估计，可以准确地恢复与目标模型相似的网络架构。
以GPU平台为例，DeepSniffer通过学习内核的架构级执行特征以及DNN设计中常见的层间时序关联信息来进行模型提取。
最近的一些研究工作表明，攻击者还可以利用功率侧信道（Power Side-Channel）\cite{DBLP:journals/iacr/BatinaBJP18, Zhang2021StealingNN}、内存侧信道（Memory Side-Channel）\cite{Zhang2021StealingNN, DBLP:conf/sp/RakinCYF22}、电磁侧信道（Electro-Magnetic Side-Channel）\cite{liang2022clairvoyance, yu2020deepem}等侧信道信息推断神经网络模型的超参数。 


\mypara{基于查询的评估方法}
该方法主要是通过向待评估模型输入特定的样本并采集其输出进行模型超参数的推断，
一般是考虑待评估模型部署在可信执行环境（Trusted Execution Environment，TEE）的情况。
% 它们是攻击者与目标模型之间的基本交互方式。
% 仅使用此类操作的攻击类别被称为基于查询的攻击，因为查询是唯一的信息来源。因此，这些攻击在MLaaS（机器学习即服务）环境中非常合适。我们在第7节中介绍了基于查询的攻击的概述。
% 如果攻击者可以访问部署模型的计算资源的硬件或软件，这就为模型窃取攻击提供了额外的入口。在这种情况下，攻击者可以利用侧信道泄露，执行所谓的侧信道攻击（SCA）。
Oh等\cite{DBLP:conf/iclr/OhAFS18}展示了从黑盒模型的输入、输出中推断出不同机器学习模型超参数的可能性。
该工作提出基于元模型思想的超参数推断方法kennen，
不仅可以成功推断模型所使用的架构，
还可以成功推断与训练相关的超参数（如优化器和学习率）。
Wang等\cite{DBLP:conf/sp/WangG18}提出推断方法主要针对目标函数中的超参数，即用于平衡损失函数和正则项的超参数。
该方法适用于各种流行的机器学习算法，如岭回归、逻辑回归、支持向量机和深度神经网络，
并从理论和实验角度全面评估了所提方法的有效性。
文献{\rm\parencite{Rolnick2019ReverseengineeringDR}}证明了可以通过神经网络模型的线性区域边界来恢复深度ReLU网络的架构、权重和偏置。
具体来说，在深度ReLU网络中，每一个ReLU定义了一个分段线性函数，其中线性区域之间的边界对应于某些神经元在网络中切换非活跃和活跃ReLU状态的输入。
文献{\rm\parencite{Rolnick2019ReverseengineeringDR}}展示了仅通过查询深度ReLU网络在特定输入上的输出就可以重构这些边界，从而重构神经网络模型超参数。 
% 通过将区域边界集合解析为与特定神经元相关联的组件，作者从理论上和实验上都展示了恢复神经元的权重以及它们在网络中的排列的可行性。
% 我们观察到，网络通常可以明确地重构，最多只有每层神经元的排列置换和各个神经元的权重和偏置的缩放。在某些情况下，存在额外的同构性，此时我们的算法能够重构网络的大部分参数。
% 然而，我们证明实际上通常可以通过仅观察神经网络的输出来确定未知深度ReLU网络的架构、权重和偏置。

综上所述，基于侧信道的评估方法通过挖掘侧信道信息与模型超参数设置之间的关系，
然后使用模型训练或者部署过程中采集到的侧信道信息推断模型超参数，
可以得到较高的恢复精度。
但是，相比于基于查询的评估方法，基于侧信道的评估方法需要访问模型所在服务器的软件或者硬件方面的信息。
随着可信执行环境的发展和普及，基于侧信道的评估方法在实际中受到越来越多的限制。
基于查询的评估方法利用待评估模型的输入、输出推断模型超参数信息，
只需要能够正常访问模型，对模型部署的服务器没有额外的要求，所以对于部署在可信执行环境中的模型依旧是有效的。
但是，模型的超参数对模型行为的影响具有耦合性，所以需要根据模型特性设计查询输入以最大化超参数推断精度。
% 另外，现有工作主要集中在传统机器学习模型和深度学习模型，缺乏对强化学习模型超参数泄露风险的评估工作。
另外，现有工作主要集中在监督学习和非监督学习场景，缺乏对强化学习模型超参数泄露风险的评估工作。

% 对于训练后的模型权重，
% % 假设攻击者可以访问神经网络模型运行所在的计算机硬件，则可以利用运行功耗\cite{DBLP:journals/iacr/BatinaBJP18}、执行时间\cite{DBLP:journals/corr/abs-1812-11720}和缓存数据\cite{DBLP:conf/uss/YanFT20, DBLP:journals/corr/abs-1810-03487}等侧信道信息推断神经网络模型的详细信息。

% 在本文中，我们提出了一种新的模型提取攻击，称为“千里眼”，它利用GPU发出的某些远场电磁信号，在距离受害者机器几米远的地方窃取DNN模型，即使中间有一些障碍物\cite{liang2022clairvoyance}


% 我们提出了一种基于侧信道信息的高效模型窃取攻击方法。具体来说，所提出的攻击方法包括两个阶段:1)通过EM侧通道信息推断底层网络架构;2)通过基于边缘的对抗性主动学习方法估计参数，特别是权重。实验结果表明，该攻击方法可以准确地恢复EM侧信道信息泄漏的大规模神经网络。总的来说，我们的攻击强调了在实际应用中屏蔽大规模NN加速器的EM轨迹的重要性。\cite{yu2020deepem}


% 特别是，我们专注于模型开发人员和对手在训练深度神经网络(DNN)模型时共享相同GPU的场景。我们利用基于上下文切换惩罚的GPU侧通道。这个侧通道允许我们提取DNN模型的细粒度结构秘密，包括其层组成和超参数。利用这个侧信道，我们开发了一个名为MosConS的攻击原型，它应用基于lstm的推理模型来识别结构秘密。我们对MosConS的评估表明，可以准确地恢复结构信息。因此，我们认为应该开发新的防御机制来保护训练免受GPU侧信道的影响。\cite{Wei2020LeakyDS}


% 本文调查了相对于在微控制器和fpga上实现嵌入式dnn的最先进的物理SCA攻击，以便对当前的情况进行彻底的分析。它提供了当前攻击的分类和详细分类。它首先讨论缓解技术，然后为未来的研究线索提供见解。\cite{Real2021PhysicalSA}


% 在本文中，我们展示了部署的深度学习模型对计时侧信道攻击的实际脆弱性。 通过测量推理的执行时间，对手可以从已知的深度学习模型系列中确定和重建模型，然后使用可用技术恢复剩余的超参数。 该漏洞在 Intel Compute Stick 2 上针对 VGG 和 ResNet 系列模型进行了验证。 此外，所提出的攻击非常具有破坏性，因为它可以在跨设备设置中执行，在合法拥有的设备上构建的对手配置文件可以用于通过单个查询来利用受害设备，并且仍然可以获得近乎完美的成功率。\cite{Won2021TimeTL}

% 在这项工作中，我们提出了一个先进的模型提取框架 DeepSteal，它首次借助内存侧信道攻击远程窃取 DNN 权重。 我们提出的 DeepSteal 包括两个关键阶段。 首先，我们通过采用基于行锤的故障技术作为信息泄漏向量，开发了一种新的权重位信息提取方法，称为HammerLeak。 HammerLeak 利用为 DNN 应用程序量身定制的多种新型系统级技术来实现快速高效的权重窃取。\cite{Rakin2021DeepStealAM}

% 在本文中，我们首先分析对 DNN 模型的代表性内存侧信道攻击，并确定泄漏的主要原因。 我们还发现，用于保护模型参数的完全加密可能会增加大量开销。 根据我们的观察，我们提出了 DNNCloak，这是一个轻量级的安全框架，旨在减轻对常见 DNN 架构的逆向工程攻击。 DNNCloak 包括一组混淆方案，增加了对 DNN 结构进行逆向工程的难度。 此外，DNNCloak 通过高效的矩阵置换方案减少了全权重加密的开销，从而减少了内存访问时间并增强了针对模型参数再训练攻击的安全性。 最后，我们展示了 DNNCloak 如何以最小的性能开销有效地保护 DNN 模型免受侧信道攻击。\cite{Che2022DNNCloakSD}

% 我们将这项工作重点放在高端 32 位微控制器 (Cortex-M7) 中嵌入的深度神经网络的软件实现上，并通过侧信道分析揭示与基于保真度的参数提取相关的几个挑战，从基本的乘法运算到 通过层的前馈连接。 为了精确提取单精度浮点 IEEE-754 标准中表示的参数值，我们提出了一个迭代过程，该过程通过来自 Cortex-M7 目标的模拟和跟踪进行评估。 据我们所知，这项工作是第一个针对此类高端 32 位平台的工作。\cite{Joud2022API}

% 我们研究了指纹攻击的潜在途径，以识别 CPU-GPU 边缘设备上的（运行中的）DNN 模型架构系列（不属于最先进的 DNN 类别）。 我们利用对聚合系统级边信道信息的秘密分析，例如用户空间级别可用的内存、CPU 和 GPU 使用情况。 据我们所知，这是同类攻击中的第一次，不需要对受害设备进行物理访问和/或 sudo 访问，只被动地收集系统跟踪，这与大多数现有的基于逆向工程的攻击相反 DNN 模型架构提取攻击。\cite{Patwari2022DNNMA}

% 基于GPU profile窃取模型的工作\cite{o2023ezclone}，我们提出了两种适用于各种威胁模型的 DNN 架构提取技术。 第一种技术使用 PyTorch 的恶意动态链接版本，通过 PyTorch 分析器暴露受害 DNN 架构。 第二个称为 EZClone，利用聚合（而不是时间序列）GPU 配置文件作为预测 DNN 架构的侧通道，采用简单的方法并假设与以前的工作相比对手能力很小。


\section{研究挑战}
针对流通场景的数据安全问题，本文以机器学习场景为例，围绕三种关键数据要素形式展开研究：
对于原始数据，在其收集阶段往往涉及大量的用户数据，所以原始数据中不乏一些用户隐私数据，如何设计合理的保隐私数据聚合分析机制，以保证用户隐私不泄漏并实现较高的数据可用性；
对于标准化数据集，在数据发布和使用过程中存在未授权复制分发，侵犯数据产权的风险，
如何设计具有高鲁棒性和灵敏性的数据产权审计机制，以保护标准化数据集的知识产权；
对于数据模型，在部署过程中存在模型超参数泄漏的风险，进而对模型在实际中的安全应用带来威胁，如何设计模型超参数风险评估技术，对模型超参数的泄漏风险进行事前评估，为部署过程中的针对性加固提供依据。
但是，隐私保护和数据可用性的矛盾、数据集格式的非统一性、数据模型的黑盒访问权限、机器学习模型的弱可解释性、针对强化学习模型的相关工作缺乏等因素为面向流通场景的数据安全关键技术研究带来了诸多挑战。
本文归纳了需要面对的部分挑战：
\begin{itemize}
    \item[\textbf{1.}]\textbf{隐私保护和数据可用性存在矛盾。}
    基于数据扰动的隐私保护技术凭借计算、通信开销低，适合本文所关注的大规模的原始数据收集任务。
    但是，基于数据扰动的隐私保护技术通过向原始数据中添加噪声的方式实现用户隐私数据脱敏，其隐私保护强度一般是和引入扰动的强度成正相关。
    所以，在用户需要较强的隐私保护时，数据的可用性会出现明显的下降。
    另外，随着数据采集设备不断发展和丰富，原始数据一般具有较高的数据维度，数据扰动方法会破坏不同维度数据之间的相关性，加剧数据可用性的衰减。
    为了更好地实现隐私保护和数据可用性之间的权衡，即在保证相同的隐私保护强度下尽可能具有更高的数据可用性，需要设计一种新颖高效的保隐私数据聚合分析方法。
    \item[\textbf{2.}]\textbf{数据维度和数据格式的差异性。} 
    对于不同的任务场景，数据维度和数据格式也千差万别。
    目前的方法往往关注低维度、格式统一的数据，
    不能很好的适配现实场景中广泛出现的高维度、格式不同的数据。
    例如，现有的基于数据扰动的隐私保护方法往往针对一维和二维数据场景进行设计，缺乏考虑更高维度的数据场景；
    现有的标准化数据集产权保护技术一般是面向具有固定色彩通道和色彩深度的离散图像数据，缺乏考虑现实物理场景中采集到的具有不同格式、离散和连续混合的数据。
    为了更好地匹配现实需求，即满足用户和数据发布者对于隐私保护和数据产权保护的需求。本文需着重提升所提方法对高维度、格式不统一数据的兼容性和性能。
    \item[\textbf{3.}]\textbf{数据模型的黑盒访问权限。} 为了保护数据模型所有者的知识产权并降低潜在的安全风险，现实中部署的数据模型通常开放黑盒访问权限。
    用户通过开放的模型接口向数据模型发送查询，并得到模型输出，而模型架构和内部参数信息未知。
    对于数据集产权审计任务，模型的黑盒访问权限减少了可以用来进行审计任务的信息，增加了审计难度。
    对于数据模型超参数泄漏风险评估任务，由于数据模型的高度非线性和超参数耦合性，所提方法需要仅有模型输入、输出的情况下推断的超参数信息同样是困难的。
\end{itemize}
\section{本文研究内容}
数据流通是实现数据社会化利用和实现数据资源价值的必然路径，
现有研究已经提出了一些面向流通场景中的数据安全问题的解决方案。
但是，现有研究仍然存在一些不足和待解决的问题，主要有以下四个方面：
1）目前保隐私数据分析技术主要面向低维数据，对于高维数据的保隐私分析方法的研究较少。另外，已有工作在高维数据上表现不佳，往往具有较高的计算复杂度和较低的数据可用性；
2）对于标准化数据集产权保护的工作主要集中在侵入式方法，缺乏对数据集分布影响更小、适用性更广的非侵入式方法研究；
3）现有数据模型超参数泄漏风险评估方法主要是依据侧信道信息，缺乏适用于可信执行环境的基于查询方法的研究；
4）现有工作主要面向监督学习和非监督学习范式，对于强化学习范式的数据安全问题缺乏关注。

\subsection{研究思路}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\hsize]{figure/others/全文框架5.pdf}
    \caption{本文组织架构图} 
    \label{fig:paper-framework}
\end{figure}
本文面向流通场景的数据安全关键技术展开研究，全文的组织结构如\autoref{fig:paper-framework}所示。
第一章绪论介绍相关研究背景和研究现状。
第二章介绍差分隐私技术和强化学习的定义及基础理论。
第三至五章是本文的主体部分：
第三章针对原始数据聚合分析过程中存在的用户敏感数据泄露问题，结合本地差分隐私技术，设计了保隐私的高维数据聚合机制；
第四章针对经过预处理之后的高质量标准化数据集的产权保护问题，提出了在黑盒访问权限下模型训练数据集产权审计方法；
第五章针对训练后的数据模型，探索模型部署使用阶段的超参数泄露风险评估技术。
考虑到现有工作主要面向监督和非监督学习，缺乏对强化学习的研究，第四章和第五章所提方法主要是面向深度强化学习模型。
第六章对本文的研究内容进行了总结并归纳了未来的研究方向。


%%%20230616 1750 改到这里了
\subsection{研究内容}
本文主体部分的具体研究内容总结如下：
\begin{itemize}
    \item 第二章研究了原始数据保隐私分析问题。
    在数据收集和初步分析阶段，为了保护的用户敏感数据的隐私，我们基于本地差分隐私设计了一种自适应层级分解的范围查询分析协议（\myahead ）。该方法克服了现有基于本地差分隐私的范围查询分析方法普遍存在的缺陷，即根据预先定义的静态编码方式来收集分析用户数据。在保隐私的数据收集过程中，这些静态编码方式可能会引入过量的扰动噪声。相比于现有方法，\myahead 可以自适应地、动态地调整数据收集过程中的编码方式，从而使注入的噪声强度得到很好的控制，在保护用户隐私的同时，保持了分析结果的高效用。
    另外，我们通过分析扰动噪声的特性，提供了选择\myahead 超参数的理论方法，以便在严格满足LDP的同时，保证\myahead 具有优异的整体效用。我们使用多个真实数据集和合成数据集，综合展示了\myahead 在低维和高维范围数据范围查询分析任务中的有效性，以及相比于现有方法的优势。
    最后，基于定性分析和实验结果，我们总结了一些规律以便于在实际中部署\myahead 。
    总之，本文提出的方法很好地解决了现有方法依赖静态编码方式的局限性，实现了在相同隐私保护强度下，更准确的聚合分析结果。
    \item 第三章考虑标准化数据集产权保护问题。
    数据是机器学习中的关键资产，高质量的数据集可以显著提高机器学习模型的性能。
    强化学习凭借着擅长处理序列决策方面的优势，现在已广泛应用于自动驾驶、电力调度等生产场景。
    在这些生命攸关的场景中，强化学习模型不能直接在实际环境中进行训练，通常依赖于预先采集、处理的高质量数据集进行模型初始化训练，即离线强化学习（Offline Reinforcement Learning, offline RL）。
    为了支持这些模型的开发，许多商业机构耗费大量人力、物力进行数据的收集、清洗和标注，以形成标准化数据集。
    这些标准化数据集具有很高的商业价值，甚至成为机构的核心竞争力。
    另外，一些非盈利组织提供了具有开放源代码许可证的数据集。
    但是，这些标准化数据集在复制和分发的过程中，容易受到潜在的滥用或侵权行为的威胁。
    现有工作通过注入水印以进行数据追溯，保护数据所有者的知识产权。
    但是，其面临两个主要挑战：
    （1）审计策略必须具有灵活性，以处理已经发布且无法事后更改的数据集；
    （2）审计人员不能使用除正在接受审计的数据集之外的任何辅助数据集。
    为了克服这些挑战，我们提出了面向离线强化学习场景的轨迹级数据集产权审计机制\sysnameo 。
    本文发现累计奖励可以作为数据集的内生指纹，以判断强化学习模型是否基于特定数据集训练。
    通过对多个强化学习模型和任务进行综合实验，
    本文展现了 \sysnameo 的有效性，产权审计的真正率（True Positve Rate，TPR）和真负率（True Negative Rate，TPR）均超过了 $90\%$。
    另外，本文总结出多个实验结论，可帮助审计员在实际中部署 \sysnameo 。
    最后，本文使用 \sysnameo 审计基于 Google 和 DeepMind 的开源数据集训练的离线强化学习模型，
    实验结果证明了所提方法在审计已发布实际数据集方面的有效性。
    \item 第四章研究了数据模型超参数泄露风险评估问题。
    鉴于强化学习模型已被应用于许多关键系统，例如智能电网、交通控制、自动驾驶等，
    为了保护知识产权和减少潜在的安全风险，实际中部署的强化学习模型通常只具有黑盒访问权限，即接收输入并给出相应的输出。
    模型内部的神经网络结构和训练过程中超参数设置通常不会向用户公开，以免侵犯模型拥有者的知识产权或增加模型所面临的安全风险。
    但是，现有工作在监督学习和非监督学习场景下已经证明，基于模型的输入、输出可以准确地推断出模型超参数。
    所以，考虑到实际系统的安全性要求，在强化学习模型部署前评估模型的输入、输出泄漏模型超参数的风险十分必要。
    为了有效地评估模型超参数泄露风险，本文主要面临两个主要挑战：
    （1）为保证与实际场景一致，所提方法需适用于只具有黑盒访问权限的模型，可以获取信息十分有限；
    （2）不同类型的超参数对强化学习模型训练的影响具有较强的耦合性。
    为了克服上述困难，本文提出了一种针对强化学习模型的超参数泄露风险评估方法（\sysname）。
    在相同的环境中，具有不同超参数设置的强化学习模型具有不同的行为偏好。
    基于上述事实，本文设计了两种模型输入生成机制来激发强化学习模型的产生输出分歧，
    并训练了一个攻击模型来拟合强化学习模型的行为和超参数设置之间的关系。
    在多个强化学习模型和环境的设置组合下，本文全面地展示了\sysname 的超参数推断的有效性。
    例如，在PPO模型和CartPole环境上，所提方法推断准确度超过$90\%$。
    另外，本文展示了模型的超参数信息可以作为增强其他下游攻击的基础。
    依据实验结果，模型的超参数信息可以有效提高对抗样本攻击在不同模型之间的可迁移性。
\end{itemize}

% \subsection{创新性}
\chapter{预备知识}
\section{差分隐私技术}
\subsection{本地差分隐私}
在LDP中，每个用户通过扰动机制 $\Psi$ 扰动其私有数据 $v$，然后将 $\Psi(v)$ 发送给聚合器，
同时满足以下严格的LDP保证：
% \begin{definition}
%     $\epsilon$-Local Differential Privacy ( $\epsilon$-LDP ) \cite{kasiviswanathan2011can}. 
% \end{definition}

\begin{definition}{$\epsilon$-本地差分隐私（$\epsilon$-LDP~\cite{kasiviswanathan2011can}）} 
    对于任意 $\epsilon > 0$ 和可能的输入对 $v_1$, $v_2 \in D$，
    扰动函数 $\Psi(\cdot)$ 满足$\epsilon$-LDP当且仅当：
    \begin{equation}
        \forall T \in \operatorname{Range}(\Psi): \Pr{\Psi\left(v_{1}\right) \in T} \leq e^{\epsilon} \Pr {\Psi\left(v_{2}\right) \in T},  \nonumber
    \end{equation}
    其中 $\operatorname{Range}(\Psi)$ 表示 $\Psi$ 的所有可能输出构成的集合。
\end{definition}

\subsection{保隐私频率估计算法}
频率估计（Frequency Oracle，简称\fo）协议用于估计私有属性上的频率分布$F$，是一种LDP任务的基本构建块之一，如边际发布\cite{zhang2018calm}和范围查询\cite{wang2019answering, cormode2019answering}。
\fo 协议一般包括三个步骤，即编码、扰动和聚合\cite{wang2017locally}。
接下来，我们介绍两种最先进的 \fo 协议。 

\subsubsection{广义随机响应（\grr）}
\grr 算法是随机响应\cite{warner1965randomized}的一种广义版本。

\mypara{编码} \grr 直接扰动私有值 $v$，因此对于用户 $i$，编码值 $x_i$ 等于 $v_i$。

\mypara{扰动} 用户 $i$ 以概率 $p = \frac{e^{\epsilon}}{e^{\epsilon}+|D|+1}$ 保留 $v_i$，并以概率 $q=\frac{1}{e^{\epsilon}+|D|+1}$ 随机选择 $v_i^{\prime} \in D$，其中 $v_i \neq v_i^{\prime}$，然后将 $x_i^{\prime}$ 上传到服务器，其中 $x_i^{\prime}$ 为 $x_i$ 扰动后的值。

\mypara{聚合} 聚合器计算 $v$ 被报告的次数，表示为 $\operatorname{count}[v] = \sum_{i = 1}^{N} \mathbb{I}_{{x^{\prime}=v}}$。$v$ 频率的无偏估计为
$\hat{f_v} = \frac{\operatorname{count}[v]- Nq}{N(p-q)}$。

\mypara{估计误差}
$\hat{f_v}$是真实频率$f_v$的无偏估计 \cite{wang2019answering}。因此，\grr 的估计误差源自于算法统计方差，即
\begin{equation}
\myvar_{\grr(\epsilon)} = \frac{|D| - 2 + e^{\epsilon}}{N\left(e^{\epsilon}-1\right)^{2}}
\label{GRRVAR}
\end{equation}

\subsubsection{优化一元编码（\oue）}
\label{myoue}
\oue \cite{wang2017locally} 是基于 \rappor 协议\cite{erlingsson2014rappor} 的优化算法。

\mypara{编码}
用户 $i$ 将其私有数据编码为一个独热二进制向量，即 $ x_i = [0,\ 0,\ \ldots,\ 1,\ \ldots,\ 0]$，长度为 $|D|$，其中只有第 $v_i$ 个位置是 $1$。

\mypara{扰动}
用户 $i$ 根据概率 $p = \frac{1}{2}$ 和 $q = \frac{1}{e^\epsilon + 1}$ 翻转 $x_i$ 的每个比特。
其中，比特 1 和 0 以不同的概率翻转，即1（或 0）以概率 $p$（或 $1-q$）保持不变，以概率 $1-p$（或 $q$）翻转为相反的值。
然后，用户 $i$ 将 $x_i^{\prime}$ 上传到服务器。

\mypara{聚合}
聚合器收集用户上传的 $\{x_i^{\prime}\}_{i=1}^{N}$，并计算每个比特位中 1 的出现次数，例如，对于第 $v$ 个比特，需要对 $\operatorname{count}[v] = \sum_{i = 1}^{N} x_i^{\prime}[v]$ 进行校正，以获得无偏估计 $\hat{f}[v] = \frac{\operatorname{count}[v] - Nq}{N(p - q)}$。

\mypara{估计误差}
在 \cite{wang2017locally} 中证明了 \oue 具有方差
\begin{equation}
\myvar_{\oue(\epsilon)} = \frac{4e^{\epsilon}}{N\left(e^{\epsilon}-1\right)^{2}}
\label{OUEVAR}
\end{equation}
\grr 和 \oue 都实现了频率值的无偏估计。
如 \autoref{GRRVAR} 和 \autoref{OUEVAR} 所示，\oue 具有独立于 $|D|$ 的方差。
对于较小的 $|D|$（例如，$|D|-2<3e^\epsilon$），\grr 更好；对于较大的 $|D|$，\oue 更优。


\section{在线强化学习}

\subsection{在线强化学习问题}
\label{sec:Reinforcement Learning Problem}
在强化学习问题中，模型的目标是寻找一种最优或接近最优的策略，以最大化完成一段序列决策后累积的“奖励函数”或其他用户提供的奖励信号。
模型的学习过程与动物心理学中的条件反射实验相似。
例如，假设在投喂食物（奖励）之前播放音乐，
那么狗倾向于将声音与食物联系起来，并在仅听到声音刺激时流口水~\cite{PG41}。

强化学习问题可以被建模为一个马尔科夫决策过程（Markov Decision Process，MDP）问题，通常由四个变量组成：
\begin{itemize}
% \setlength\itemsep{-0.3em}
\item 状态空间 $\mathbb{S}$ 包含模型的所有可能状态，其中 $s_t \in \mathbb{S}$ 表示时间步 $t$ 时的模型状态。
\item 动作空间 $\mathbb{A}$ 包含模型的所有可能动作，其中 $a_t \in \mathbb{A}$ 表示时间步 $t$ 时的模型动作。
\item 状态转移函数 $\mathcal{T}$ 定义为从状态-动作对到新状态的概率映射 $\mathcal{T}: (\mathbb{S} \times \mathbb{A}) \times \mathbb{S} \rightarrow [0, 1]$。
\item 奖励函数 $\mathcal{R}: \mathbb{S} \times \mathbb{A} \rightarrow \mathbb{R}$ 输出在状态 $s_t$ 上采取动作 $a_t$ 时预期的奖励 $r_t \in \mathbb{R}$。
\end{itemize}
训练后的模型学习到一个策略 $\pi$，
它将状态-动作对映射到概率分布 $\pi: \mathbb{S} \times \mathbb{A} \rightarrow [0, 1]$，以最大化累计奖励。
\begin{equation}
    \pi^{*}=\underset{\pi}{\arg \max } \sum_{t=t_{0}}^{T} \sum_{a_{t} \in \mathbb{A}} \gamma^{t-t_{0}} \mathcal{R}\left(s_{t}, a_{t}\right) \pi\left(s_{t}, a_{t}\right)
\end{equation}
其中，折扣因子 $\gamma$ 被用于折算累计奖励中的未来奖励，$T$ 是一个游戏回合的终止时间步，$t_0$ 是初始时间步。

\mypara{示例}
\begin{figure}[!t]
    \center
    \includegraphics[width=0.6\hsize]{figure/drl_hypertheif/fig-drl-examplech.pdf}
    \caption{强化学习问题示例}
    \label{fig:running example}
\end{figure}
\autoref{fig:running example} 给出了一个运行示例，
其中模型与Flappy Bird游戏环境\footnote{\url{https://flappybird.io}} 进行互动。
状态包括鸟和障碍物的位置。
模型选择是否点击（动作）来让鸟向上飞或下落。
奖励为每次操作之后获得的分数。
模型从环境中获取状态和奖励，然后在每个时间步从动作空间中选择一个动作。
然后，环境更新状态并给予奖励。
基于环境状态、执行动作和奖励，模型训练一个策略（神经网络）来最大化累计奖励，即游戏结束时的总分数。

\subsection{在线强化学习模型}
\label{sec:Deep Reinforcement Learning model}
强化学习的目标是找到一种最优行为策略以获取高奖励。
基于不同的策略优化方法，我们将现有的强化学习模型分为三类。

\mypara{基于值的强化学习模型}
Q-learning是一种主要的基于值的强化学习模型 \cite{watkins1989learning}，
通过最大化连续步骤中总奖励的期望值来找到最优策略。
Q-learning通过训练拟合一个用于计算状态-动作组合质量的函数：
$
\mathcal{Q}: \mathbb{S} \times \mathbb{A} \rightarrow \mathbb{R}.
$
对于基本的Q-learning算法，$\mathcal{Q}$ 在训练初始化阶段被定义为一张充满随机值的表格。
在每个时间步 $t$，模型基于当前的环境状态，选择一个执行动作 $a_t$，并收到奖励 $r_t$。
然后，环境更新状态至 $s_{t+1}$（取决于先前的状态 $s_t$ 和选择的动作 $a_t$），
模型根据交互过程中的状态、动作和奖励数据更新 $\mathcal{Q}$ 中的值。
$\mathcal{Q}$每次更新过程如下所示。
\begin{equation}
    \label{eq:q-learning-obj}
    \mathcal{Q}^{\text{new}} \leftarrow \mathcal{Q} + 
    \alpha \cdot\left(r_{t}+\gamma \max_{a} \mathcal{Q}\left(s_{t+1}, a\right)-\mathcal{Q}\left(s_{t}, a_{t}\right)\right), 
\end{equation}
其中，$\alpha \in (0,1]$ 是学习率，$\mathcal{Q}^{\text{new}}$ 的变化速度随着 $\alpha$ 的增大而加快。

如\autoref{eq:q-learning-obj}所示，Q learning使用原始的 $\mathcal{Q}$ 值和通过交互获取的新信息的加权平均值来进行更新。

然而，当状态和动作空间十分庞大时，Q-learning 很难维护和存储一个巨大的$\mathcal{Q}$。
为了解决这个问题，深度Q-learning网络（Deep Q-learning Network, DQN)~\cite{MKSGAWR13} 将深度神经网络用来拟合$\mathcal{Q}$。
凭借着神经网络在学习高维特征表示和函数逼近方面的卓越性能，DQN在围棋 \cite{SSSAHGHBLB17} 和 Atari 游戏上达到了人类专家水平~\cite{MKSGAWR13}。

\mypara{基于策略更新的强化学习模型}
相比于Q-learning通过和环境交互拟合$\mathcal{Q}$，基于策略更新的强化学习模型直接学习状态和动作之间的映射关系。
例如，REINFORCE~\cite{W92}通过梯度下降更新神经网络参数的以获得交互策略。
目前，基于策略更新的强化学习模型倾向于使用深度神经网络，如多层感知机 \cite{XLZP18} 或循环神经网络 \cite{ZMFLA16}）作为策略$\pi$的载体。
基于策略更新的强化学习模型的目标函数为最大化交互累计奖励。
\begin{equation}
    \label{eq:pg-obj}
    J(\theta)=\mathbb{E}_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta}}\left[\sum_{t=0}^{\infty} \gamma^{t} r_{t}\right]. 
\end{equation}
根据文献{\rm\parencite{KT99}}，目标函数的梯度可写成如下方程：
\begin{equation}
    \nabla_{\theta} J(\theta)=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) v_t\right], 
\end{equation}
其中 $v_t$ 是 $Q_{\pi_{\theta}}(s,a)$ 的采样值，$Q_{\pi_{\theta}}(s,a)$则表示MDP问题中真实的累计奖励函数，而 $\pi\left(s, a\right)$ 则是策略网络。

\mypara{Actor-Critic强化学习模型}
Actor-Critic 强化学习模型 \cite{MBMGLHSK16} 由两个主要的部分构成：Actor 和 Critic，分别表示策略和动作值函数。
Actor 负责决定采取什么动作，而 Critic 则对 Actor 采取的动作进行评分。
Actor 训练过程与基于策略更新的强化学习模型相似，
Critic 通过计算动作值函数来评估 Actor 采取的动作。
由于动作值函数有助于策略更新过程，
因此 Actor-Critic 强化学习模型可以减少参数更新过程中的梯度方差，加速模型收敛过程。
Actor-Critic 强化学习模型有两个目标函数，分别用于 Actor 和 Critic 网络。
对于 Actor，其目标函数形式类似于 \autoref{eq:pg-obj}，但是使用的是 $\mathcal{A}_{\pi_{\theta}}(s, a)$ 而不是 $Q_{\pi_{\theta}}(s, a)$。
\begin{equation}
    \mathcal{A}_{\pi_{\theta}}(s, a)=Q_{\pi_{\theta}}(s, a)-V_{\pi_{\theta}}(s), 
\end{equation}
其中 $V_{\pi_{\theta}}(s)$ 表示状态 $s$ 的平均值。
当 $\mathcal{A}{\pi{\theta}}(s, a) = 0$ 时，调整动作概率不会带来额外的收益，
此时 Actor 的参数将不再更新。
Critic 的目标函数是最小化估计动作值函数和真实动作值函数之间的误差。

为了加速 Actor-Critic 强化学习模型的训练过程\cite{SLAJM15}，
文献{\rm\parencite{SWDRK17,HTSLMWTEWERS17}}将一个新的目标函数称为“裁剪代理目标函数”（clipped surrogate objective function）整合到了 Actor-Critic 算法中，
新的算法被称为近端策略优化（proximal policy optimization, PPO）。
由于PPO易用性，OpenAI将PPO作为其默认的深度强化学习算法\footnote{\url{https://openai.com/blog/openai-baselines-ppo/}}。


\section{离线强化学习}


\subsection{离线强化学习问题}
\label{sec:Offline Reinforcement Learning Problem}
离线强化学习模型旨在从预设数据集 $D$ 中学习一个最优（或近似最优）策略，无需与交互式环境进行交互。
在离线 RL 模型中，利用预设数据集最大化累计奖励或其他用户提供的奖励信号。
我们使用 $\mathbb{S}$（或 $\mathbb{A}$）来表示状态（或动作）空间，其包含模型的所有可能状态（或动作）。
$r_t \in \mathbb{R}$ 是每个时间步的奖励，其中 $\mathbb{R}$ 是实数集。
在预设数据集中，最小的数据单位为迁移（Transition），它是一个由四个元素构成的集合：$\{s_t, a_t, r_t, s_{t+1}\}$。
其中 $s_t \in \mathbb{S}$，$a_t \in \mathbb{A}$，$s_{t+1} \in \mathbb{S}$ 是 $s_t$ 的下一个状态。
而一组按时间顺序排列的迁移构成了数据集 $D$ 中的一条轨迹（Trajectory）。
基于数据集中的轨迹信息，离线 RL 模型刻画数据集中蕴含的马尔可夫决策过程（Markov Decision Process, MDP），
并形成行为策略 $\pi_\theta(a\mid{s})$ 来最大化 $J(\pi)$。
\begin{equation}
    J(\pi)=\mathbb{E}_{{s_t} \sim d_{\beta}({s,~a}),~{a_t} \sim \pi_\theta({a} \mid {s})}\left[\sum_{t=0}^{{H}} \gamma^{t} r_t\right]
\end{equation}
这里我们使用 $d_{\beta}$ 表示数据集 $D$ 中状态和动作的分布，其中动作根据行为策略 ${a_t}\sim \pi_\theta(a\mid{s})$ 采样得到。
折扣因子 $\gamma$ 用于折现累计奖励中的未来奖励。
$H$ 是一个轨迹的终止时间步长。

\mypara{例子}
\autoref{fig:running example}展示了一个模型学习“CartPole”任务的例子\footnote{\url{https://www.gymlibrary.dev/environments/classic_control/cart_pole/}}。
在数据收集过程中，数据集是基于操作员和环境之间的操作日志产生的。
其中包含小车和杆的位置和速度（即状态）、操作员的施力方向（即动作）以及相应的奖励。
然后，在训练和评估过程中，离线 RL 模型仅从收集的数据集中学习如何玩“CartPole”任务。
最后，我们将经过训练的离线 RL 模型部署到环境中执行任务。

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.8\hsize]{figure/orl_auditor/offline-rlch.pdf}
    \caption{离线 DRL 模型的运行示例。}
    \label{fig:running example}
\end{figure}


\subsection{离线强化学习模型}
\label{sec:offline reinforcement learning model}
在本节中，我们首先分别介绍两种离线 RL 算法~\cite{DBLP:conf/icml/FujimotoMP19, DBLP:journals/corr/abs-1910-01708, DBLP:conf/iclr/KostrikovNL22}，它们分别代表了离线 RL 模型的两种基本思想，即策略约束方法（Batch-Constrained Q-learning，BCQ）和值函数正则化方法（Implicit Q-Learning，IQL）~\cite{DBLP:journals/corr/abs-2203-01387}。
许多最先进的离线 RL 模型~\cite{DBLP:conf/nips/YuKRRLF21, DBLP:conf/nips/KidambiRNJ20, DBLP:conf/nips/FujimotoG21, DBLP:conf/iclr/KostrikovNL22}都是基于这两种方法进行改进。
接着，我们介绍一种新的极简且实用的算法，即融合行为克隆的时序差分学习方法（Temporal-Difference Learning 3 Plus Behavior Clone，TD3PlusBC）~\cite{DBLP:conf/nips/FujimotoG21}，该方法在较低的计算和超参数调试开销下实现了一流的性能。
此外，行为克隆方法（Behavior Clone，BC）~\cite{DBLP:conf/nips/Pomerleau88}通过有监督学习方式拟合数据集中的状态-动作分布。
尽管 BC 不属于典型的强化学习方法，但它可以解决离线 RL 问题，通常作为离线 RL 评估的基准方法。

\mypara{行为克隆方法\cite{DBLP:conf/nips/Pomerleau88}}
BC将数据集中成对的状态$s$和动作$a$分别作为输入和标签，基于\autoref{eq:BC}优化策略。
\begin{equation}
    \theta^*=\arg \min _\theta \mathbb{E}_{(s, a) \sim B}\left[\mathcal{L}\left(\pi_\theta(s), a\right)\right]
    \label{eq:BC}
\end{equation}
其中，$D$ 是离线强化学习数据集，$\mathcal{L}$ 是损失函数。
由于BC只是拟合数据集中状态-动作分布，其性能一般受限于采集数据集时所使用的策略。

\mypara{策略约束方法\cite{DBLP:conf/icml/FujimotoMP19, DBLP:journals/corr/abs-1910-01708}} 
BCQ是第一个具有实用意义的离线强化学习算法。
BCQ的关键思想是整合生成模型来实现批量约束的概念，即最小化候选动作与数据集中动作之间的偏差。
为了保持动作的多样性，BCQ构建了一个扰动模型来扰动每个选择的动作。
然后，BCQ通过一个$Q$网络选择具有最高价值的动作，该网络学习估计给定状态和动作对的预期累积奖励。
因此，BCQ的目标函数如下。
\begin{equation}
    \begin{aligned}
    \pi(s)= & \underset{a_i+\xi_\phi\left(s, a_i, \Phi\right)}{\operatorname{argmax}} Q_\theta\left(s, a_i+\xi_\phi\left(s, a_i, \Phi\right)\right) \\
    & \left\{a_i \sim G_\omega(s)\right\}_{i=1}^n
    \end{aligned}
\end{equation}
其中，$G_\omega(s)$ 是基于条件变分自编码器（Variational Autoencoder，VAE）\cite{DBLP:journals/corr/KingmaW13}的生成模型，$Q_\theta$是值函数，从$G_\omega(s)$生成的$n$个候选动作中选择价值最高的动作。
$\xi_\phi\left(s, a_i, \Phi\right)$是扰动模型，它可以在$[-\Phi,\Phi]$范围内输出对动作$a$的调整。
然后，可以通过确定性策略梯度算法\cite{DBLP:conf/icml/SilverLHDWR14}来优化扰动模型，如下所示。
\begin{equation}
    \phi \leftarrow \underset{\phi}{\operatorname{argmax}} \sum_{(s, a) \in \mathcal{B}} Q_\theta\left(s, a+\xi_\phi(s, a, \Phi)\right)
\end{equation}
其中，$\mathcal{B}$表示数据集中的小批量状态-动作对。
为了惩罚稀有状态，BCQ采用两个$Q$网络输出的凸组合，并设置一个新的目标值$y$来进一步更新这两个$Q$网络。
\begin{equation}
    y = r+\gamma \max _{a_i}\left[\lambda \min _{j=1,2} Q_{\theta_j^{\prime}}\left(s^{\prime}, a_i\right)+(1-\lambda) \max _{j=1,2} Q_{\theta_j^{\prime}}\left(s^{\prime}, a_i\right)\right], \nonumber
\end{equation}
其中，$a_i$对应于从生成模型$G_\omega(s)$中采样的扰动动作。

\mypara{值函数正则化方法~\cite{DBLP:conf/iclr/KostrikovNL22}}
与BCQ~\cite{DBLP:conf/icml/FujimotoMP19, DBLP:journals/corr/abs-1910-01708}的批次约束思想相比，
IQL采用了一种避免查询数据集中未见动作值的方法。
因此，IQL从考虑适应$Q$网络的方程\autoref{eq:sarsa}开始。
\begin{equation}
    L(\theta)=\mathbb{E}_{\left(s, a, s^{\prime}, a^{\prime}\right) \sim \mathcal{D}}\left[\left(r+\gamma Q_{\theta^\prime}\left(s^{\prime}, a^{\prime}\right)-Q_\theta(s, a)\right)^2\right], 
    \label{eq:sarsa}
\end{equation}
其中，$s^{\prime}$和$a^{\prime}$是$s$和$a$的接续状态和动作。
$\theta^\prime$是$\theta$的一个快照，每隔几次$\theta$的更新就从$\theta$中复制一次。
IQL通过expectile回归来优化状态值函数。
\begin{equation}
    L_V(\psi)=\mathbb{E}_{(s, a) \sim D}\left[L_2^\tau\left(Q_\theta(s, a)-V_\psi(s)\right)\right], \nonumber
\end{equation}
其中，$L_2^\tau(u)=|\tau-\mathbbm{1}(u<0)| u^2$。
最后，采用优势加权回归（Advantage-weighted Regression）更新策略函数。
\begin{equation}
    L_\pi(\phi)=\mathbb{E}_{(s, a) \sim \mathcal{D}}\left[\exp \left(\beta\left(Q_{\theta}(s, a)-V_\psi(s)\right)\right) \log \pi_\phi(a \mid s)\right]
\end{equation}
在IQL的工作流程中，首先交替优化参数$\theta$和$\psi$，然后在固定$\theta$和$\psi$的情况下更新$\phi$。

\mypara{融合行为克隆的时序差分学习方法~\cite{DBLP:conf/nips/FujimotoG21}}
先前的方法~\cite{DBLP:conf/icml/FujimotoMP19, DBLP:journals/corr/abs-1910-01708, DBLP:conf/iclr/KostrikovNL22} 通过限制或规范化动作选择来使学习到的策略更易于使用给定数据集进行评估。
然而，它们引入了新的超参数，并引入附加组件（例如生成模型）。
在应用时，需要对已部署的离线强化学习算法做出较大的调整。
TD3PlusBC是一种基于TD3~\cite{DBLP:conf/icml/FujimotoHM18}的离线强化学习算法，使用BC正则化项限制动作数据增强，将优化策略约束在接近数据集$D$的状态-动作空间。
\begin{equation}
    \pi=\underset{\pi}{\operatorname{argmax}} \mathbb{E}_{(s, a) \sim \mathcal{D}}\left[\lambda Q(s, \pi(s))-(\pi(s)-a)^2\right]
\end{equation}
对于包含 $N$ 个迁移 $\left(s, a\right)$ 的数据集，
$\lambda=\frac{\alpha}{\frac{1}{N} \sum_{\left(s, a\right)}\left|Q\left(s, a\right)\right|}$。
为了方便策略训练，TD3PlusBC 对给定数据集中的每个状态进行规范化，
其中 $s_i=\frac{s_i-\mu}{\sigma+\epsilon}$，$\mu$ 和 $\sigma$ 分别是均值和标准差。

在评估中，我们的实验主要针对以上四种算法进行。
然而，只要审核人员具有对可疑模型的黑盒访问权限，
\sysname 可以应用于任何类型的离线 DRL 模型。


\section{强化学习模型超参数}
\begin{table}[!t]
    \caption{超参数总结}
    \label{tab:hyper-parameter settings}
    \centering
    \resizebox{0.5\linewidth}{!}{
    \setlength{\tabcolsep}{0.6em}
    \begin{tabular}{ccc}
    % \hline
    \toprule
    \textbf{参数类型}  & \textbf{参数名称} & \textbf{参数集合}             \\ %\hline
    \toprule
    \multirow{3}{*}{\textbf{模型结构}} & \textbf{激活函数}      & ReLU, ELU, Tanh    \\ 
    % \rowcolor{mygray}
     & \textbf{隐含层数}   & 2, 3, 4            \\
     & \textbf{神经元数}   & 64, 128, 256       \\ 
    % \rowcolor{mygray}
    \toprule
    \multirow{2}{*}{\textbf{模型优化}} &\textbf{优化器}       & SGD, Adam, RMSprop \\
     & \textbf{学习率}   & 1e-3, 5e-4, 1e-4   \\ 
    % \rowcolor{mygray}
    \toprule
    \textbf{RL特有参数} & \textbf{Gamma}           & 0.9, 0.95, 0.99    \\ %\hline
    \bottomrule
    \end{tabular}
    }
    % \vspace{-0.3cm}
\end{table}
DRL模型通过调整DNN模型来拟合行为策略，
因此，按照超参数对DRL模型的不同影响，
DRL模型的超参数可以分为三个部分：DNN模型结构超参数、DNN模型优化超参数以及RL模型特有参数。
在本章中，我们主要关注六种DRL模型超参数（参见\autoref{tab:hyper-parameter settings}），
% 其中包括先前DNN超参数窃取攻击\cite{OASF18}中的超参数和RL模型的折扣因子$\gamma$。
这些超参数常作为DRL模型训练的默认可调参数。
例如，亚马逊公司的DeepRacer实验平台\cite{BMGGDKRSTTCMK19}开放这些超参数的配置接口供用户调整DRL模型。

我们在本章没有考虑更复杂的神经网络结构，主要原因是过大或过深的神经网络对会使DRL模型难以收敛~\cite{ota2020can}。
首先，相比于监督学习可以在整个训练结束后使用已训练好的模型，深度强化学习模型需要少量的训练后立即使用刚刚训练好的策略，这限制了DRL只能使用相对较浅的神经网络以确保快速拟合。
其次，深度强化学习模型的训练数据不如监督学习稳定，并且不能将训练数据分出验证集以避免过拟合。
因此，DRL模型不能使用过于宽阔的网络以避免由于参数过度冗余而产生的过拟合。
最后，DRL模型对超参数的选择比较敏感，一定程度限制了超参数调整的范围~\cite{henderson2018deep}。





% \section{神经网络模型}
% \section{深度强化学习}

% \section{差分隐私技术}
% \subsection{集中式差分隐私技术}
% \subsection{本地差分隐私技术}



\chapter{原始数据保隐私分析技术}
% 为了保护用户的私人数据，局部差分隐私（LDP）已被用来提供保护隐私的范围查询，从而支持进一步的统计分析。然而，现有的基于LDP的范围查询方法受到其属性的限制，即根据预先定义的结构来收集用户数据。这些静态的框架会招致过多的噪音添加到汇总的数据中，特别是在低隐私预算的设置中。在这项工作中，我们提出了一个自适应分层分解（\myahead ）协议，它可以自适应地、动态地控制所建的树状结构，从而使注入的噪声得到很好的控制，以保持高效用。此外，我们还得出了正确选择\myahead 参数的指导原则，以便在严格满足LDP的同时，使整体效用能够持续具有竞争力。利用多个真实和合成数据集，我们广泛地展示了\myahead 在低维和高维范围查询场景中的有效性，以及其相对于最先进方法的优势。此外，我们为在实践中部署\myahead 提供了一系列有用的意见。
本章摘要: 
在数据收集和初步分析阶段，为了保护的用户敏感数据的隐私，我们基于本地差分隐私（LDP）设计了一种自适应层级分解的范围查询分析协议（\myahead ）。该方法克服了现有基于LDP的范围查询分析方法普遍存在的缺陷，即根据预先定义的静态编码方式来收集分析用户数据。在保隐私的数据收集过程中，这些静态编码方式可能会引入过量的扰动噪声。相比于现有方法，\myahead 可以自适应地、动态地调整数据收集过程中的编码方式，从而使注入的噪声强度得到很好的控制，在保护用户隐私的同时，保持了分析结果的高效用。
另外，我们通过分析扰动噪声的特性，提供了选择\myahead 超参数的理论方法，以便在严格满足LDP的同时，保证\myahead 具有优异的整体效用。我们使用多个真实数据集和合成数据集，综合展示了\myahead 在低维和高维范围数据范围查询分析任务中的有效性，以及相比于现有方法的优势。
最后，基于定性分析和实验结果，我们总结了一些规律以便于在实际中部署\myahead 。
总之，我们提出的方法很好地解决了现有方法的编码方式的局限性，实现了在相同隐私保护强度下，更准确的统计分析结果。

关键词：本地差分隐私；范围查询分析；自适应层级分解

\section{引言} 
% 随着 Facebook [44]、Marriott [47]、Exactis [23] 等数据泄露事件的增多，用户隐私已成为许多实际应用中的严重障碍。 作为一种有前途的对策，差分隐私 (DP) [16、17] 已被学术界和工业界接受为保护数据隐私的事实标准 [18、28、32、33、41、49、74]，因为它具有 严格的理论保证和攻击者背景知识的独立性。 集中设置中的 DP 需要一个可信的聚合器，该聚合器从用户那里收集敏感数据并执行扰动分析，然后通过回答查询或发布合成数据来提供数据服务 [22, 79]。
近年来，数据泄露事件不断发生。例如，万豪国际泄露3.39亿条用户隐私，涉及客户姓名、电子邮件地址、电话号码、护照号等隐私信息[47]；淘宝近12亿条用户数据泄露，包含淘宝用户的数字ID、淘宝昵称、手机号码等淘宝客户信息共计11.81亿条。 
因此，用户数据的隐私保护在许多实际应用中已成为不可或缺的组成部分。
作为一种具有严格理论保证和对攻击者背景知识无关的隐私保护方法，差分隐私 (DP) [16, 17] 已成为学术界和工业界保护用户数据隐私的黄金准则之一 [18, 28, 32, 33, 41, 49, 74]。传统的集中式差分隐私方法需要一个可信的聚合器从用户那里收集敏感数据并执行扰动分析，然后通过回答查询或发布合成数据提供保隐私的数据服务[22, 79]。

然而，在实际中假设具有一个可信的聚合器是困难的，因为用户通常不愿意在没有保护的情况下共享他们的私人数据。为了解决这个问题，文献[15,55]提出了本地式差分隐私（LDP）。
相比于集中式差分隐私，LDP允许用户在本地对其私人数据进行编码、扰动和上传。
近年来，谷歌[20,21]、苹果[13]和微软[14]都将LDP应用于保隐私的数据收集分析。
例如，谷歌基于LDP收集用户的热门主页，而苹果则使用LDP分析用户的表情符号偏好。
基于LDP的保隐私数据收集分析方法[6、7、20、59、61]主要集中在获得整个数据域的频率分布(FO)[65]。然而，在实际应用中，人们可能更感兴趣的是特定数据域范围内的用户频率值。例如，大型商场往往需要掌握其顾客中高收入群体的比例，以进行更有针对性的商业决策。

对于范围查询分析任务，现有的解决方案可以按照敏感数据的维度分为两类。对于低维（数据维度$\leq 2$）查询场景，Wang等人[62]提出了基于完全$B$叉树的静态结构来编码整个数据域，并通过累加叶子节点的频率值来完成范围查询任务。Cormode等人[11]提出应用离散小波变换将每个用户的敏感数据转换为Haar小波系数向量进行扰动，并在聚合器侧执行逆变换以恢复成用户频率完成后续范围查询任务。
对于高维（数据维度\textgreater~2）范围查询，Yang等人[73]提出了结合来自1、2维网格的信息的方法，并利用加权更新策略来估计高维范围查询。然而，现有方法存在几个局限性。首先，大多数实际数据集的数据域存在稀疏区域。例如，50-60岁的会员在足球俱乐部中占很小的比例。因此，在完全树（网格）中具有小值的节点（单元格）很可能会被注入的噪声淹没。此外，现有技术主要是针对特定维度的查询设计的，即[11、62]适用于1、2维查询，[73]适用于高（≥2）维查询。虽然[11、62、73]在技术上没有受到查询维度的限制，但在非目标维度的情况下效果较差。由于数据集的维度在实践中各不相同，因此聚合器需要结合不同场景的算法，这限制了这些算法的适应性和适用性。


% 为了抑制过量的扰动噪声，我们提出了一种自适应数据域分解的方式来编码、扰动用户敏感数据，其核心是根据用户频率来
% 提供了一种精细的域分解机制来容纳树中不同粒度节点的注入噪声。
% 为了抑制过多的注入噪声，\myahead 提供了一种精细的域分解机制，以适应树中各种粒度节点的注入噪声。。
相比于现有工作，\myahead 中引入了自适应数据域分解机制，以调节扰动过程中的注入噪声。
% 为了使\myahead 能够找到合适的域分解，我们仔细分析了\myahead 获得的查询答案的错误来源，并提供了获得分解的指导原则。
% 在\myahead 完成与所有用户的交互后，对节点的价值存在一定的约束，例如，子节点的价值之和等于其父节点的价值。
首先，为了保证数据域分解的合理性，我们通过分析\myahead 查询结果的误差来源，得到了算法关键超参数的设置方法。
其次，在\myahead 完成用户数据聚合之后，我们基于统计结果的物理约束设计了相应的后向处理方法，以进一步提高查询准确性。
例如，在\myahead 的树结构中，子节点的值之和等于它们的父节点的值。
最后，对于高维范围查询分析场景，我们比较了两种不同的扩展方法，即直接估计（DE）和利用低维度估计（LLE），并根据实验结果展示了LLE的优势。

我们在多个真实和合成数据集上评估\myahead 的性能，并阐述\myahead 相比于现有方法的优势。
具体来说，在所使用的真实数据集上，\myahead 可以将查询结果的误差显著降低，最高可以达到两个数量级的降低幅度。
对于低维场景，我们全面测试了关键超参数对于\myahead 性能的影响，例如隐私预算、数据域大小、用户规模和用户数据分布特性。
基于757个不同参数组合的实验结果，我们总结出一系列有利于\myahead 实际部署的结论。
对于高维场景，我们研究了\myahead 在不同数据维度和属性相关性下的查询精度，并展示其变化特性。

总的来说，本文的贡献有三个方面：
\begin{itemize}
    \item 我们提出了一种基于自适应数据域分解机制的算法。\myahead 可以在满足LDP前提下进行用户数据的范围查询分析。因为\myahead 可以自适应地调整数据域分解的粒度，所以\myahead 可以有效降低注入噪声对范围查询结果的影响，相比于现有方法具有明显的精度提升。
    \item 我们通过分析查询结果中的误差来源，提供了\myahead 关键超参数的设置依据，保证了\myahead 的高效用。此外，我们将\myahead 成功推广至高维范围查询场景。
    \item 通过多个真实世界数据集上的实验评估，我们阐明了\myahead 在查询结果精度方面相比于现有方法具有明显的优势。
    此外，我们还从实验结果中总结出多个可以指导\myahead 实际部署的结论。
\end{itemize}


% \section{预备知识}
% \subsection{本地差分隐私}
% 在LDP中，每个用户通过扰动机制 $\Psi$ 扰动其私有数据 $v$，然后将 $\Psi(v)$ 发送给聚合器，
% 同时满足以下严格的LDP保证：
% % \begin{definition}
% %     $\epsilon$-Local Differential Privacy ( $\epsilon$-LDP ) \cite{kasiviswanathan2011can}. 
% % \end{definition}

% \begin{definition}{$\epsilon$-本地差分隐私（$\epsilon$-LDP~\cite{kasiviswanathan2011can}）} 
%     对于任意 $\epsilon > 0$ 和可能的输入对 $v_1$, $v_2 \in D$，
%     扰动函数 $\Psi(\cdot)$ 满足$\epsilon$-LDP当且仅当：
%     \begin{equation}
%         \forall T \in \operatorname{Range}(\Psi): \Pr{\Psi\left(v_{1}\right) \in T} \leq e^{\epsilon} \Pr {\Psi\left(v_{2}\right) \in T},  \nonumber
%     \end{equation}
%     其中 $\operatorname{Range}(\Psi)$ 表示 $\Psi$ 的所有可能输出构成的集合。
% \end{definition}

% \subsection{保隐私频率估计算法}
% 频率估计（Frequency Oracle，简称\fo）协议用于估计私有属性上的频率分布$F$，是一种LDP任务的基本构建块之一，如边际发布\cite{zhang2018calm}和范围查询\cite{wang2019answering, cormode2019answering}。
% \fo 协议一般包括三个步骤，即编码、扰动和聚合\cite{wang2017locally}。
% 接下来，我们介绍两种最先进的 \fo 协议。 

% \subsubsection{广义随机响应（\grr）}
% \grr 算法是随机响应\cite{warner1965randomized}的一种广义版本。

% \mypara{编码} \grr 直接扰动私有值 $v$，因此对于用户 $i$，编码值 $x_i$ 等于 $v_i$。

% \mypara{扰动} 用户 $i$ 以概率 $p = \frac{e^{\epsilon}}{e^{\epsilon}+|D|+1}$ 保留 $v_i$，并以概率 $q=\frac{1}{e^{\epsilon}+|D|+1}$ 随机选择 $v_i^{\prime} \in D$，其中 $v_i \neq v_i^{\prime}$，然后将 $x_i^{\prime}$ 上传到服务器，其中 $x_i^{\prime}$ 为 $x_i$ 扰动后的值。

% \mypara{聚合} 聚合器计算 $v$ 被报告的次数，表示为 $\operatorname{count}[v] = \sum_{i = 1}^{N} \mathbb{I}_{{x^{\prime}=v}}$。$v$ 频率的无偏估计为
% $\hat{f_v} = \frac{\operatorname{count}[v]- Nq}{N(p-q)}$。

% \mypara{估计误差}
% $\hat{f_v}$是真实频率$f_v$的无偏估计 \cite{wang2019answering}。因此，\grr 的估计误差源自于算法统计方差，即
% \begin{equation}
% \myvar_{\grr(\epsilon)} = \frac{|D| - 2 + e^{\epsilon}}{N\left(e^{\epsilon}-1\right)^{2}}
% \label{GRRVAR}
% \end{equation}

% \subsubsection{优化一元编码（\oue）}
% \label{myoue}
% \oue \cite{wang2017locally} 是基于 \rappor 协议\cite{erlingsson2014rappor} 的优化算法。

% \mypara{编码}
% 用户 $i$ 将其私有数据编码为一个独热二进制向量，即 $ x_i = [0,\ 0,\ \ldots,\ 1,\ \ldots,\ 0]$，长度为 $|D|$，其中只有第 $v_i$ 个位置是 $1$。

% \mypara{扰动}
% 用户 $i$ 根据概率 $p = \frac{1}{2}$ 和 $q = \frac{1}{e^\epsilon + 1}$ 翻转 $x_i$ 的每个比特。
% 其中，比特 1 和 0 以不同的概率翻转，即1（或 0）以概率 $p$（或 $1-q$）保持不变，以概率 $1-p$（或 $q$）翻转为相反的值。
% 然后，用户 $i$ 将 $x_i^{\prime}$ 上传到服务器。

% \mypara{聚合}
% 聚合器收集用户上传的 $\{x_i^{\prime}\}_{i=1}^{N}$，并计算每个比特位中 1 的出现次数，例如，对于第 $v$ 个比特，需要对 $\operatorname{count}[v] = \sum_{i = 1}^{N} x_i^{\prime}[v]$ 进行校正，以获得无偏估计 $\hat{f}[v] = \frac{\operatorname{count}[v] - Nq}{N(p - q)}$。

% \mypara{估计误差}
% 在 \cite{wang2017locally} 中证明了 \oue 具有方差
% \begin{equation}
% \myvar_{\oue(\epsilon)} = \frac{4e^{\epsilon}}{N\left(e^{\epsilon}-1\right)^{2}}
% \label{OUEVAR}
% \end{equation}
% \grr 和 \oue 都实现了频率值的无偏估计。
% 如 \autoref{GRRVAR} 和 \autoref{OUEVAR} 所示，\oue 具有独立于 $|D|$ 的方差。
% 对于较小的 $|D|$（例如，$|D|-2<3e^\epsilon$），\grr 更好；对于较大的 $|D|$，\oue 更优。


\section{问题建模及现有工作}
\subsection{问题建模}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\hsize]{ldp_range_query/figures_others/multi-dimensionalDataset.pdf}
    \caption{示例数据集包含 $N$ 个用户以及每个用户的三个属性值：年龄，收入和已贷款额度} 
    \label{Example dataset}
\end{figure}

% Assume there are $N$ users, where the $i$-th user has an $m$-dim ordinal record ${\bf{v}}^{m}_i = (v^{(1)}_i, v^{(2)}_i, \cdots, v^{(m)}_i)$, with $v_i^{(j)}$ representing the $j$-th private attribute value owned by user $i$. 
假设参与数据收集的有 $N$ 个用户，其中第 $i$ 个用户拥有一个 $m$ 维数据记录 ${\bf{v}}^{m}_i = (v^{(1)}_i, v^{(2)}_i, \cdots, v^{(m)}_i)$，其中 $v_i^{(j)}$ 表示用户 $i$ 拥有的第 $j$ 维的属性值。
将第 $j$ 个属性值的数据域表示为 $D_j$，
给定一系列范围 $\alpha_j$，$\beta_j$（$j=1,2,\cdots,m$），可以计算出一个 $m$ 维范围查询结果。
\begin{equation}
    R_{\bigcap {[\alpha_j,\beta_j]}_{j=1}^{m}}=\frac{1}{N} \sum_{i=1}^{N} \mathbb{I}_{\bigcap{\{\alpha_j \leq v_i^{(j)} \leq \beta_j\}_{j=1}^{m}}}
\end{equation}
其中 $\mathbb{I}_\phi$ 是一个指示函数，如果谓词 $\phi$ 成立则取值为1，否则为0。
% For example, the proportion of people within 20 years to 40 years old constitutes a 1-dim range query, 
% while the ratio of people within 20 years to 40 years old, with salary less than 5000, and with loan amount less than 20000 constitutes a 3-dim range query, 
% where the three dimensions corresponding to age, salary and loan amount, respectively.
\autoref{Example dataset} 给出了一个范围查询的运行示例。
例如，统计年龄在20岁到40岁之间的用户在群体中所占比例是一个一维范围查询，
而统计同时满足年龄在20岁到40岁之间、工资低于5000元和已贷款金额低于20000元的用户在群体中所占比例是一个三维范围查询。

\subsection{分层-间隔优化方法（\myhio）}
\label{exsitingHIO}
基于$B$叉树，\myhio \cite{wang2019answering}将整个数据域分解成互不相交的子集，称为“间隔”。
根节点表示整个数据域，叶节点表示单个值。同一层上的节点表示相同粒度的间隔。然后，\myhio 通过\oue \cite{wang2017locally}算法获得每层节点的频率估计。
当回答范围查询时，\myhio 使用来自不同层的最少间隔完全覆盖查询范围。
例如，当用户的私有属性域大小$|D|=8$，树分支因子$B=2$时，范围查询$[2,7]$可以分解为间隔$[2,3] \cup [4,7]$。
然后，\myhio 将上述两个间隔的估计频率值相加，以回答范围查询。
对于长度为$r$的一般查询，\myhio 最多可以使用$2(B-1)\log_B|D|$个间隔来回答。
与直接使用\fo 机制相比，\myhio 在回答查询时可以有效地减少使用的间隔数量，从而大大减少由于在范围内添加嘈杂的频率值所引起的累积误差。

但是，\myhio 存在两个限制其在实践中适用性的弱点。
1）\myhio 将相同水平的噪声插入到所有间隔的估计频率中。对于间隔较小的节点，扰动噪声往往会掩盖真实频率值，从而降低整个算法的效用。
2）在多维场景中，树层数随维数指数增加。对于高维场景，查询误差随着过多的小值节点而明显地增加。

\subsection{离散哈尔小波变换方法（\mydht）}
\label{exsitingDHT}
\mydht \cite{cormode2019answering} 在域上施加完全二叉树结构，并将用户私有值 $v$ 编码为一组哈尔小波系数。 
\mydht 的动机是，在长度为 $r$ 的范围查询中，相比直接应用\fo ，使用哈尔小波域中估计的值数量更少。

\mydht 也面临一些限制：
1）与 \myhio 相似，\mydht 将相同级别的噪声插入到所有估计的哈尔小波系数中。
对于一些数值较低的系数，噪声倾向于使估计系数偏离实际值，导致查询误差增加；
2）它主要设计用于一维场景，因此在实际应用中存在限制。


\subsection{高维边缘列联表发布方法（\mycalm）}
\label{Consistent Adaptive Local Marginal}
\mycalm \cite{zhang2018calm} 是一种边缘列联表发布 LDP 协议，可在保护隐私的同时构建 $m$ 个属性的联合分布。
\mycalm 不是直接估计包含所有属性边际表，而是根据用户数据域的大小和数据维度的数量进行策略性的属性组合统计，基于属性组合的统计结果可以重建包含所有属性的边际表。
我们注意到，\mycalm 可以用于回答范围查询。
更具体地，为了回答一个多维范围查询，\mycalm 可以将查询包含的重建的边际值求和。

然而，当域大小 $|D|$ 很大时，\mycalm 需要将大量经过扰动的边际值求和以回答范围查询，这可能会向结果中引入过量的噪声。

\subsection{混合维度网格方法（\myHDG）}
\myHDG \cite{yang2020answering}是在LDP场景下回答多维范围查询的现有最好方法。
\myHDG 首先将用户所有属性进行两两组合形成二维数据域，然后基于用户数据域大小将二维数据域精心分桶成粗略的二维网格，并统计每个子数据域内的用户频率。
之后，\myHDG 基于二维范围查询的结果估计出更高维范围查询结果。
为了刻画用户数据的细粒度分布信息，\myHDG 使用粒度更细的一维网格统计每个用户属性的频率分布信息，并结合来自一维和二维网格的信息来回答高维范围查询。

然而，\myHDG 存在下列缺陷：
1) \myHDG 的等粒度网格无法处理各种分布的用户数据。
对于分布较为偏斜的数据集，其用户数据集中在整个数据域的一小部分。因此，在某些网格中噪声误差或非均匀误差会占主导地位，从而降低结果精度；
2) 使用一维网格可能会破坏用户数据中不同属性之间的相关性。

\subsection{总结}
\label{Remarks}

为了克服现有低维机制(\myhio、\mydht)和高维机制(\mycalm、\myHDG)的局限性，我们旨在实现以下两个设计目标:
1）找到合理的区间分解方法，避免引入过多的噪声；
2）设计的机制可以扩展到多维场景，比现有算法具有更好的查询精度。
在这些目标的推动下，我们提出了\myahead，与现有工作存在以下主要差异：
1）与现有方法采用的静态数据编码机制相比，\myahead 采用的是一种自适应、动态的数据编码机制；
2）\myahead 通过合并区间减少噪声对小值节点的影响；
3）所设计的机制可以从一维迁移到多维场景。
接下来，我们将详细阐述\myahead 的动机和设计。

\section{本文方法（\myahead）}
\label{Adaptive Hierarchical Decomposition}

\subsection{设计思路}
\label{Motivation and Overview}
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.75\hsize]{figure/ldp_range_query/figures_others/Motivation3ch.pdf}
    % \vspace{-0.2cm}
    \caption{基线策略与自适应策略}
    \label{Method Motivation}
    % \vspace{-0.3cm}
\end{figure} 

\begin{figure*}[t]
    \centering
    \includegraphics[width=\hsize]{figure/ldp_range_query/figures_others/OverviewAHEAD4ch.pdf}
    % \vspace{-0.4cm}
    \caption{\myahead 的工作流程。从左到右依次展示了\myahead 算法中的四个步骤，即用户分组、频率估计、数据域分解和后向处理。 
    \myahead 根据右侧子图中的树回答范围查询。} 
    \label{fig:ahead-method}   
    % \vspace{-0.4cm}
\end{figure*}

在这个章节中，我们使用一个例子来说明现有算法的局限性以及\myahead 的合理性。
如\autoref{Method Motivation}所示，左侧的表格显示了带有相应实际频率值的数据区间。
例如，区间$[0,1]$的真实频率值为$0$，意味着这个区间内没有用户数据。
然后，中间的表格显示了使用不同策略分别估计的区间频率值。
其中，$\sigma^2$代表扰动过程中引入的噪声方差。
右侧的剩余部分展示了基于估计值回答查询的过程。

首先，我们关注基线策略，例如\myhio。
基线策略选择发布每个区间的估计频率值。
每个估计值通过\fo 机制引入了“噪声误差”，以满足LDP的要求。
对于区间$[0,1]$，它的真实值为$0$，这意味着这些区间的估计值完全被噪声填充。
当回答范围查询时，例如区间$[0,5]$的用户频率，提问者想知道区间$[0,5]$的频率值。
基线策略的结果是$0.5 + 3\sigma^2$。
在这个例子中，区间$[0,1]$的真实值并未对查询答案做出贡献，却带来了同等程度的噪声，降低了基线策略的查询准确度。

另一方面，对于自适应策略，如果我们知道各区间的真实频率值，就可以合并区间$n_0$和$n_1$，只需要估计合并后的区间$n_p$。
当需要输出区间$[0,5]$的用户频率时，自适应策略的结果是$0.5 + 2\sigma^2$，与基线方法相比，将噪声误差降低了30\%。

自适应策略可以有效减小扰动噪声对频率值较小区间的影响。
但是，当查询需要使用合并区间的部分区域时，自适应策略必须通过对合并区间内频率分布的作出假设才能进行后续的计算。
主流方法是假设合并区间内的频率分布满足均匀分布\cite{ioannidis2003history}，即合并区间用户频率均匀分布。
如果实际用户频率不满足该假设时，自适应策略的结果将会产生“非均匀误差”。
例如，如果查询者想要知道区间$[2,3]$的用户频率，那么自适应策略将基于两个区间的尺寸在均匀分布的假设下进行计算，即 $[0,3]$ 区间频率值的一半。
与基准策略相比，自适应策略将噪声误差从 $\sigma^2$ 降低到 $\frac{\sigma^2}{2}$，
同时它也带来了非均匀误差 $0.05 - \frac{0.05}{2} = 0.025$。

自适应性通过合并区间来减少噪声误差，同时通过假设均匀分布引入了非均匀误差。
因此，我们希望找到最优的数据域分解来平衡这两种误差，进而减少总查询误差。
文献\cite{muthukrishnan1999rectangular}指出即便是在二维数据场景下，找到最优的数据域分解是困难的，那么在隐私约束下这个问题将变得更具挑战性。 
综上，我们提出了一种基于自适应层级递归划分策略（详见\autoref{Workflow of ahead}），旨在平衡噪声误差和非均匀误差，并解决现有方法的不足。

\subsection{一维数据场景\myahead 的工作流程}
\label{Workflow of ahead}
在本章，我们通过\autoref{fig:ahead-method}中的例子阐述\myahead 的工作流程。
聚合器希望基于\myahead 完成有关用户收入的范围查询任务。
收入数据被分成了8个有序级别，即域大小$|D|=8$。
树的扇出度$B=2$，这意味着\myahead 树的每个节点最多有两个子节点。
在\myahead 原型树的每个节点中，$n_i$表示节点索引，$[a,b]$表示节点的区间，$\hat{f_i}$表示估计的频率值。
值得注意的是，\myahead 采用了抽样原则\cite{cormode2019answering}，将全部参与用户划分为子用户组，每组用户使用全部隐私预算。
抽样原则可以显著减少本地差分隐私场景中查询结果的噪声误差\cite{nguyen2016collecting, wang2017locally, wang2019locally}
% （有关更多详细信息，请参见\autoref{The rationality of sampling principle}）。
接下来，我们将\myahead 的工作流程分为四个步骤。

\mypara{第一步：用户分组}
如\autoref{fig:ahead-method}中所示的左虚线框中，
聚合器确定分区数$c$，其中$c=\log_B|D|$旨在确保将用户分配到\myahead 树的每一层。
用户随机选择在范围$[1,2,3,\cdots,c]$内的组号。
此外，用户还可以利用他们的公共信息来选择组，例如帐户注册时间，用户ID等。
分区过程应确保每个组代表整体人口，具有相似数量的用户。

\mypara{第二步：频率估计}
在中间的虚线框中，聚合器首先建立代表整个数据域的根节点$n_0$。
然后，聚合器对域进行初始分解，即将整个域分成$B$个相等大小的区间，然后将区间节点连接到根节点$n_0$上。
根节点的子节点代表一种将整个数据域分割的方法，表示为数据域分解$E_1$。
聚合器选择第一组用户，并向他们发送分解$E_1$和隐私预算$\epsilon$。
第一组用户中的每个用户将自己的私有值$v$投影到$E_1$的区间上，并通过\oue 上传$v$的投影值。在接收到用户报告后，聚合器使用聚合算法获得估计的频率分布$\hat{F_1}$，表示落在每个节点区间内的用户比例。

\mypara{第三步：数据域分解}
聚合器将$\hat{F_1}=\{\hat{f_1}, \hat{f_2}\}$的每个频率值与阈值$\theta$进行比较，并决定是否进一步分割$E_1=\{[0,3], [4,7]\}$中的相应区间。
具体来说，由于$\hat{f_2}$大于设定的$\theta$，因此应将$E_1$中的相应区间$[4,7]$分成$B$个大小相等的子区间$[4,5]$和$[6,7]$。
而节点$n_1$的频率值$\hat{f_1}$小于$\theta$，那么区间$[0,3]$不需要进一步划分。
对于新的区间节点，我们将它们附加到相应的父区间节点上。
当$\hat{F_1}$中的所有元素都被遍历时，我们可以获得一组新的数据域分解$E_{2}$。

然后，聚合器将分解$E_{2}$发送给第二组用户并获得估计的频率分布$F_2$。聚合器重复以上步骤，直到应用所有用户组并获得\autoref{fig:ahead-method}中显示的\myahead 原型树。由于估计频率小于阈值$\theta$，\myahead 不会在后续交互中分解区间$[0,3]$和$[6,7]$。
为了满足LDP要求，区间$[0,3]$和$[6,7]$依然参与后续用户组的频率估计。

在构建原型树时，\myahead 分别估计每个层的频率分布。
在这个过程中，\myahead 没有考虑树中频率值的约束，例如子节点的频率值之和等于其父节点。
因此，在步骤四中，我们通过对节点估计进行“非负性”和“加权平均”处理，来进一步提高\myahead 查询结果的准确性。

\mypara{第四步：后向处理}
% 后向处理包含两个步骤：非负性和加权平均。
首先是对节点频率值进行非负性处理，\myahead 使用Norm-Sub \cite{wang2019consistent}对同一层中的节点进行处理，以确保节点的估计频率为非负数且频率总和为1。
\myahead 将负值转换为0，并计算正值的总和与1之间的差异。
接下来，每个正值减去平均差异，该平均差异是通过将总差异除以正估计值的数量获得的。
非负性处理过程重复进行，直到所有节点的频率值都变为非负数。

然后\myahead 从下往上对所有节点的频率值进行加权平均。
\myahead 计算非叶节点$n$和其子节点频率值的加权平均值，以更新$n$的估计频率，即通过融合$n$的多个估计结果来降低扰动噪声的影响。

\begin{equation}
    \label{weighted average}
    \tilde{f}(n)=\left\{
    \begin{array}{rcl}
    \lambda_1\hat{f}(n) + \lambda_2 \sum_{u \in child(n)}\hat{f}(u), &   \mbox{如果$u$是一个叶子节点} \\
    \lambda_1\hat{f}(n) + \lambda_2 \sum_{u \in child(n)}\tilde{f}(u),    & \mbox{其他情况}  \\
    \end{array}\right.
\end{equation}

权重 $\lambda_1$ 和 $\lambda_2$ 与估计值的方差成反比，即 $\lambda_1 = \frac{\myvar_{child(n)}}{\myvar_{child(n)} + \myvar_{(n)}}$ 和 $\lambda_2 = \frac{\myvar_{(n)}}{\myvar_{child(n)} + \myvar_{(n)}}$，其中 $\myvar_{child(n)}$ 表示节点 $n$ 的子节点方差之和，$\myvar_{n}$ 表示节点 $n$ 的方差。
$\tilde{f}$ 表示 $\hat{f}$ 的后向处理版本，将用于回答查询。
% 加权平均过程可以最小化噪声的幅度，如下定理所示。 \autoref{theorem: weighted average} 的证明可以在 \autoref{The proof of weighted average} 中找到。

\begin{theorem}
    \label{theorem: weighted average}
    使用{\rm\autoref{weighted average}}来更新节点的频率，节点$n$可以达到最小扰动方差。
\end{theorem} 
\begin{proof}
    根据文献{\rm\parencite{wanglocally}}，经过扰动后的区间估计频率值$\hat{f}$近似于$f+X$，其中$f$是真实的频率值，$X$是一个服从\Gaussian 分布$\mathcal{N}(0,\myvar_{\oue})$的随机变量。
    假设$u$为叶子节点，那么有
    \begin{align}
    \hat{f}(n) = f(n) + \mathcal{N}(0,\myvar_{(n)}) \nonumber
    \end{align}
    \begin{align}
    \sum_{u \in child(n)} \hat{f}(u) = \sum_{u \in child(n)}f(u) + \mathcal{N}(0,\myvar_{(u)}) \nonumber
    \end{align}
    节点$n$的区间等于其子节点区间的组合。因此
    \begin{align}
    f(n) = \sum_{u \in child(n)}f(u) \nonumber
    \end{align}
    设加权平均的系数为$\lambda_1$和$\lambda_2$，$f(n)$和$\sum_{u \in child(n)}f(u)$之间的加权平均值是节点$n$频率的无偏估计，其中$\lambda_1+\lambda_2=1$。
    \begin{align}
    \tilde{f}(n)=\lambda_1\hat{f}(n)+\lambda_2\sum_{u \in child(n)}\hat{f}(u) \nonumber
    \end{align}
    $\tilde{f}(n)$的方差等于$\lambda_1^2\myvar_{(n)}+\lambda_2^2\myvar_{child(n)}$。
    当且仅当
    \begin{align}
    \lambda_1=\frac{\myvar_{child(n)}}{\myvar_{child(n)}+\myvar_{(n)}} \nonumber,~~~~
    \lambda_2=\frac{\myvar_{(n)}}{\myvar_{child(n)}+\myvar_{(n)}}，\nonumber
    \end{align}
    我们可以最小化$\tilde{f}(n)$的方差。
    加权平均后，扰动噪声仍服从\Gaussian 分布。
    因此，当$u$不是叶子节点时，上述证明仍然成立。
\end{proof}

最后，从根节点到叶子节点，\myahead 基于均匀分布假设递归地分解父节点的频率值，并获取完整的层级分解树以回答范围查询，具体如\autoref{fig:ahead-method} 的最右边子图所示。

\begin{algorithm}[!htbp]
	\begin{algorithmic}[1]
        \REQUIRE 用户私有值 $V=\{v_1, \ldots, v_N\}$，数据域 $D$，树扇出度 $B$，隐私预算 $\epsilon$，阈值 $\theta$。
        \ENSURE 经过后向处理的\myahead 层级分解树 $T$
		\STATE $c = {\log_B}|D|$
		\STATE $//$ 用户划分
        \STATE 随机将所有用户划分为 $c$ 个用户组 $\left\{ V_1, V_2, \ldots, V_c \right\}$
        \STATE 创建表示整个数据域的树 $T$的根节点 $e^0_0 = [1, |D|]$，并初始化其频率值$T.\operatorname{node}(e^0_0).\operatorname{frequency} = 1$
		\FOR {$i$ from 1 to $c$}
            \STATE $//$ 数据域分解
            \FOR {$j$, node in enumerate($T.\operatorname{node}(\operatorname{level}=i-1)$)}
            \IF {$\operatorname{node.frequency} > \theta$}
            \STATE 将区间 $e^{i-1}_j$ 分解为 $B$ 个不相交的子区间 $\{e^{i-1}_{j,k}\}$
            \FOR {$k$ from $1$ to $B$}
    		\STATE $\operatorname{node.add\_child}(e^{i-1}_{j,k})$
            \ENDFOR
            \ELSE
            \STATE $\operatorname{node.add\_child}(e^{i-1}_{j})$
            \ENDIF    		
            \ENDFOR
            \STATE $//$ 频率估计
            \STATE $F = \fo(V_i, T.\operatorname{node}(\operatorname{level}=i).\operatorname{interval}, \epsilon)$
            \FOR {$k$, node in enumerate $T.\operatorname{node}(level=i)$}
    		\STATE $\operatorname{node}.\operatorname{frequency} = F[k]$
            \ENDFOR
        \ENDFOR
        \STATE $//$ 后向处理
        \STATE 运行 \autoref{alg:1-dim post_processing}
        \STATE 返回 $T$
	\end{algorithmic}
    \caption{构建一维\myahead 层级分解树}
    \label{Construct 1-dim prototype tree}
\end{algorithm}

\begin{algorithm}[!ht]
	\caption{后向处理}
	\label{alg:1-dim post_processing}
	\begin{algorithmic}[1]
        \REQUIRE \myahead 层级分解树 $T$, 树扇出度 $B$
        \ENSURE 经过后向处理的\myahead 层级分解树 $T$
        \FOR {$i$ from $1$ to $c$}
		\STATE norm\_sub($T.\operatorname{node}(\operatorname{level}=i).\operatorname{frequency}$)
        \ENDFOR    
        \FOR {$j$ from $c-1$ to $1$}
            \FOR {\_, node in enumerate $T.\operatorname{node}(\operatorname{level}=j)$}
            \STATE $f_1$ = $\operatorname{node}.\operatorname{frequency}$, $f_2$ = $\sum{\operatorname{node.children().frequency}}$
		    \STATE $\operatorname{node}.\operatorname{frequency}$ = $\lambda_1$$f_1$ + $\lambda_2$$f_2$
            \ENDFOR 
        \ENDFOR    
        \FOR {$k$ from $1$ to $c$}
            \FOR {\_, node in enumerate $T.\operatorname{node}(\operatorname{level}=k)$}
                    \IF {$\operatorname{node}.\operatorname{children()}$ == None}
		            \STATE $\operatorname{node}.\operatorname{add\_children}()$
		            \STATE $\operatorname{node}.\operatorname{children()}.\operatorname{frequency} = \operatorname{node}.\operatorname{frequency}/B$
                    \ENDIF
            \ENDFOR 
        \ENDFOR    
    \end{algorithmic}
\end{algorithm}

\subsection{高维数据场景\myahead 的工作流程}
\label{Extension to Multi-dimensional Settings}
\subsubsection{二维数据场景}

首先我们讨论二维数据场景。
我们假设所有属性都具有相同的域 $D = \{1, 2, \cdots, d\}$，
其中 $d$ 是树扇出度 $B$ 的幂。
如果实际用户的数据域不同，我们可以添加一些填充值使所有数据域长度一致。
一维数据场景和二维数据场景的主要区别在于分解过程，体现在\autoref{Construct 1-dim prototype tree} 的第4行和第9行。
对于一维数据场景，\myahead 按层次划分整个数据域 $[1, |D|]$。
整个数据域划分为粒度不同的子区间。
对于二维数据场景，整个数据域变成了一个正方形区域 $[1, |D|] \times [1, |D|]$。
所以，\myahead 需要生成不同粒度的二维网格来分解整个数据域。
我们在\autoref{Multi-dimensional tree}给出了二维数据场景的\myahead 的工作流程，并在\autoref{Construct 2-dim prototype tree} 提供了相应的伪代码。
在上述示例中，用户私有属性的数据域大小为 $8\times8$，树扇出度 $B=4$。
与一维数据场景类似，二维\myahead 也需要四个步骤来估计用户的频率分布。

\mypara{第一步：用户分组}
用户在 $\{1, 2, 3, \cdots, c\}$ 的集合中随机选择用户组编号，其中 $c = \log_B|D|^2$。

\mypara{第二步：频率估计}
接着，聚合器将整个数据域划分为 $B$ 个等大小的正方形区域，并将初始数据划分发送给第一组用户。
用户将其私有数据投影到初始划分中，并通过 \oue 上传其数据。
服务器使用聚合算法获取估计的频率分布，该分布表示落入每个子域内的用户比例。

\mypara{第三步：数据域分解}
然后，聚合器将每个估计频率值与阈值 $\theta$ 进行比较，决定是否进一步划分相应的子域。
重复频率估计和数据域分解过程，\myahead 递归地分解域并构建 \myahead 原型树。

\mypara{第四步：后向处理}
最后，\myahead 在每个层内执行非负性处理，并在两个相邻层之间执行父节点和子节点估计频率之间的加权平均，以进一步减小估计误差。
基于均匀分布假设，\myahead 得到完整的树结构来回答范围查询。

因为查询结果的误差随使用节点数量的增加而增加，为了减少查询误差，\myahead 更倾向于使用粗粒度节点来回答查询。
如\autoref{Multi-dimensional tree} 所示，\myahead 回答一个二维查询 $[0,5]\times[0,5]$。 
\myahead 从树的顶层向下搜索，并计算范围查询完全覆盖的子域的频率之和。
范围查询完全覆盖了顶层 $[0,3]\times[0,3]$ 的区域以及中间层的五个区域 $[0,1]\times[4,5]$、$[2,3]\times[4,5]$、$[4,5]\times[4,5]$、$[4,5]\times[0,1]$、$[4,5]\times[2,3]$（这些区域用蓝色突出显示），
因此查询答案为 $(\tilde{f}1 + \tilde{f}9 + \tilde{f}{11} + \tilde{f}{13} + \tilde{f}{14} + \tilde{f}{4}/4)$。

对于二维范围查询，我们设置$B=2^2$，即每个维度的树扇出度为$2$。
作为比较，我们在实验中还给出了$B=4^2$的结果。
由于$\theta$的推导不涉及维度变化，
因此我们仍然根据\autoref{theta setting1}选择$\theta$。

\begin{algorithm}[!htbp]
    \caption{构建二维\myahead 层级分解树}
    \label{Construct 2-dim prototype tree}
	\begin{algorithmic}[1]
        % \REQUIRE All users’ value set $V = \{v_1, v_2,\ldots,v_N \}$, attribute domain $D$, tree fanout $B$, privacy budget $\epsilon$, threshold $\theta$
        % \ENSURE \myahead Tree $T$
        \REQUIRE 用户私有值 $V=\{v_1, \ldots, v_N\}$，数据域 $D$，树扇出度 $B$，隐私预算 $\epsilon$，阈值 $\theta$。
        \ENSURE 经过后向处理的\myahead 层级分解树 $T$
		\STATE $c = {\log_B}|D|^2$
        \STATE $//$ 用户划分
        % \STATE Randomly divide users into $c$ parts $\left\{ V_1, V_2, \ldots, V_c \right\}$
        % \STATE Create the root node of tree $T$ with initial interval $e^0_0 = [1, |D|] \times [1, |D|]$ and $\operatorname{T.node}(e^0_0).frequency = 1$ 
        \STATE 随机将所有用户划分为 $c$ 个用户组 $\left\{ V_1, V_2, \ldots, V_c \right\}$
        \STATE 创建表示整个二维数据域的树 $T$的根节点 $e^0_0 = [1, |D|] \times [1, |D|]$ 和 $\operatorname{T.node}(e^0_0).\operatorname{frequency = 1}$
		\FOR {$i$ from 1 to $c$}
            \STATE $//$ 数据域分解
            \FOR {$j$, node in enumerate($T.\operatorname{node}(\operatorname{level}=i-1)$)}
            \IF {$\operatorname{node.frequency} > \theta$}
            % \STATE Divide $\operatorname{interval} e^{i-1}_j$ into $B$ disjoint intervals $\{e^{i-1}_{j,k}\}$
            \STATE 将二维区间 $e^{i-1}_j$ 分解为 $B$ 个不相交的二维子区间 $\{e^{i-1}_{j,k}\}$
            \FOR {$k$ from $1$ to $B$}
    		\STATE $\operatorname{node.add\_child}(e^{i-1}_{j,k})$
            \ENDFOR
            \ELSE
            \STATE $\operatorname{node.add\_child}(e^{i-1}_{j})$
            \ENDIF 		
            \ENDFOR

            \STATE $//$ 频率估计
            \STATE $F = \fo(V_i, T.\operatorname{node}(\operatorname{level}=i).\operatorname{interval}, \epsilon)$
            \FOR {$k$, node in enumerate $T.\operatorname{node}(\operatorname{level}=i)$}
    		\STATE $\operatorname{node}.\operatorname{frequency} = F[k]$
            \ENDFOR
        \ENDFOR
        \STATE $//$ 后向处理
        \STATE 运行 \autoref{alg:1-dim post_processing}
        \STATE 返回 $T$
	\end{algorithmic}
\end{algorithm}

\subsubsection{三维及以上数据场景}

\myahead 可以通过两种方式扩展到更高维度。

\mypara{直接估计算法（Direct Estimation，\de）}
基于扇出度为 $B=2^m$ 的树，\myahead 同时分解 $m$ 个维度。
例如，\myahead 将三维域视为一个立方体，然后聚合不同粒度的子立方体的频率以回答查询。
在 \autoref{theta setting1} 的阈值设置下，\myahead 可以很好地控制子域的整体估计误差。
然而，叶节点数目随着维度呈指数级增长，这使得在高维数据集上查询答案过程非常耗时。

\mypara{利用低维估计算法（Leveraging Low-dimensional Estimation，\lle）}
为了解决数据维度上升引起的子域爆炸问题，
\lle 首先将属性两两配对，然后为每个属性对构建一个二维\myahead 树。
接着，在回答一个 $m$ 维查询时，\lle 构建一个包含 $2^m$ 个关联查询的查询集合。
最后，利用二维频率作为约束条件，\lle 通过最大熵优化估计所有 $2^m$ 个查询的频率值。
接下来，我们将详细介绍\lle 方法的三个步骤。
\begin{enumerate}
\item [1)] \mypara{构建二维树} \myahead 将所有属性成对分组，形成 $C_m^2$ 个二维属性对。
然后，\myahead 分别为这 $C_m^2$ 个二维属性对进行二维频率分布估计，即总共构建了 $C_m^2$ 个二维树。
\item [2)] \mypara{属性分布对齐} 
\myahead 需要在二维树集合中进行 $m$ 个数据维度的边际分布的对齐。
这保证了从不同二维树中进行某一属性的范围查询时结果的唯一性。
例如，属性 $a$ 参与了 $(m-1)$ 个属性对。
假设这 $(m-1)$ 个二维树分别为 $\{T_1, T_2, T_3, \cdots, T_{m-1}\}$，每个树除了根节点外都有 $\ell$ 层。
对于整数 $k \in [1, B^{\ell/2}]$，我们定义 $f_{T_i}(a, \ell, k)$ 为属于第 $\ell$ 层的 $T_i$ 节点的频率之和，
这些节点所对应的子域在属性 $a$ 上的覆盖范围为 $[(k-1)\cdot\frac{|D|}{B^{\ell/2}}+1, k\cdot\frac{|D|}{B^{\ell/2}}]$。
为了使所有 $f_{T_i}(a, \ell, k)$ 的频率分布在属性$a$的边际分布上保持一致，
\myahead 计算它们的加权平均值 $f(a, \ell, k) = \sum_{i=1}^{m-1}\lambda_{i} \cdot f_{T_i}(a, \ell, k)$，
其中 $\lambda_i$ 是 $f_{T_i}(a, \ell, k)$ 的权重，且 $\sum_{i=1}^{m-1}\lambda_i = 1$。
设置的权重值 $\lambda$ 用来最小化 $f(a, \ell, k)$ 的方差，
即 $\myvar[f(a, \ell, k)] = \sum_{i=1}^{m-1}\lambda_i^2\cdot \myvar[f_{T_i}(a, \ell, k)]$，其中 $\myvar[f_{T_i}(a, \ell, k)]$ 是用于计算 $f_{T_i}(a, \ell, k)$ 的节点理论方差之和。
当权重与估计值的方差成反比时，$\myvar[f(a, \ell, k)]$ 最小\cite{yang2020answering}。
因此，最优权重 $\lambda_i = \frac{1}{\myvar[f_{T_i}(a, \ell, k)]}/\sum_{i=1}^{m-1}\frac{1}{\myvar[f_{T_i}(a, \ell, k)]}$。
我们通过在原始估计频率上添加 $(f(a, \ell, k) - f_{T_i}(a, \ell, k))/B^{\ell/2}$ 来更新 $T_i$ 中原有的频率估计值，最终使每个 $f_{T_i}(a, \ell, k)$ 等于 $f(a, \ell, k)$。
根据文献{\rm\parencite{qardaji2014priview}} 的分析结论，
关于属性 $a$ 在 $\{T_1, T_2, T_3, \cdots, T_{m-1}\}$ 进行属性分布对齐不会影响其他属性的频率分布。
% ${T_1, T_2, T_3, \cdots, T_{m-1}}$ 在不改变的情况下达成对 的一致。
因此，我们可以按任意属性顺序进行属性分布对齐，\myahead 可以实现所有 $m$ 个属性的一致性。注意到每个二维树有 $\ell$ 层，\myahead 需要对所有 $\ell$ 层分别执行属性分布对齐。
\item [3)] \mypara{最大熵优化} 
我们采用了最大熵原理\cite{qardaji2014priview}，基于二维查询的结果估计 $m$ 维查询结果。
具体来说，对于一个 $m$ 维范围查询 $q$，我们可以定义一组范围查询：
$$Q(q)=\left\{\wedge\left(a_{j},\left[\alpha_{j}, \beta_{j}\right] \text { or }\overline{\left[\alpha_{j}, \beta_{j}\right]}\right) \mid a_{j} \in A\right\}\text{,}$$
其中 $[\alpha_t, \beta_t]$ 表示查询区间，$\overline{\left[\alpha_{t}, \beta_{t}\right]}$ 是其补集。
对于 $2^m$ 个查询 $g \in Q(q)$，
我们定义 $f_q(g)$ 为 $Q(q)$ 查询结果集合。
我们可以获得 $Q(q)$ 对应的二维查询集合以及其对应的查询结果集合$f_{q^{(j,k)}}$：
$$Q(q^{(j,k)})=\left\{\left(a_{j},\left[\alpha_{j}, \beta_{j}\right] \text { or }\overline{\left[\alpha_{j}, \beta_{j}\right]}\right) \wedge \left(a_{k},\left[\alpha_{k}, \beta_{k}\right] \text { or }\overline{\left[\alpha_{k}, \beta_{k}\right]}\right)\right\}\text{,}$$
对于任何查询 $g^{(j,k)} \in Q(q^{(j,k)})$，我们使用 $f_{q^{(j,k)}}(g^{(j,k)})$ 来表示其查询结果。
例如，对于一个 $g^{(j,k)} \in Q(q^{(j,k)})$，$f_{q}(g^{(j,k)})$ 表示通过对 $Q(q)$ 集合中的查询结果求和，以构建高维查询 $g^{(j,k)}$ 的结果 $f_q$。
接着，我们可以通过求解下列优化问题得到具体的高维查询结果。
$$
\begin{array}{ll}
\text { maximize } & -\sum_{g \in Q(q)} f_{q}(g) \cdot \log \left(f_{q}(g)\right) \\
\text { subject to } & \forall_{g \in Q(q)} f_{q}(g) \geq 0 \\
& \forall_{a_j, a_k \in A} \forall_{g^{(j, k)} \in Q\left(q^{(j, k)}\right)} f_{q^{(j, k)}}(g^{(j, k)})=f_{q}(g^{(j, k)})
\end{array}
$$
以上优化问题可以通过现有凸优化工具来解决。
为了更快速的计算出查询结果，降低查询延迟，\myahead 采用文献{\rm\parencite{yang2020answering}}提出的加权更新算法，
该算法在降低计算复杂度的同时实现了与最大熵估计相当的精度。
\end{enumerate}

\de 方法的实现相对简单，而且用户分区的数量不会随着维数的增加而增加。然而，\de 中的 \myahead 树在高维情况下可能非常大，从而使得树的构建和查询回答过程变得耗时。
相比之下，\lle 方法将属性成对组合，为每个属性对构建一个二维 \myahead 树，使得每棵树的规模不会太大。
% 我们在 \autoref{Effectiveness of ahead for high-dimensional Range Query} 中从经验上展示了这两种策略的性能，并讨论了如何在它们之间进行选择。
% 之前的方法也利用最大熵优化将低维机制有效地扩展到高维场景中。例如，\PriView\cite{qardaji2014priview} 提出构建低维视图，即二维、三维边缘表，并将最大熵优化应用于从视图中重建更高维的边缘表。

% \begin{algorithm}[!h]
% 	\caption{Post-processing}
% 	\label{alg:2-dim post_processing}
% 	\begin{algorithmic}[1]
%         \REQUIRE \myahead tree $T$, tree fanout $B$
%         \ENSURE \myahead tree $T$
%         \FOR {$i$ from $1$ to $c$}
% 		\STATE norm\_sub($T.\operatorname{node(level=i)}.\operatorname{frequency}$)
%         \ENDFOR    
%         \FOR {$j$ from $c-1$ to $1$}
%             \FOR {\_, node in enumerate $T.\operatorname{node}(level=j)$}
%             \STATE $f_1$ = node.frequency, $f_2$ = $\sum{\operatorname{node.children().frequency}}$
% 		    \STATE node.frequency = $\lambda_1$$f_1$ + $\lambda_2$$f_2$
%             \ENDFOR 
%         \ENDFOR    
%         \FOR {$k$ from $1$ to $c$}
%             \FOR {\_, node in enumerate $T.\operatorname{node}(level=k)$}
%                     \IF {node.children() == None}
% 		            \STATE $\operatorname{node}.\operatorname{add\_children}()$
% 		            \STATE $\operatorname{node}.\operatorname{children()}.\operatorname{frequency} = \operatorname{node}.\operatorname{frequency}/B$
%                     \ENDIF
%             \ENDFOR 
%         \ENDFOR    
        
%     \end{algorithmic}
% \end{algorithm}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\hsize]{figure/ldp_range_query/figures_others/GridAHEAD_workflow2ch.pdf.pdf}
    \vspace{-0.7cm}
    \caption{
        % 2-dim \myahead algorithm, 
    % where the decomposition is implemented by decomposing both dimensions simultaneously.
    \myahead 构建二维层级分解树的流程。在数据域分解过程中，\myahead 同时对二维数据域的两个维度同时进行分解。}
    \vspace{-0.4cm}
    \label{Multi-dimensional tree}
\end{figure*}

\subsection{算法本地差分隐私合规性及误差分析}
\label{Privacy and Utility Analysis}

\subsubsection{本地差分隐私合规性}

\myahead 与所有用户顺序交互 \cite{joseph2019role, joseph2020exponential, duchi2013local, acharya2020interactive, kasiviswanathan2011can}。
在整个数据收集过程中，每个用户只通信一次（\autoref{Workflow of ahead} 中的第2步），
但随机化取决于之前用户的消息（\autoref{Workflow of ahead} 中的第3步）。
每个用户的私有数据通过保隐私的 \oue 协议传输到聚合器，不泄露其他用户的任何信息。

\begin{theorem}
    \label{satisfy LDP}
    \myahead 满足 $\epsilon$-LDP.
\end{theorem} 
\begin{proof}
    在第一步中，用户在 $[1,2, \cdots, c]$ 中随机选择他们的组ID，不需要使用隐私预算。
    在第二步和第三步中，\myahead 逐个与用户交互，每个用户产生一个输出。
    在第四步中，\myahead 不会触及用户的私人数据，因此不会产生额外的隐私预算消耗。
    因此，如果聚合器与用户的交互过程（第二步和第三步）符合 $\epsilon$-LDP，则 \myahead 满足 $\epsilon$-LDP \cite{joseph2020exponential}。
    
    在与用户的交互过程中，\myahead 使用隐私预算 $\epsilon$ 并基于 \oue 协议进行频率估计。
    对于任意的两个用户私有数据值 $v_1, v_2 \in D$，上传结果都是经过 \oue 扰动的带噪声的二进制向量 $O$。
    我们选取第$g$组用户为分析对象，其他组同理可证。
    \begin{align}\label{6}
        \frac{\Pr{O|v_1, g}}{\Pr{O|v_2, g}} &= \frac{\Pi_{i=1}^{\ell}\Pr{O[i]|v_1, g}}{\Pi_{i=1}^{\ell}\Pr{O[i]|v_2, g}} \nonumber \\
                              &= \frac{\Pi_{i=1}^{\ell}\Pr{O[i]|v_1} \cdot \Pr g}{\Pi_{i=1}^{\ell}\Pr{O[i]|v_2} \cdot \Pr g} \nonumber \\
                                                &\leq{\frac{\Pr{O[v_1]=1|v_1}\Pr{O[v_2]=0|v_1}}{\Pr{O[v_1]=1|v_2}\Pr{O[v_2]=0|v_2}}} \nonumber \\
                                                &= \frac{p}{q}\cdot\frac{1-q}{1-p} \nonumber \\
                                                &= \frac{1/2}{1/(1+e^\epsilon)}\cdot\frac{1-1/(1+e^\epsilon)}{1-1/2}= e^\epsilon, 
    \end{align}

    其中，$\ell$是$O$的长度。
    $p$和$q$是\oue 协议的翻转概率。
    当$p = \frac{1}{2}$，$q = \frac{1}{(e^\epsilon +1)}$时，\oue 可以获得最小方差 {\rm\cite{wang2017locally}}。
    从{\rm\autoref{6}}，第二步满足$\epsilon$-LDP。
    因为第三步处理对象是扰动后的数据，即未接触用户的原始私有数据，因此没有消耗额外的隐私预算。
    综上，\myahead 算法满足$\epsilon$-LDP。
\end{proof}


\subsubsection{查询结果误差分析}
真实查询结果和\myahead 输出结果之间的误差来源于三个方面。
其中，扰动噪声误差和采样误差源于 \oue 的扰动和用户采样过程，
非均匀误差是由于在 \myahead 树中，一些区间的频率值是由较大区间的值近似计算而来的。

\mypara{噪声误差和采样误差} 
如\autoref{myoue}所示，尽管 \oue 可以获得频率值的无偏估计，但仍存在由扰动引起的估计方差。
此外，\myahead 将用户分为 $c$ 组，并使用每个用户组来表示所有用户的频率估计。 
基于 \cite{yang2020answering} 中的分析，采样误差是一个常数，远小于插入的噪声。
由于每个用户随机选择其中一个 $c$ 组来报告私有数据，每个组的用户数量近似于 $\frac{N}{c}$。
根据\autoref{OUEVAR}，扰动噪声 $X$ 的方差与组数 $c$ 成比例，即 $\sigma^2 = c \cdot \frac{4e^{\epsilon}}{N\left(e^{\epsilon}-1\right)^{2}}$。
由于阈值设置的原因，一些细粒度区间的频率值可能无法直接估算。
对于未估算的间隔，其频率值应该从更大的间隔中计算，即 \myahead 树中的父节点。
如果未估算的间隔的频率值是从一个比它大小大 $k$ 倍的大区间中计算的，则 \myahead 将大区间的估计频率的 $\frac{1}{k}$ 分配给未估算的区间。
因此，未估算区间的噪声误差可以视为来源于随机变量 $\frac{X}{k}$。

\mypara{非均匀误差}
对于一个没有直接估计的区间 $n$，假设其真实频率值为 $f_n$。
其对应的父节点区间的尺寸为 $n$ 的 $k$ 倍，此处 $k$ 的含义与噪声误差和采样误差部分中的 $k$ 相同。
假设父节点区间的频率分布满足均匀分布\cite{qardaji2013differentially}。
那么，区间 $n$ 的分配频率值为 $\frac{f_p}{k}$，其中 $f_p$ 是父节点区间的真实频率值。
如果区间 $p$ 中的值满足均匀分布时，则没有非均匀误差，即 $f_n = \frac{f_p}{k}$。
如果区间 $p$ 中的值不符合均匀分布时，区间 $n$ 的频率值偏差为 $|f_n - \frac{f_p}{k}|$。
因此，非均匀误差受父节点区间 $p$ 真实分布的影响。
如果频率分布接近均匀分布，则非均匀误差较小。
否则，非均匀误差将变大，并且非均匀误差的上限取决于真实频率值 $f_p$，即 $|f_p - \frac{f_p}{k}|$。
综上，在节点真实频率更均匀分布的情况下，\myahead 的表现更好，因为非均匀误差更小。



\subsection{算法参数 $\theta$ 和 $B$ 设置分析}
\label{Selection parameters}
\myahead 的最重要的参数是分解阈值$\theta$和树的扇出度$B$。
基于\autoref{satisfy LDP}，\myahead 已满足LDP的要求。
在本节中，我们的目标是分析如何设置算法参数$B$和$\theta$，以最大化\myahead 的查询结果的精度。
在实际中，参与数据收集的用户数量较大。所以在下面的分析中，我们假设每个用户组具有相等数量的用户。
基于\autoref{Privacy and Utility Analysis}中的误差来源分析，我们主要关注噪声误差和非均匀误差，因为它们在查询结果误差中占据主导地位。

\subsubsection{参数$\theta$}
我们通过设置\myahead 的参数来权衡噪声误差和非均匀误差，保证\myahead 能够实现较好的整体效用。
对于一组参数，即树扇出度$B$、隐私预算$\epsilon$、用户规模$N$和用户组数$c$，分解阈值$\theta$的设置遵循下面的公式。
\begin{equation}
    \theta = \sqrt{(B+1)\myvar}, 
    \label{theta setting1}
\end{equation}    
其中，\myvar 等于$\frac{4e^{\epsilon}c}{N\left(e^{\epsilon}-1\right)^{2}}$，即每个估计频率值中添加噪声的方差。
\autoref{theta setting1}的具体分析过程如下。
回顾\autoref{Workflow of ahead}中的新分解生成步骤，\myahead 通过将每个区间的估计值与阈值进行比较来单独划分每个区间。
因此，我们的分析可以集中在\myahead 树的一个区间节点上。
假设我们有一个具有真实频率值$f$的节点$n$，用$f_1$，$f_2$，$\cdots$，$f_B$表示其子节点的真实频率值，$n$的一个子节点频率值为$\eta f$，其他子节点的频率值之和为$(1-\eta)f$，其中$\eta \in [0,1]$。
参数$\eta$由用户数据的分布确定。当真实分布偏离均匀分布时，$\eta$趋近于0或1；
否则，$\eta$趋近于$\frac{1}{B}$。
我们对比\autoref{Motivation and Overview}中提到的两种不同的策略来估计$n$的子节点的频率值。
首先，我们使用\myhio 中使用的基线策略来获取它们的频率值，可以得出子节点总体估计误差的期望值，如\autoref{method_1}所示。
\begin{align}
    \mathbb{E}\left[\myErr_1\right] & = \mathbb{E}\left[(\hat{f_1} - f_1)^2 + (\hat{f_2} - f_2)^2 + \cdots + (\hat{f}_B - f_B)^2\right]        \nonumber \\
        & = \mathbb{E}\left[(f_1 + X_1 - f_1)^2 +  \cdots + (f_B + X_B - f_B)^2\right] \nonumber \\
        & = \mathbb{E}\left[B \cdot X^2\right] = B\mathbb{E}\left[X^2\right] = B\myvar
    \label{method_1}
\end{align}    

在\autoref{method_1}中，我们首先进行了一次\oue 机制，以获取每个层中用户数据在$B$个区间内的分布情况。
$X_i$代表添加到第$i$个子区间的估计值中的扰动噪声。
对于每个层，\autoref{OUEVAR}中的用户数量在所有区间中都是相同的，即对于任何$i$，$\mathbb{E}[X_i^2] = O(c/N)$。
利用\myahead 中使用的自适应策略，我们估计节点$n$的频率值，并将平均值分配给子节点的值。
然后，我们得出子节点总体估计误差的期望值，如\autoref{method_2}所示。
\begin{align}
    \mathbb{E}\left[\myErr_2\right] &= \mathbb{E}\left[ (\frac{\hat{f}}{B} - f_1)^2 + (\frac{\hat{f}}{B} - f_2)^2 + \cdots + (\frac{\hat{f}}{B} - f_B)^2 \right]  \nonumber \\
                & = \mathbb{E}\left[ (\frac{f+X_1}{B} - \eta f)^2 + \cdots + (\frac{f+X_B}{B} - f_B)^2 \right] \label{method_2_1} \\
                & = \mathbb{E}\left[ \eta^2 f^2 + \sum_{i=2}^B f_i^2 - \frac{f^2}{B}\right] + \mathbb{E}\left[\frac{X^2}{B} \right]  \nonumber \\
                & \leq \eta^2 f^2 + (1-\eta)^2f^2 - \frac{f^2}{B} + \frac{1}{B}\myvar \label{method_2_6} \\
                & \leq \frac{1}{B}((B-1)f^2+\myvar)
    \label{method_2}
\end{align}  

在\autoref{method_2}的推导中，每个子节点的误差包含噪声误差和非均匀误差。
例如，\autoref{method_2_1}中第一个子节点的平方误差可以重写为$\left(\frac{X_{1}}{B}+\left(\frac{f}{B}-f_{1}\right)\right)^{2}$。
我们令\autoref{method_2}小于\autoref{method_1}，以确保自适应策略的整体估计误差比基线策略低。
然后我们得到\autoref{theta setting 2}。
\begin{align}
    f < \sqrt{(B+1)\myvar}
    \label{theta setting 2}
\end{align}
我们进一步计算节点$n$子树中节点的频率值，自适应策略始终优于基准策略。
基于对\autoref{theta setting 2}的分析，如果节点的频率值符合\autoref{theta setting 2}，则不能再进行划分；
否则，区间需要进一步划分。
因此，我们将阈值设为$\theta=\sqrt{(B+1)\myvar}$，以确保\myahead 不会划分具有较小频率值的节点，减少估计误差。
需要注意的是，在经过\autoref{Workflow of ahead}中的后向处理后，$\theta$可能不是最优取值。
然而，后向处理与树结构相关，而选择$\theta$时树结构是未知的，所以我们很难把后向处理纳入到$\theta$的理论分析中。
我们目前对$\theta$的分析独立于树结构和用户的真实数据分布，使得导出的$\theta$适用于任何输入数据。

在\myahead 的实现中，我们只能访问估计的频率值$\hat{f}$，即真实频率值$f$加上一个随机噪声变量$X$。
由于\oue 是一个无偏协议，$\hat{f}$的期望值等于$f$。
因此，我们仍然使用$\theta$作为\myahead 中的阈值。
% 并在\autoref{Validation of Threshold Choice}中通过实验验证所选取阈值的合理性。

\subsubsection{参数$B$}
树扇出度$B$ 用于平衡树的层数和回答单次查询所需的节点数。
现有文献{\rm\cite{wang2019answering, cormode2019answering}}在仅考虑噪声误差和采样误差时得出最佳扇出度 $B \approx 4$。
不同于现有文献，\myahead 在合并区间的过程中引入了非均匀误差。
所以，通过综合分析 \myahead 查询结果中的三个误差来源，我们设置 $B=2$ 并在下文进行分析。

对于真实频率为 $f$（$f<\theta$）的节点 $n$，\myahead 由于阈值$\theta$的设置不会进一步分解节点 $n$ 的区间，
所以，节点 $n$ 的子节点不能通过\myahead 的第二步直接获取其估计频率。
从\autoref{OUEVAR}可得，扰动噪声 $X$ 在节点 $n$ 上的方差与用户组数 $c$ 成正比，
即 $\sigma^2 = c \cdot \frac{4e^{\epsilon}}{N\left(e^{\epsilon}-1\right)^{2}}$。
为了得到一个完整的树来回答查询，
\myahead 在后向处理中将节点 $n$ 的估计频率值的 $\frac{1}{B}$ 分配给其子节点。
因此，我们可以将节点 $n$ 的子节点的噪声误差表示为 $\frac{X}{B}$。
对于非均匀误差，我们没有关于数据分布的先验知识，所以考虑最坏情况，即非均匀误差为 $f-\frac{f}{B}$。
考虑到 $f$ 应该接近于阈值 $\theta$，期望的估计误差可以表示为
\begin{align}
    \mathbb{E}\left[\myErr_3\right] &= \mathbb{E}\left[(\frac{X}{B}+(f-\frac{f}{B}))^2\right] \nonumber \\
    &=\frac{c \sigma ^2}{B} + (f-\frac{f}{B})^2 \nonumber \\ 
    &=\frac{c \sigma ^2}{B} + (\frac{B-1}{B})^2 (B+1) c \sigma^2 \nonumber \\ 
    &=(\sigma^2\ln{|D|})\frac{B+(B-1)^2(B+1)}{B^2\ln{B}},
    \label{B setting}
\end{align}  
其中 $|D|$ 是属性的域大小，$\epsilon$ 是隐私预算，$N$ 是用户规模，$c$ 是用户组数。
令\autoref{B setting}的导数为0，我们得到 $B=0.6$ 和 $B=2.2$。
由于树的扇出度 $B$ 是大于1的整数，并且在 $B=2$ 时\autoref{B setting} 的值比 $B=3$ 时小，我们在\myahead 设置$B=2$ ，并与文献{\rm\parencite{wang2019answering, cormode2019answering}}推荐的 $B=4$ 进行查询误差比较。
% 我们在 \autoref{Evaluation} 中经验证实验证明了该参数设置的有效性。



\subsection{算法复杂度分析}
\label{Complexity Analysis}
\begin{table}[h]
    \centering
    \caption{文中所涉及方法的复杂度比较，表格列出了聚合器计算、存储空间和聚合器-用户通信复杂度。}
    \resizebox{0.6\textwidth}{!}{
        \begin{tabular}{|c|c|c|c|c|}
        \hline
        \multicolumn{2}{|l|}{}   & 时间 & 空间 & 通信 \\ \hline
        \multirow{4}{*}{一维场景} & \myahead &  $O(log_{|D|}2\cdot N \cdot|D|)$   & $O(|D|)$    &  $O(|D|)$          \\ \cline{2-5} 
                             & \mycalm  &  $O(N\cdot|D|)$                    & $O(|D|)$    &  $O(|D|)$          \\ \cline{2-5} 
                             & \myhio   &  $O(log_{|D|}5\cdot N \cdot|D|)$   & $O(|D|)$    &  $O(log(|D|))$     \\ \cline{2-5} 
                             & \mydht   &  $O(N+|D|^3)$                      & $O(|D|^2)$  &  $O(log(|D|))$     \\ \hline
        \multicolumn{5}{|l|}{}                             \\ \hline
        \multirow{2}{*}{二维场景} & \myahead & $O(log_{|D|}2\cdot N \cdot|D|^2)$  & $O(|D|^2)$  &  $O(|D|^2)$        \\ \cline{2-5} 
                             & \myHDG   & $O(N)$  & $O(N^\frac{1}{2})$       & $O(log|D|)$                      \\ \hline
        \end{tabular}}
    \label{tab:complexity analysis}
    \end{table}

\autoref{tab:complexity analysis}给出了不同方法的时间复杂度、空间复杂度以及通信代价。
为了方便分析，我们假设所有属性具有相同的数据域$D$。

\mypara{时间复杂度}
对于一维场景，聚合器的计算负担主要来自于处理用户的上传数据。
对于第 $i$ 组用户，其时间复杂度为 $O(\frac{N}{log_2^{|D|}}\cdot2^i)$，总时间复杂度为 $O(log_{|D|}2\cdot N \cdot|D|)$。
类似的结论也适用于基于层次结构的 \myhio 方法，即总时间复杂度为 $O(log_{|D|}5\cdot N \cdot|D|)$。
由于范围查询场景中数据域大小 $D$ 通常比较大，
\mycalm 采用 \oue 聚合用户数据，即每个用户的时间复杂度为 $O(|D|)$，总时间复杂度为 $O(N \cdot |D|)$。
对于 \mydht，运行时间主要来自于求和操作和逆变换过程。
具体来说，该方法首先对具有相同索引的用户上传结果求和，这个过程的时间复杂度为 $O(N)$。
然后，\mydht 还需要进行逆变换过程以产生估计的频率，即时间复杂度为 $O(|D|^3)$。
对于二维情形，定义域大小从 $|D|$ 变为 $|D| \times |D|$，\myahead 选择扇出 $B=4$，时间复杂度变为 $O(log_{|D|}4\cdot N \cdot|D|^2)$。
为了估计用户频率分布，\myHDG 需要基于哈希函数处理每个用户的上传结果，所以总时间复杂度为 $O(N)$。

\mypara{空间复杂度}
这里我们只衡量计算过程的中间变量所需的存储空间，
不包括输入和输出所占用的空间，因为输入和输出所占用的空间对于所有方法来说都是相同的。
对于一维场景，\myahead 和\myhio 需要维护层次树结构，需要$O(|D|)$的存储空间。
\mycalm 方法需要将每个用户上传的长度为$|D|$的向量求和以计算频率分布，同样需要$O(|D|)$的存储空间。
由于\mydht 需要记录哈尔小波逆变换过程中的数据，所以需要$O(|D|^2)$的空间。
对于二维场景，数据域变成了$D \times D$，因此\myahead 需要$O(|D|^2)$的存储空间来维护层次树结构。
\myHDG 的存储空间取决于二维网格的粒度。
根据文献{\rm\parencite{yang2020answering}}的分析结果，\myHDG 需要$O(N^\frac{1}{2})$的存储空间。


\mypara{通信复杂度}
\myhio 、\mydht 和\myHDG 使用\olh \cite{wang2017locally}作为\fo 。
因为\olh 上传数据仅为一个比特和索引值，可以用$log|D|$比特或$log|D|^2$比特来表示二维场景的\myHDG 算法。 
\mycalm 方法采用\oue ，每个用户需要报告一个长度为$|D|$的二进制向量，需要$O(|D|)$比特。
\myahead 的主要通信开销是将数据域分解传递给每个用户。
考虑最差情况，\myahead 的通信复杂度为$O(|D|^2)$。




\section{性能评估}
\label{Evaluation}
我们在多个真实数据集和合成数据集上评估\myahead 的性能，
并与 \myhio \cite{wang2019answering}（一维查询）、\mydht \cite{cormode2019answering}（一维查询）、\mycalm \cite{zhang2018calm}（一维、二维查询）和 \myHDG \cite{yang2020answering}（二维和高维查询）比较查询结果的精度。

\subsection{实验设置}
\begin{table}[t]
    \centering
    \caption{数据集信息汇总}
    \label{realDatasets}
    \setlength{\tabcolsep}{0.8mm}{
        \resizebox{0.6\textwidth}{!}{
          \begin{tabular}{c c c c c}
            \toprule
            \bf{数据集名称}  & \bf{数据分布} & \bf{数据总量}  & \bf{数据种类} & \bf{数据来源}    \\  
            \midrule
            Salaries      &      --         & 148,654       &   员工薪水 & 真实数据 \\   
            \rowcolor{mygray}
            BlackFriday   &      --         & 537,577       &   购物记录    & 真实数据 \\
            Loan          &      --         & 2,260,668    &   线上贷款  & 真实数据 \\             
            \rowcolor{mygray} 
            Financial    &      --         & 6,362,620    & 诈骗检测   &  合成数据\\
            Cauchy       &      Cauchy     &  --          & --              &  合成数据               \\    
            \rowcolor{mygray} 
            Zipf         &      Zipf (power-law)     &  --          &  --   &  合成数据    \\
            Gaussian     &      Gaussian  & --   &  --      &  合成数据 \\
            \rowcolor{mygray} 
            Laplacian      &    Laplacian   & --  &  --    &  合成数据\\  
          \bottomrule
          \end{tabular}}}
\end{table}

\mypara{实验硬件平台}
所有的实验评估都是在一台配备有Intel Xeon Platinum 8269@2.5GHz处理器和32GB内存的个人主机上进行。

\mypara{数据集}
在我们的实验中，我们使用了3个真实世界的数据集和5个合成数据集来评估 \myahead 的性能，\autoref{realDatasets} 提供了所有数据集的概述。
% 关于四个数据集，即 Salaries、BlackFriday、Loan 和 Financial，详细信息在 \autoref{Dataset Description} 中有所展示。

另外，我们通过从 \Cauchy（$x_0=0$，$\gamma=1$）、\Zipf（$\alpha=1.1$）、\Gaussian（$\mu=0$，$\sigma^2=1$）和 \Laplacian（$\mu=0$，$\lambda^2=1/2$）分布中进行采样，生成一维数据集\cite{ye2019privkv, cormode2019answering, yang2020answering}。
多维合成数据集则是从均值为 $0$，标准差为 $1$的多维 \Gaussian 和 \Laplacian 分布中采样的\cite{yang2020answering}。

\mypara{度量指标}
为了量化 \myahead 的性能，
现有文献中广泛使用的均方误差（Mean Square Error, MSE）\cite{ye2019privkv, cormode2019answering, wang2019consistent} 来衡量算法计算的估计值与真实值之间的偏差。
对于每个实验设置，我们计算200个随机查询结果以及对应的MSE，然后重复20次实验计算平均值和标准差。
此外，我们还展示出MSE数值的95\%置信区间，以反映其数值的抖动情况。

\mypara{对比方法}
为了保证评估的客观性，我们使用与原文献相同的参数设置来实现\myhio \cite{wang2019answering}、\mydht \cite{cormode2019answering}、\mycalm \cite{zhang2018calm}和\myHDG \cite{yang2020answering}方法。
我们还在1维和2维场景中引入了基准方法\myuni。
该方法假设所有用户数据集均满足均匀分布，并按照查询区间占数据域的比例直接计算查询结果。
显然，如果一种方法的性能不如\myuni ，那么该方法的查询答案是无意义的。
对于高维范围查询，我们以\myHDG 为基准方法。

\subsection{一维数据场景}
\label{Evaluation for 1-dim Range Query}

\begin{figure*}[ht]
    \centering
    \subfigure[Loan, 256, vary $\epsilon$]{
    \includegraphics[width=0.23\hsize]{figure/ldp_range_query/figures_experiment_result/0531RandQuery/0404_CI_Rand_ep-Loan-Domain_2_8.pdf.pdf}
    }
    \subfigure[Financial, 512,  vary $\epsilon$]{
    \includegraphics[width=0.23\hsize]{figure/ldp_range_query/figures_experiment_result/0531RandQuery/0404_CI_Rand_ep-Financial-Domain_2_9.pdf.pdf}
    }
    \subfigure[BlackFriday, 1028,  vary $\epsilon$]{
    \includegraphics[width=0.23\hsize]{figure/ldp_range_query/figures_experiment_result/0531RandQuery/0404_CI_Rand_ep-BlackFriday-Domain_2_10.pdf.pdf}
    }
    \subfigure[Salaries, 2048,  vary $\epsilon$]{
    \label{overall compare Salaries}
    \includegraphics[width=0.23\hsize]{figure/ldp_range_query/figures_experiment_result/0531RandQuery/0404_CI_Rand_ep-Salaries-Domain_2_11.pdf.pdf}
    }
    \\[-0.5ex]
    \subfigure{
    \includegraphics[width=0.8\hsize]{figure/ldp_range_query/figures_others/comparison_1D_legend6.pdf.pdf} 
    }
    \vspace{-0.2cm}
    \caption{不同方法的MSE，隐私预算从0.1到1.5变化，结果以对数刻度显示。}
    \label{overall compare}
\end{figure*}

我们在多种隐私预算条件下评估\myahead 的有效性。
具体来说，我们将隐私预算$\epsilon$从0.1（表示高隐私保护强度）变化到1.5（表示低隐私保护强度）进行考虑\cite{zhang2018calm, cormode2019answering}。 
\autoref{overall compare}展示了\myahead 与现有文献{\rm\parencite{cormode2019answering, wang2019answering, zhang2018calm}}在一维查询中的MSE比较。
对于每个图，我们在横轴上变化隐私预算$\epsilon$，并在纵轴上展示其相应的MSE。
每个图中的直方图显示20次重复实验MSE的平均值，误差线表示MSE的标准差。

从\autoref{overall compare}中，我们得出以下结论。
1）\myahead 的MSE始终比其他算法小。
回顾\autoref{Selection parameters}中的\autoref{theta setting 2}，对于频率值低于阈值的节点，\myahead 采用自适应策略比采用基线策略的对比方法具有更小的总体估计误差。
2）\myahead 在不同数据集上表现不同。对于Salaries数据集，\myahead 在整个隐私预算范围内都比之前的方法显著优越，其中\myahead 的MSE几乎比现有方法小一个数量级。
对于Loan数据集，\myahead 略优于\mydht。
主要是因为这两个数据集具有完全不同的分布特性，
例如Salaries数据集上有更多频率值小于阈值的子域，
而Loan数据集则相对较少，使得\myahead 在这两个数据集上表现不同。


\subsection{二维数据场景}
\begin{figure*}[h!]
    \centering
    \subfigure[\Laplacian, $256^2$, vary $\epsilon$]{\label{fig:Two_Laplace08-Set_10_7-Domain_8_8}\includegraphics[width=0.24\hsize]{figure/ldp_range_query/figures_experiment_result/overall_result/20210404_CI_Rand_ep-Two_Laplace08-Set_10_7-Domain_8_8.pdf.pdf}}
    \subfigure[\Laplacian, $1024^2$, vary $\epsilon$]{\label{fig:Two_Laplace08-Set_10_7-Domain_10_10}\includegraphics[width=0.24\hsize]{figure/ldp_range_query/figures_experiment_result/overall_result/20210404_CI_Rand_ep-Two_Laplace08-Set_10_7-Domain_10_10.pdf.pdf}}
    \subfigure[\Gaussian, $256^2$, vary $\epsilon$]{\label{fig:Two_Normal08-Set_10_7-Domain_8_8}\includegraphics[width=0.24\hsize]{figure/ldp_range_query/figures_experiment_result/overall_result/20210404_CI_Rand_ep-Two_Normal08-Set_10_7-Domain_8_8.pdf.pdf}}
    \subfigure[\Gaussian, $1024^2$, vary $\epsilon$]{\label{fig:Two_Normal08-Set_10_7-Domain_10_10}\includegraphics[width=0.24\hsize]{figure/ldp_range_query/figures_experiment_result/overall_result/20210404_CI_Rand_ep-Two_Normal08-Set_10_7-Domain_10_10.pdf.pdf}}
    \\[-0.5ex]
    \subfigure{\includegraphics[width=0.4\textwidth]{figure/ldp_range_query/figures_others/comparison_2D_legend.pdf.pdf}} 
    \vspace{-0.2cm}
    \caption{
        % Comparison of different methods on \Laplacian and \Gaussian datasets under various privacy budgets. We only plot the methods that are scalable in each setting. \myHDG is a baseline method. The results are shown in log scale.
        基于\Laplacian 和 \Gaussian 数据集，我们比较了\myahead 和现有方法在不同隐私预算下的性能，结果以对数刻度显示。其中，\myHDG 是基准方法。}
    % \vspace{-0.5cm}
    \label{2-dim overall compare}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \subfigure[\Laplacian, $256^2$, vary $r$]{\label{fig:Two_Laplace08-Set_10_7-Domain_8_8}\includegraphics[width=0.24\hsize]{figure/ldp_range_query/figures_experiment_result/overall_result/20210404Correlation-Two_Laplace-Set_10_7-Domain_8_8.pdf.pdf}}
    \subfigure[\Laplacian, $1024^2$, vary $r$]{\label{fig:Two_Laplace09-Set_10_7-Domain_8_8}\includegraphics[width=0.24\hsize]{figure/ldp_range_query/figures_experiment_result/overall_result/20210404Correlation-Two_Laplace-Set_10_7-Domain_10_10.pdf.pdf}}
    \subfigure[\Gaussian, $256^2$, vary $r$]{\label{fig:Two_Laplace08-Set_10_7-Domain_10_10}\includegraphics[width=0.24\hsize]{figure/ldp_range_query/figures_experiment_result/overall_result/20210404Correlation-Two_Normal-Set_10_7-Domain_8_8.pdf.pdf}}
    \subfigure[\Gaussian, $1024^2$, vary $r$]{\label{fig:Two_Laplace09-Set_10_7-Domain_10_10}\includegraphics[width=0.24\hsize]{figure/ldp_range_query/figures_experiment_result/overall_result/20210404Correlation-Two_Normal-Set_10_7-Domain_10_10.pdf.pdf}}
    \\[-0.5ex]
    \subfigure{\includegraphics[width=0.4\textwidth]{figure/ldp_range_query/figures_others/comparison_2D_legend.pdf.pdf}}  
    \vspace{-0.2cm}
    \caption{基于\Laplacian 和 \Gaussian 数据集，我们比较了\myahead 和现有方法在不同属性相关性下的性能，结果以对数刻度显示。其中，\myHDG 是基准方法。}
    % \vspace{-0.5cm}
    \label{2-dim correlation}
\end{figure*}

\mypara{不同隐私预算下的算法查询精度}
我们评估了在二维查询下不同隐私预算对算法性能的影响。
% 在这里，我们专注于合成数据集，因为它们可以反映算法在标准分布上的性能，并有助于调整数据集参数。
每个数据集包含$10^7$条记录，分别从二维\Laplacian 分布和\Gaussian 分布中进行采样，
其中数据域大小分别为$256 \times 256$和$1024 \times 1024$。

\autoref{2-dim overall compare}展示了在二维\Laplacian 分布和\Gaussian 分布的结果。
我们采用与\cite{yang2020answering}相同的相关系数$r=0.8$。
由于\myuni 和\mycalm 的MSE超出了图表范围，因此我们省略了它们的结果。
根据结果，我们有以下观察结果。
1）在不同的隐私预算设置下，\myahead 表现优于\myHDG 。
\myHDG 利用粗粒度的二维网格对二维数据域进行划分，并刻画两个属性之间的相关信息。
当数据域大小为$1024 \times 1024$时，\myHDG 的最精细粒度是$8 \times 8$。
采用粗粒度的网格进行划分会导致数据相关性信息丢失。
相比之下，\myahead 针对具有不同频率值的子域使用多粒度分解方式。
对于一个用户数据集中分布的子域，\myahead 采用细粒度分解，以更准确地提取数据相关性。
2）\myahead 对于不同数据域大小具有鲁棒性。
一方面，更大的数据域意味着需要将用户划分为更多的组。
例如，对于$256 \times 256$的数据域，\myahead 将用户划分为8组。
而对于$1024 \times 1024$的数据域，\myahead 将用户划分为10组。
对于更大的数据域，每个组中的用户数量较少，这将增加频率值中添加的噪声。
另一方面，频率小于阈值的子域不会被进一步划分。
我们使用多个用户组的上传数据估计这些子域的频率值。
在后向处理中进行加权平均处理后，这些子域的噪声误差将变为原始噪声误差的$\frac{1}{\beta}$，其中$\beta$是估计次数。
% 通过比较\autoref{2-dim overall compare}中的两个相邻子图，阈值设置的影响比域大小变化的影响更大。因此，\myahead的MSE几乎不随不同域大小变化而变化，我们选择不同的域大小来进一步验证这一事实，详见\autoref{Impact of Domain Size}。

\mypara{不同属性相关性下的算法查询精度}
我们在\autoref{2-dim correlation}中展示了在不同属性相关性下查询误差。
在每个子图中，我们使用固定的隐私预算$\epsilon=1.1$，并将相关系数$r$从0.1（弱相关）变化到0.9（强相关）。
基于实验结果，我们总结出以下结论。
1）\myahead 的MSE几乎不随属性相关性而改变。
\myahead 同时分解了两个维度，因此更好地保护了数据的相关性。
2）\myHDG 的数据效用会随着属性相关性的增加而降低，特别是在属性相关性较强的情况下。
\myHDG 结合了更细粒度的一维网格来估计频率分布。
如果两个属性之间的相关性较弱，细粒度的一维网格可以弥补粗粒度的二维网格的精度不足的缺陷。
当属性之间的相关性较强时，缺少属性相关性信息的一维网格可能会对查询精度造成负面影响。
% 有趣的是，相关系数$r=0.5$似乎是\myHDG 和\myahead 的交点，这可能引导聚合器根据属性的相关性选择更好的算法。
% 对于高维场景，我们还在\autoref{Impact of Attribute Correlation}中评估了不同属性相关性对查询误差的影响，其中\myahead 的反应与二维场景类似。

\subsection{高维数据场景}
\label{Effectiveness of ahead for high-dimensional Range Query}
\begin{figure*}[h!]
    \centering
    \subfigure[\Laplacian~(3), $10^6$, vary $\epsilon$]{\label{fig:Three_Laplace08-Set_10_6-Domain_6_6_6}\includegraphics[width=0.24\hsize]{figure/ldp_range_query/figures_experiment_result/overall_result/0406_CI_Rand_ep-Three_Laplace08-Set_10_6-Domain_6_6_6.pdf.pdf}}
    \subfigure[\Laplacian~(3), $10^7$, vary $\epsilon$]{\label{fig:Three_Laplace08-Set_10_7-Domain_6_6_6}\includegraphics[width=0.24\hsize]{figure/ldp_range_query/figures_experiment_result/overall_result/0406_CI_Rand_ep-Three_Laplace08-Set_10_7-Domain_6_6_6.pdf.pdf}}
    \subfigure[\Gaussian~(3), $10^6$, vary $\epsilon$]{\label{fig:Three_Normal08-Set_10_6-Domain_6_6_6}\includegraphics[width=0.24\hsize]{figure/ldp_range_query/figures_experiment_result/overall_result/0406_CI_Rand_ep-Three_Normal08-Set_10_6-Domain_6_6_6.pdf.pdf}}
    \subfigure[\Gaussian~(3), $10^7$, vary $\epsilon$]{\label{fig:Three_Normal08-Set_10_7-Domain_6_6_6}\includegraphics[width=0.24\hsize]{figure/ldp_range_query/figures_experiment_result/overall_result/0406_CI_Rand_ep-Three_Normal08-Set_10_7-Domain_6_6_6.pdf.pdf}}
    \subfigure[\Laplacian~(5), $10^6$, vary $\epsilon$]{\label{fig:Five_Laplace08-Set_10_6-Domain_6_6_6_6_6}\includegraphics[width=0.24\hsize]{figure/ldp_range_query/figures_experiment_result/overall_result/0406_CI_Rand_ep-Five_Laplace08-Set_10_6-Domain_6_6_6_6_6.pdf.pdf}}
    \subfigure[\Laplacian~(5),$10^7$, vary $\epsilon$]{\label{fig:Five_Laplace08-Set_10_7-Domain_6_6_6_6_6}\includegraphics[width=0.24\hsize]{figure/ldp_range_query/figures_experiment_result/overall_result/0406_CI_Rand_ep-Five_Laplace08-Set_10_7-Domain_6_6_6_6_6.pdf.pdf}}
    \subfigure[\Gaussian~(5), $10^6$, vary $\epsilon$]{\label{fig:Five_Normal08-Set_10_6-Domain_6_6_6_6_6}\includegraphics[width=0.24\hsize]{figure/ldp_range_query/figures_experiment_result/overall_result/0406_CI_Rand_ep-Five_Normal08-Set_10_6-Domain_6_6_6_6_6.pdf.pdf}}
    \subfigure[\Gaussian~(5), $10^7$, vary $\epsilon$]{\label{fig:Five_Normal08-Set_10_7-Domain_6_6_6_6_6}\includegraphics[width=0.24\hsize]{figure/ldp_range_query/figures_experiment_result/overall_result/0406_CI_Rand_ep-Five_Normal08-Set_10_7-Domain_6_6_6_6_6.pdf.pdf}}
    \subfigure{\includegraphics[width=0.85\textwidth]{figure/ldp_range_query/figures_others/comparison_3D_legend5.pdf.pdf}}  
    \vspace{-0.1cm}
    \caption{在不同的隐私预算下，在高维\Laplacian 和\Gaussian 数据集上对比不同方法的查询精度。 
    \de 和 \lle 分别代表两种不同的高维扩展方法，即直接估计算法和利用低维估计算法。
    其中，\myHDG 是对比方法。图中结果以对数刻度呈现。}
        % Comparison of different methods on high-dimensional \Laplacian and \Gaussian datasets under various privacy budgets. \de and \lle respectively represent two high-dimensional expansion methods, \ie, ``direct estimation'' and ``leveraging low-dimensional estimation''. \myHDG is a baseline method. The results are shown in log scale.
        
    % \vspace{-0.6cm}
    \label{3,5-dim overall compare}
\end{figure*}

\begin{figure*}[!h]
    \centering
    \subfigure[\Laplacian, $10^6$, $64^m$]{
    \includegraphics[width=0.23\hsize]{figure/ldp_range_query/figures_experiment_result/various_dimension/0416_CI_Rand-Attribute-Ten_Laplace08-Set_10_6.pdf.pdf}
    \label{fig: Attribute-Ten_Laplace08-Set_10_6}
    }
    \subfigure[\Laplacian, $10^7$, $64^m$]{
    \includegraphics[width=0.23\hsize]{figure/ldp_range_query/figures_experiment_result/various_dimension/0416_CI_Rand-Attribute-Ten_Laplace08-Set_10_7.pdf.pdf}
    \label{fig: Attribute-Ten_Laplace08-Set_10_7}
    }
    \subfigure[\Gaussian, $10^6$, $64^m$]{
    \includegraphics[width=0.23\hsize]{figure/ldp_range_query/figures_experiment_result/various_dimension/0416_CI_Rand-Attribute-Ten_Normal08-Set_10_6.pdf.pdf}
    \label{fig: Attribute-Ten_Normal08-Set_10_6}
    }
    \subfigure[\Gaussian, $10^7$, $64^m$]{
    \includegraphics[width=0.23\hsize]{figure/ldp_range_query/figures_experiment_result/various_dimension/0416_CI_Rand-Attribute-Ten_Normal08-Set_10_7.pdf.pdf}
    \label{fig: Attribute-Ten_Normal08-Set_10_7}
    }
    \subfigure{\includegraphics[width=0.55\textwidth]{figure/ldp_range_query/figures_others/comparison_3D_legend3.pdf.pdf}}  
    \vspace{-0.1cm}
    \caption{在不同的数据维度$m$下，不同方法的均方误差（MSE），其中 $\epsilon = 1.1$。结果以对数刻度呈现。
    % The MSE of different methods when varying data dimension $m$ with $\epsilon = 1.1$. The results are shown in log scale. 
    }
    % \vspace{-0.4cm}
    \label{fig: varying attribute dimension}
\end{figure*}

在本小节中，我们评估 \myahead 在高维场景下的表现。
基于\autoref{2-dim overall compare}和\autoref{2-dim correlation}的实验结果，\myHDG 和 \myahead 的查询精度对数据域大小不敏感。
因此，我们将数据域大小设置为 $|D|=2^6$ ，
并分别从高维 \Laplacian 和 \Gaussian 分布中采样 $10^6$ 和 $10^7$ 条记录的合成数据集进行实验。

\mypara{不同高维场景扩展方法下的算法查询精度}
回顾 \autoref{Extension to Multi-dimensional Settings} 中的两种扩展方法，即 \de 和 \lle。
我们比较了这两种高维扩展方法在三维范围查询下的 MSE。
对于高于三维的查询，因为\de 方法计算时间过长，我们在实验中只考虑\lle。
从 \autoref{3,5-dim overall compare} 中可以观察到，
\lle 下的 \myahead 比 \de 的 MSE 要低。
通过 \de 得到的子域是等边的高维立方体，
\myahead 倾向于使用底层的节点，导致在查询结果中积累了较多的误差。
例如，对于一个三维查询 $[1,1]\times[1,8]\times[1,8]$，其中数据集的范围为 $[1,8]\times[1,8]\times[1,8]$，\myahead 选择使用底层的 64 个叶节点来回答查询，
而不是使用具有更大子域的高层节点，这会导致误差的累积。
尽管 \lle 需要将用户分组，并从每个用户组上传数据中计算不同属性组合的频率分布。
相对于 \de， \lle 在估计每一层的频率分布时持有较少的用户上传数据，
但是 \lle 的最大熵优化步骤融合了更多的二维分布信息，得到了更小的 MSE。

\mypara{不同隐私预算下的算法查询精度}
\autoref{3,5-dim overall compare} 展示了算法在三维和五维数据集上的查询结果。
我们采用相关系数 $r=0.8$ 来量化属性之间的相关性，保持了与文献{\rm\parencite{yang2020answering}}相同的设置。

根据实验结果，我们总结出以下结论。
% 这些结果与我们在 \autoref{Extension to Multi-dimensional Settings} 中的分析一致。
1）\myahead 对数据分布的变化具有鲁棒性。
回顾\autoref{Selection parameters}中参数$\theta$和$B$的推导过程，\myahead 的参数设置对用户数据的分布没有特定要求。
当整个数据域被自适应地划分时，相同的$\theta$可以确保区间的频率具有类似的总体误差。
因此，从\autoref{fig:Three_Laplace08-Set_10_7-Domain_6_6_6}，\autoref{fig:Three_Normal08-Set_10_7-Domain_6_6_6}（或\autoref{fig:Five_Laplace08-Set_10_7-Domain_6_6_6_6_6}，\autoref{fig:Five_Normal08-Set_10_7-Domain_6_6_6_6_6}）来看，
\myahead 在不同的数据集上表现类似。
在相同的数据集参数下，\myHDG 使用相同粒度的一、二维网格来聚合用户数据，然后基于网格内用户满足均匀分布的假设回答查询。
因此，\myHDG 在频率分布更不均匀的\Laplacian 数据集上的均方误差大于其在\Gaussian 数据集上的均方误差。
2）对于高维查询，\myahead 的性能更依赖较大的用户数据规模。
回顾\autoref{Extension to Multi-dimensional Settings}，\myHDG 只需要将总体用户分成与两两属性组合数量相同的用户组，而\myahead 还需要用户组进一步划分到二维树的不同层。
对于一个三维数据集，属性数据域大小为$|D|=64$，\myahead 将用户随机划分为$C_3^2 \cdot 6$组，而\myHDG 只需将用户分为$C_3^2 + C_3^1$组。
这种情况下，\myahead 中每个组的用户记录数是\myHDG 组的一半，导致\myahead 算法中的噪声误差翻倍。
因此，从\autoref{fig:Three_Laplace08-Set_10_6-Domain_6_6_6}和\autoref{fig:Three_Laplace08-Set_10_7-Domain_6_6_6}中可以看出，
\myahead 相比于\myHDG 的查询精度优势随着用户记录减少而减小。

\mypara{不同数据维度下的算法查询精度}
我们评估了\myahead 和 \myHDG 在高维\Laplacian 和\Gaussian 数据集上的查询精度。其中，数据维度从三维变化到十维，属性相关系数为 $r=0.8$。
从\autoref{fig: varying attribute dimension} 可以看出，\myahead 对数据维度的变化具有鲁棒性。
回顾\autoref{Extension to Multi-dimensional Settings}，随着数据维度$m$的增加，属性组合数量增大。
\lle 需要将用户划分为更多的用户组，即降低了用于估计每个二维\myahead 树频率分布的用户数据数量。
例如，当$m=3$时，属性组合数为$C_3^2=3$，当$m=10$时，属性组合数为$C_{10}^2=45$。
另一方面，由于\lle 使用与高维查询相关的$2^m$个二维查询结果构建高维查询结果（回顾\autoref{Extension to Multi-dimensional Settings}），
因此，\lle 的最大熵优化步骤包含更多维度的查询结果信息。
综上，\lle 方法随着维度的增加，查询结果的MSE几乎保持不变。

\section{本章小结}
本文提出了一种新的本地差分隐私协议，其核心贡献是基于自适应层级分解解决了一维和多维范围查询问题。
我们的方法在满足严格的本地差分隐私保证的同时，实现了优越的查询精度。
通过理论分析和大量实验评估，我们展示了\myahead 可以很好的权衡范围查询的查询精度和用户数据的隐私保护强度，相对于现有方法具有显著的优势。
此外，通过不同的实验参数设置，我们总结出多个有助于在实际中部署\myahead 的结论。




% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=.3\linewidth]{example-image-a}
%     \caption{\label{fig:fig-placeholder}图片占位符}
% \end{figure}


\chapter{标准化数据集产权保护技术}
% 强化学习模型训练数据产权审查研究
本章摘要：
数据是人工智能中的关键资产，高质量的数据集可以提高机器学习模型的性能。
在像自动驾驶这样的安全关键应用中，模型通常依赖于离线深度强化学习（离线 DRL）使用预设的数据集，而不是在实际环境中进行训练。
为了支持这些模型的开发，许多机构提供了具有开放源代码许可证的数据集，但这些数据集容易受到潜在的滥用或侵权行为的威胁。
注入水印可能会保护数据的知识产权，但面临两个主要挑战：
（1）审计策略必须具有灵活性，以处理已经发布且无法事后更改的数据集；
（2）审计人员不能使用除正在接受审计的数据集之外的任何辅助数据集。

为了克服这些挑战，我们提出了 \sysnameo 轨迹级数据集审计机制。
我们发现累计奖励可以作为数据集的内生水印，以判断 DRL 模型是否基于特定数据集训练。
通过对多个离线 DRL 模型和任务进行综合实验，
我们展现了 \sysnameo 的有效性，审计的真正率和真负率均超过了 $90\%$。
另外，我们总结出多个实验结论，可帮助审计员在实际中部署 \sysnameo 。
最后，我们使用 \sysnameo 审计基于 Google 和 DeepMind 的开源数据集训练的离线强化学习模型，实验结果证明了所提方法在审计已发布实际数据集方面的有效性。

关键词：数据集审计；离线强化学习；产权保护

\section{引言}
\label{sec:intro}
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.5\hsize]{figure/orl_auditor/intuition-v1.pdf}
    \caption{
    % Intuitive explanation of \sysnameo. The middle surface is the cumulative rewards of the state-action pairs from a dataset. The auditor outputs a positive result if the cumulative rewards of a suspect model's state-action pairs are between the boundaries. 
    \sysnameo 机理的示意图。
    中间面是由基于数据集估计的状态-动作对的累计奖励值。
    如果可疑模型的状态-动作对的累计奖励落在两个边界面之间，
    则审计员会输出一个阳性结果，即模型使用了被审计数据。}
    \label{fig:intuition}
\end{figure}

深度强化学习（Deep reinforcement learning，DRL）已成功应用于许多复杂的决策任务，
例如，自动驾驶（autopilot）~\cite{FHOL18}、机器人控制（robot control）~\cite{A17}、金融交易（financial transactions）~\cite{DBKRD17} 和电子竞技（esports）~\cite{SSSAHGHBLB17,BBCCDDFFHHJGOPPPRSSSSSTWZ19}。
然而，涉及人身、财产安全的关键领域一般不允许未经充分训练的深度强化学习模型进行探索学习，原因一方面是模型在环境中探索需要花费大量的时间，另外可能会因为误操作导致生产事故。
例如，直接使用深度强化学习模型控制机械臂进行环境探索很可能会导致机械臂硬件或周围物体的损坏。

针对上述问题，研究人员提出了\textit{离线深度强化学习}（Offline Deep reinforcement learning，Offline DRL）\cite{DBLP:journals/corr/abs-2005-01643} 范式，
也称为全批次深度强化学习\cite{DBLP:books/sp/12/LangeGR12}，
该方法可以从由专家、人工控制器或甚至是符合系统约束条件的随机策略与环境交互的数据记录中进行学习。
为了促进Offline DRL模型的发展，第三方企业和研究机构已经发布一些高质量的数据集，
例如，DeepMind~\cite{DBLP:journals/corr/abs-2006-13888}、伯克利人工智能研究（BAIR）~\cite{DBLP:journals/corr/abs-2004-07219}、南栖仙策~\cite{DBLP:journals/corr/abs-2102-00714}和TensorFlow~\cite{tensorflow2015-whitepaper}。
这些数据集都是在严格的开源许可证下发布的。
例如，GNU通用公共许可证~\cite{DBLP:journals/corr/BeattieLTWWKLGV16}和Apache许可证~\cite{DBLP:journals/corr/abs-2006-13888, DBLP:journals/corr/abs-2004-07219, tensorflow2015-whitepaper}，以保护数据所有者的知识产权（Intellectual Property，IP）。
然而，恶意攻击者可能会获取这些数据集，训练自己的离线深度强化学习模型，并在封闭形式下从模型服务中获益，
这将违反开源许可证并侵犯数据所有者的知识产权。
而抵御这种攻击的关键在于“数据集所有者如何证明可疑模型是基于其数据集训练的”。

一种直观的方法是在数据集发布前注入特定分布的样本，例如基于注入指纹的方法~\cite{DBLP:conf/uss/000100022, DBLP:conf/uss/WangYVZZ18}。
但是，由于开源数据集已经发布，审计员需要设计一个事后机制以满足实际需求，而不是类似于注入指纹的事前机制。
此外，审计员很难保证有效的指纹具有与原始数据集一致的分布，这必然会干扰训练模型的在正常任务上的表现。

我们提出了首个面向离线强化学习模型 (\sysnameo) 的实用数据集审计方法。
所提方法的核心思想是数据集中状态-动作对的期望累计奖励可以作为数据集的固有特征，
因此，可以使用期望累计奖励来区分在特定数据集上训练的模型。
\autoref{fig:intuition} 提供了 \sysnameo 的原理示意图，其中状态、动作和累计奖励组成了一个三维空间。
中间面表示数据集中状态-动作对的真实累计奖励，而其他两个曲面则显示了审计决策边界。
对于一个可疑的模型，如果它的状态-动作对的累计奖励落在两个边界之间，
审计员会输出一个阳性结果，即数据被用于训练这个模型；
否则，输出一个阴性结果。

为了进行审计，我们首先训练一个评论家模型来预测目标数据集中的状态-动作对的累计奖励。
接下来，直观的想法是将可疑模型的状态-动作对的累计奖励与目标数据集进行比较，
并通过预设的相似性判断阈值得出审计结果。
然而，找到一个合适的阈值十分困难，因为它取决于预先收集的数据集的分布，这些分布可能因不同的任务设置、收集过程和数据后处理方法而异。
模型中嵌入的状态-动作对的累计奖励是从目标数据集中采样得到的，因为离线DRL模型在训练期间拟合了数据集的累计奖励。
因此，我们在目标数据集上使用不同的模型初始化和优化方法训练多个阴影模型来收集模型拟合的累计奖励。
通过比较可疑模型和阴影模型的累计奖励，我们基于假设检验方法来做出最终的审计决策。

实验结果表明，\sysnameo 在多个深度强化学习模型和任务中，整体真正率（True Positve Rate, TPR）和真负率（True Negative Rate, TNR）均超过 $90\%$。
通过对训练于不同数据集上的阴影模型的累计奖励进行可视化，我们证明了累计奖励可以作为是数据集审计的依据。
我们进一步评估了\sysnameo 实际应用的两个影响因素，即阴影模型数量和显著性水平。
首先，随着阴影模型数量的增加，审计准确度会提高，而\sysnameo 只需要15个阴影模型就表现很好。
其次，\sysnameo 的最小显著性水平 $\alpha$ 约为 0.001，这意味着审计员可以每次以 $99.9\%$ 的置信度输出审计结果。
为了评估\sysnameo 的鲁棒性，我们使用高斯噪声扰动可疑模型的输出，其中输出空间的所有维度都被归一化为$[-1,1]$。
\sysnameo 受 $\mu=0$ 和 $\sigma=0.01$ 的高斯噪声影响较小。
对于 $\sigma=0.1$，\sysnameo 的TPR会显著下降，同时扰动噪声也会降低可疑模型的性能，特别是在复杂任务中。
我们进一步实现了\sysnameo 来审计Google~\cite{DBLP:journals/corr/abs-2004-07219}和DeepMind~\cite{DBLP:conf/nips/Gulcehre0NPCZAM20}的开源数据集，实验结果再次佐证了\sysnameo 在实践中的有效性。

综上，我们有三个方面的贡献。
\begin{itemize}
    \item 据我们所知，\sysnameo 是首个使用累计奖励作为离线 DRL 模型数据集指纹的轨迹级别数据集审计方法。
    \item 我们在四个离线 DRL 模型和三个任务上展示了 \sysnameo 的有效性，并系统地分析了各种实验因素，例如超参数设置和 \sysnameo 的对于可疑模型输出扰动的鲁棒性，归纳了一些有助于实际应用 \sysnameo 的结论。
    \item 我们通过在 DeepMind~\cite{DBLP:conf/nips/Gulcehre0NPCZAM20} 和 Google~\cite{DBLP:conf/nips/Pomerleau88} 的开源数据集上进行审计测试，表明 \sysnameo 可以作为针对离线 DRL 模型的一种有效的数据集审计方案。
\end{itemize}


% \section{预备知识}
% \label{sec:background}
% \subsection{离线强化学习问题}
% \label{sec:Offline Reinforcement Learning Problem}
% 离线强化学习（Offline Reinforcement Learning, offline RL）模型旨在从预设数据集 $D$ 中学习一个最优（或近似最优）策略，无需与交互式环境进行交互。
% 在离线 RL 模型中，利用预设数据集最大化累计奖励或其他用户提供的奖励信号。
% 我们使用 $\mathbb{S}$（或 $\mathbb{A}$）来表示状态（或动作）空间，其包含模型的所有可能状态（或动作）。
% $r_t \in \mathbb{R}$ 是每个时间步的奖励，其中 $\mathbb{R}$ 是实数集。
% 在预设数据集中，最小的数据单位为迁移（Transition），它是一个由四个元素构成的集合：$\{s_t, a_t, r_t, s_{t+1}\}$。
% 其中 $s_t \in \mathbb{S}$，$a_t \in \mathbb{A}$，$s_{t+1} \in \mathbb{S}$ 是 $s_t$ 的下一个状态。
% 而一组按时间顺序排列的迁移构成了数据集 $D$ 中的一条轨迹（Trajectory）。
% 基于数据集中的轨迹信息，离线 RL 模型刻画数据集中蕴含的马尔可夫决策过程（Markov Decision Process, MDP），
% 并形成行为策略 $\pi_\theta(a\mid{s})$ 来最大化 $J(\pi)$。
% \begin{equation}
%     J(\pi)=\mathbb{E}_{{s_t} \sim d_{\beta}({s,~a}),~{a_t} \sim \pi_\theta({a} \mid {s})}\left[\sum_{t=0}^{{H}} \gamma^{t} r_t\right]
% \end{equation}
% 这里我们使用 $d_{\beta}$ 表示数据集 $D$ 中状态和动作的分布，其中动作根据行为策略 ${a_t}\sim \pi_\theta(a\mid{s})$ 采样得到。
% 折扣因子 $\gamma$ 用于折现累计奖励中的未来奖励。
% $H$ 是一个轨迹的终止时间步长。

% \mypara{例子}
% \autoref{fig:running example}展示了一个模型学习“CartPole”任务的例子\footnote{\url{https://www.gymlibrary.dev/environments/classic_control/cart_pole/}}。
% 在数据收集过程中，数据集是基于操作员和环境之间的操作日志产生的。
% 其中包含小车和杆的位置和速度（即状态）、操作员的施力方向（即动作）以及相应的奖励。
% 然后，在训练和评估过程中，离线 RL 模型仅从收集的数据集中学习如何玩“CartPole”任务。
% 最后，我们将经过训练的离线 RL 模型部署到环境中执行任务。

% \begin{figure}[!t]
%     \includegraphics[width=\hsize]{figure/orl_auditor/offline-rlch.pdf}
%     \caption{离线 DRL 模型的运行示例。}
%     \label{fig:running example}
% \end{figure}

% \subsection{离线强化学习模型}
% \label{sec:offline reinforcement learning model}
% 在本节中，我们首先分别介绍两种离线 RL 算法~\cite{DBLP:conf/icml/FujimotoMP19, DBLP:journals/corr/abs-1910-01708, DBLP:conf/iclr/KostrikovNL22}，它们分别代表了离线 RL 模型的两种基本思想，即策略约束方法（Batch-Constrained Q-learning，BCQ）和值函数正则化方法（Implicit Q-Learning，IQL）~\cite{DBLP:journals/corr/abs-2203-01387}。
% 许多最先进的离线 RL 模型~\cite{DBLP:conf/nips/YuKRRLF21, DBLP:conf/nips/KidambiRNJ20, DBLP:conf/nips/FujimotoG21, DBLP:conf/iclr/KostrikovNL22}都是基于这两种方法进行改进。
% 接着，我们介绍一种新的极简且实用的算法，即融合行为克隆的时序差分学习方法（Temporal-Difference Learning 3 Plus Behavior Clone，TD3PlusBC）~\cite{DBLP:conf/nips/FujimotoG21}，该方法在较低的计算和超参数调试开销下实现了一流的性能。
% 此外，行为克隆方法（Behavior Clone，BC）~\cite{DBLP:conf/nips/Pomerleau88}通过有监督学习方式拟合数据集中的状态-动作分布。
% 尽管 BC 不属于典型的强化学习方法，但它可以解决离线 RL 问题，通常作为离线 RL 评估的基准方法。

% \mypara{行为克隆方法\cite{DBLP:conf/nips/Pomerleau88}}
% BC将数据集中成对的状态$s$和动作$a$分别作为输入和标签，基于\autoref{eq:BC}优化策略。
% \begin{equation}
%     \theta^*=\arg \min _\theta \mathbb{E}_{(s, a) \sim B}\left[\mathcal{L}\left(\pi_\theta(s), a\right)\right]
%     \label{eq:BC}
% \end{equation}
% 其中，$D$ 是离线强化学习数据集，$\mathcal{L}$ 是损失函数。
% 由于BC只是拟合数据集中状态-动作分布，其性能一般受限于采集数据集时所使用的策略。

% \mypara{策略约束方法\cite{DBLP:conf/icml/FujimotoMP19, DBLP:journals/corr/abs-1910-01708}} 
% BCQ是第一个具有实用意义的离线强化学习算法。
% BCQ的关键思想是整合生成模型来实现批量约束的概念，即最小化候选动作与数据集中动作之间的偏差。
% 为了保持动作的多样性，BCQ构建了一个扰动模型来扰动每个选择的动作。
% 然后，BCQ通过一个$Q$网络选择具有最高价值的动作，该网络学习估计给定状态和动作对的预期累积奖励。
% 因此，BCQ的目标函数如下。
% \begin{equation}
%     \begin{aligned}
%     \pi(s)= & \underset{a_i+\xi_\phi\left(s, a_i, \Phi\right)}{\operatorname{argmax}} Q_\theta\left(s, a_i+\xi_\phi\left(s, a_i, \Phi\right)\right) \\
%     & \left\{a_i \sim G_\omega(s)\right\}_{i=1}^n
%     \end{aligned}
% \end{equation}
% 其中，$G_\omega(s)$ 是基于条件变分自编码器（Variational Autoencoder，VAE）\cite{DBLP:journals/corr/KingmaW13}的生成模型，$Q_\theta$是值函数，从$G_\omega(s)$生成的$n$个候选动作中选择价值最高的动作。
% $\xi_\phi\left(s, a_i, \Phi\right)$是扰动模型，它可以在$[-\Phi,\Phi]$范围内输出对动作$a$的调整。
% 然后，可以通过确定性策略梯度算法\cite{DBLP:conf/icml/SilverLHDWR14}来优化扰动模型，如下所示。
% \begin{equation}
%     \phi \leftarrow \underset{\phi}{\operatorname{argmax}} \sum_{(s, a) \in \mathcal{B}} Q_\theta\left(s, a+\xi_\phi(s, a, \Phi)\right)
% \end{equation}
% 其中，$\mathcal{B}$表示数据集中的小批量状态-动作对。
% 为了惩罚稀有状态，BCQ采用两个$Q$网络输出的凸组合，并设置一个新的目标值$y$来进一步更新这两个$Q$网络。
% \begin{equation}
%     y = r+\gamma \max _{a_i}\left[\lambda \min _{j=1,2} Q_{\theta_j^{\prime}}\left(s^{\prime}, a_i\right)+(1-\lambda) \max _{j=1,2} Q_{\theta_j^{\prime}}\left(s^{\prime}, a_i\right)\right], \nonumber
% \end{equation}
% 其中，$a_i$对应于从生成模型$G_\omega(s)$中采样的扰动动作。

% \mypara{值函数正则化方法~\cite{DBLP:conf/iclr/KostrikovNL22}}
% 与BCQ~\cite{DBLP:conf/icml/FujimotoMP19, DBLP:journals/corr/abs-1910-01708}的批次约束思想相比，
% IQL采用了一种避免查询数据集中未见动作值的方法。
% 因此，IQL从考虑适应$Q$网络的方程\autoref{eq:sarsa}开始。
% \begin{equation}
%     L(\theta)=\mathbb{E}_{\left(s, a, s^{\prime}, a^{\prime}\right) \sim \mathcal{D}}\left[\left(r+\gamma Q_{\theta^\prime}\left(s^{\prime}, a^{\prime}\right)-Q_\theta(s, a)\right)^2\right], 
%     \label{eq:sarsa}
% \end{equation}
% 其中，$s^{\prime}$和$a^{\prime}$是$s$和$a$的接续状态和动作。
% $\theta^\prime$是$\theta$的一个快照，每隔几次$\theta$的更新就从$\theta$中复制一次。
% IQL通过expectile回归来优化状态值函数。
% \begin{equation}
%     L_V(\psi)=\mathbb{E}_{(s, a) \sim D}\left[L_2^\tau\left(Q_\theta(s, a)-V_\psi(s)\right)\right], \nonumber
% \end{equation}
% 其中，$L_2^\tau(u)=|\tau-\mathbbm{1}(u<0)| u^2$。
% 最后，采用优势加权回归（Advantage-weighted Regression）更新策略函数。
% \begin{equation}
%     L_\pi(\phi)=\mathbb{E}_{(s, a) \sim \mathcal{D}}\left[\exp \left(\beta\left(Q_{\theta}(s, a)-V_\psi(s)\right)\right) \log \pi_\phi(a \mid s)\right]
% \end{equation}
% 在IQL的工作流程中，首先交替优化参数$\theta$和$\psi$，然后在固定$\theta$和$\psi$的情况下更新$\phi$。

% \mypara{融合行为克隆的时序差分学习方法~\cite{DBLP:conf/nips/FujimotoG21}}
% 先前的方法~\cite{DBLP:conf/icml/FujimotoMP19, DBLP:journals/corr/abs-1910-01708, DBLP:conf/iclr/KostrikovNL22} 通过限制或规范化动作选择来使学习到的策略更易于使用给定数据集进行评估。
% 然而，它们引入了新的超参数，并引入附加组件（例如生成模型）。
% 在应用时，需要对已部署的离线强化学习算法做出较大的调整。
% TD3PlusBC是一种基于TD3~\cite{DBLP:conf/icml/FujimotoHM18}的离线强化学习算法，使用BC正则化项限制动作数据增强，将优化策略约束在接近数据集$D$的状态-动作空间。
% \begin{equation}
%     \pi=\underset{\pi}{\operatorname{argmax}} \mathbb{E}_{(s, a) \sim \mathcal{D}}\left[\lambda Q(s, \pi(s))-(\pi(s)-a)^2\right]
% \end{equation}
% 对于包含 $N$ 个迁移 $\left(s, a\right)$ 的数据集，
% $\lambda=\frac{\alpha}{\frac{1}{N} \sum_{\left(s, a\right)}\left|Q\left(s, a\right)\right|}$。
% 为了方便策略训练，TD3PlusBC 对给定数据集中的每个状态进行规范化，
% 其中 $s_i=\frac{s_i-\mu}{\sigma+\epsilon}$，$\mu$ 和 $\sigma$ 分别是均值和标准差。

% 在评估中，我们的实验主要针对以上四种算法进行。
% 然而，只要审核人员具有对可疑模型的黑盒访问权限，
% \sysname 可以应用于任何类型的离线 DRL 模型。



\section{问题建模}
\label{sec:overview}
\subsection{审计员能力假设}
审计员掌握目标数据集的所有信息，例如轨迹数量以及状态和动作的范围。
在离线强化学习设置中，审计员禁止与在线环境交互以收集更多数据，这意味着整个审计过程仅依赖于目标数据集。
我们考虑审计员对可疑模型具有黑盒访问权限，这是最实际和具有挑战性的情景。
典型的应用场景是，数据集盗用者（攻击者）基于数据集训练离线强化学习模型，并通过为模型使用者提供服务获取收益。
审计员可以利用数据集的状态（输入）来查询可疑模型并获取相应的动作（输出）。

\subsection{应用场景}
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.7\hsize]{figure/orl_auditor/system-v1ch.pdf}
    \caption{应用场景示例。
    由于审计员受雇于机构1，因此审计员可以获取有关数据集$D_1$的所有信息，但不了解其他机构的数据集。}
    \label{fig:application scenario}
\end{figure}
\autoref{fig:application scenario}展示了一个典型的应用场景，数据提供方收集数据后将其发布或销售给客户。
拥有数据集访问权限的恶意客户（攻击者）会进行盗版分发或非法构建模型即服务平台（Model as a Service，MaaS）。
机构1怀疑这些模型是由其数据集$D_1$产生的，因此雇用审计员来确定模型训练者是否盗取了数据集$D_1$。

\subsection{讨论}
在DNN场景中，对于数据集的研究粒度可以分为样本级和数据集级。
在强化学习场景中，数据集具有轨迹级数据，这是操作员和环境之间顺序交互的最小记录单位。
由于单个轨迹记录了操作员从初始状态到终止状态与环境交互的全过程。
因此轨迹级数据可以被视为数据集的价值单位。
\sysnameo 以轨迹级粒度来审计数据集，决定可疑模型是否使用数据集中的特定轨迹。
此外，审计员可以通过设置盗版报警阈值将 \sysnameo 扩展到数据集级数据。
例如，如果轨迹级数据的侵权比例超过预设阈值，则审计员可以判定存在数据集级别的盗用行为。


\section{本文方法（\sysnameo）}
在 \autoref{fig:intuition} 中，我们用累计奖励来实例化 $Q$。
累计奖励作为数据集的内在特征，满足本章所提场景的审计要求。
对于 $\Delta$，我们基于在数据集上训练的阴影模型构建决策边界，而不是预设的阈值以适应不同数据集的分布。
因此，适合的审计依据 $Q$ 和决策边界 $\Delta$ 保证了 \sysnameo 的适应性和有效性。
接下来，我们介绍 \sysnameo 的工作流程。

\subsection{工作流程}
\label{sec:workflow}
\begin{figure*}[!ht]
    \includegraphics[width=\hsize]{figure/orl_auditor/framework-v8ch.pdf}
    \caption{
    % The workflow of \sysname contains three steps, \ie, model preparation, cumulative reward collection, and audit process. 
    % \sysname first trains a set of shadow DRL models and a critic model on the target dataset, then collect the cumulative rewards from the state-action pairs of the shadow models and the suspect model. 
    % Finally, \sysname audits every trajectory based on hypothesis testing. 
    \sysnameo 的工作流程包含三个步骤，即模型准备、累计奖励收集和审计过程。
    首先，\sysnameo 在目标数据集上训练一组阴影深度强化学习模型和一个评论家模型，
    然后从阴影模型和可疑模型的状态-动作对中收集累计奖励。最后，\sysnameo 基于假设检验审计每个轨迹。
    }
    \label{fig:framework}
\end{figure*}
在下文中，我们将待审计的数据集记作“目标数据集”，将可疑模型实际使用的数据集称为“实际数据集”。
如果可疑模型是在目标数据集上训练的，那么实际数据集与目标数据集相同，即审计结果为阳性；
否则，审计员无法判定可疑模型使用目标数据集，即审计结果阴性。

\mypara{第一步：模型准备}
在\autoref{fig:framework}的最左侧框中，
审计员在目标数据集训练阴影模型，该数据集包含$m$条轨迹$T$，每条轨迹的长度为$n_i$（$i \in \{1,2,\dots, m\}$）。
审计员优化评论家模型用来预测每个状态-动作对的累计奖励。
对于数据集中的每条轨迹，其状态-动作对的一系列预测累计奖励构成审计特征。
目前有两种训练评论家模型的方法，
即基于蒙特卡罗的（MC-based）和基于时序差分的（TD-based）学习算法。
我们采用基于时序差分的学习方法，并在\autoref{sec:the selection of critic model}中展开说明。
此外，审计员使用不同的模型初始化训练一组影子模型，遵循\autoref{sec:background}中介绍的模型目标函数。

\mypara{第二步：累计奖励收集}
在中间的方框中，审计员将数据集的状态输入到影子模型并记录模型的行动。
对于数据集中的第$i$条轨迹，
审计员记录每个影子模型的状态$s^{i}t$和动作$a^{i}{t}$，
其中$a^{i}_{t}$表示第$i$条轨迹的第$t$步中影子模型的动作。
在完成动作收集后，审计员从影子模型中获得$k$组状态-动作对，
这些状态-动作对代表在目标数据集上使用不同初始化和训练过程学习的策略。
使用第一步中的评论家模型，审计员计算所有状态-动作记录的预测累计奖励值。
这些预测值可以看作是数据集中状态-动作对的真实累计奖励的采样。
类似地，审计员将状态$s^i_t$输入可疑模型并观察动作$a^{i}_t$。
然后将状态-动作对输入到评论家模型中，以预测可疑模型的状态-动作对的累计奖励。

\mypara{第三步：审计过程}
在完成上述两个步骤后，审计员从影子模型和可疑模型获得了估计的累计奖励，接着进行审计过程。
对于数据集的第$j$条轨迹（$j \in {1,2,\dots, m }$），
审计员从影子模型中收集$k$组估计的累计奖励，即${\mathbb{Q}_j^i \mid i \in {1,2,\dots, k}}$，
从可疑模型中收集一组估计的累积奖励，即$\mathbb{Q}_j^s$。
为了避免预设阈值，\sysnameo 根据$\mathbb{Q}_j^i$和$\mathbb{Q}_j^s$与$\bar{\mathbb{Q}}_j$的距离进行假设检验。
如果$d(\mathbb{Q}_j^s, \bar{\mathbb{Q}}_j)$不在${d(\mathbb{Q}_j^i, \bar{\mathbb{Q}}_j) \mid i \in {1,2,\dots, k}}$的分布范围内，则审计员可以对于可疑模型的数据集盗用嫌疑。
否则，审计员将输出阳性结果，即可疑模型在训练过程中使用该轨迹。
审计员对数据集中的其他轨迹重复实施上述审计过程，以获得包含所有轨迹判断的审计报告。

在审计过程中，选择的距离度量和假设检验方法对审计准确性起着关键作用。
合适的距离度量对估计的累计奖励之间的偏差更敏感，有助于后续的假设检验。
适当的假设检验方法可以在相同置信度情况下的提供更精确的审计结果。
因此，我们在\autoref{sec:the details of audit process}中进一步阐述距离度量和假设检验方法的选择依据。

\subsection{评论家模型}
\label{sec:the selection of critic model}
审计员可以使用蒙特卡罗（MC）或时序差分（TD）算法从数据集轨迹中训练评论家模型。
这两种方法之间的主要区别在于目标函数的差异。
对于基于MC的方法，学习目标$G$是直接基于数据集中的每条轨迹计算累计奖励。
\begin{equation}
    \begin{aligned}
    G(s_t, a_t)=r_{t}+\gamma r_{t+1}+\ldots+\gamma^{H-1} r_{H}
    \end{aligned}
\end{equation}
其中，$G(s_t, a_t)$代表从$(s_t, a_t)$到一个轨迹终止时间步$H$的准确累计奖励。
折扣因子$\gamma$用于折现未来的奖励。
评论家模型通过最小化\autoref{eq:critic}来进行训练。
\begin{equation}
    \label{eq:critic}
    \begin{aligned}
    L(\theta)=\mathbb{E}_{\left(s_t, a_t, r_{t+1}, s_{t+1}\right) \sim D}\left[\left(G\left(s_t, a_t\right)-Q_\theta\left(s_t, a_t\right)\right)^2\right]
    \end{aligned}
\end{equation}

基于TD的方法采用启发式算法更新评论家，学习目标为迁移奖励和评论家预测累计奖励的加权平均，即$r_{t}+\gamma Q\left(s_{t+1}, a_{t+1}\right)$。
因此，评论家模型通过最小化 进行训练。
\begin{equation}
    \label{eq:td critic}
    \begin{aligned}
    L(\theta)=\mathbb{E}_{\left(s_t, a_t, r_{t+1}, s_{t+1}\right) \sim D}\left[\left(r_{t+1}+\gamma Q_{\theta^{\prime}}\left(s_{t+1}, a_{t+1}\right)-Q_\theta\left(s_t, a_t\right)\right)^2\right]
    \end{aligned}
\end{equation}
我们随机初始化评论家模型的参数 $\theta$。
接着，评论家会计算 $Q_\theta\left(s_t, a_t\right)$ 和奖励 $r_{t+1}$的加权平均，并更新权重。$\theta^\prime$ 是 $\theta$ 的快照，
每隔几轮$\theta$的更新，$\theta^\prime$就复制 $\theta$ 一次。

基于MC方法利用数据集中的确切累计奖励来训练评论家模型，从而对状态-动作的累计奖励进行无偏估计。
由于$G_t$的定常性质，基于MC方法还具有较好的收敛性。
然而，基于MC方法无法应用于轨迹被人为截断的情况，即数据集中的所有轨迹在采集过程中必须自然结束。
在大多数实际场景中，
序列决策任务通常具有较长的时间步长甚至是无限时间步长，
因此数据集提供者需要将交互记录按预设最大长度进行截断。
基于TD方法解决了基于MC算法对于轨迹自然结束的限制，可以从非自然结束的序列中学习。
然而，由于基于TD方法使用了启发式更新，基于TD方法预测的累计奖励和实际的累计奖励存在一定的偏差，并且对参数初始化更加敏感。
为了弥补基于TD方法的缺陷，在\autoref{sec:workflow}中，我们选择阴影模型累计奖励的逐元素平均值$\bar{\mathbb{Q}}$作为审核指标，而不是仅仅依赖于评论家模型的预测。

\subsection{审计过程}
\label{sec:the details of audit process}
\begin{algorithm}[!t]
    \caption{\sysnameo 的工作流程}
    \label{alg:Generation Mechanism 2}
    \begin{algorithmic}[1]
    \REQUIRE 待审计数据集 $D$, 可疑模型 $\pi_s$, 影子模型数量 $k$, 假设检验的显著性水平 $\alpha$
    % Shadow DRL models $M$, environment $E$, time step $T$, hyper-parameter setting label set $Y$ , attack model $f_{\theta}^{p}$
    \ENSURE 轨迹级审计报告
    \STATE $//$ 第一步：模型准备
    \STATE 训练影子模型 $\{\pi_i \mid i=1,\dots,k\}$和评论家模型$Q$。
    % $S$ = \autoref{alg:Generation Mechanism 1}~($M$, $E$, $T$)
    \STATE $//$ 第二步：累计奖励收集
    \FOR {each model $\pi$ in $\{\pi_i \mid i=1,\dots,k\} \cup \{\pi_s\} $}
    \STATE 将 $s\in D$ 输入 $\pi$ 得到相应的动作
    \STATE 基于评论家模型 $Q$ 预测 $(s, a)$ 的累计奖励
    \STATE 记录预测的累计奖励 $\{\mathbb{Q}_j \mid j=1,\dots,m\}$.
    \ENDFOR
    \STATE $//$ 第三步：审计过程
    \STATE audit\_report = []
    \FOR {each trajectory in $\{T_j \mid j=1,\dots,m\}$}
    \STATE 计算 $\{\mathbb{Q}_j^i \mid i= 1,\dots, k\}$ 的逐元素平均值 $\bar{\mathbb{Q}}_j$
    \STATE 计算每一个 $\mathbb{Q}_j^i$ 和 $\mathbb{Q}_j^s$ 与 $\bar{\mathbb{Q}}_j$之间的距离 $d(\mathbb{Q}_j, \bar{\mathbb{Q}}_j)$ 
    \STATE $//$ 假设检验
    \STATE 对于 $\{d^i \mid i=1,\dots, k\}$ 和 $d^s$, 在显著性水平为$\alpha$的前提下判断可疑模型$\pi_s$是否盗用$T_j$
    \STATE audit\_report.append($j$-th audit result)
    \ENDFOR
    \STATE 返回 audit\_report
    \end{algorithmic}
\end{algorithm}
本节首先介绍如何选择距离度量，接着介绍两种假设检验方法。

\mypara{距离度量}
我们考虑三种距离度量，即$\ell_p$范数、余弦距离和Wasserstein距离。
$\ell_p$范数是测量向量之间距离的常用方式，即向量各个分量差的绝对值之和。
在强化学习场景中，状态和动作是序列数据，因此距离度量应该同时考虑到累计奖励的值和位置偏差。
然而，$\ell_p$范数可能无法从相同值集的序列角度反映出元素位置的不同。
余弦距离源于余弦相似度的，定义为两个向量之间夹角的余弦值。
余弦距离可以同时刻画向量的值和位置方面的差异。
然而，余弦距离使用两个向量的范数对内积进行了归一化，这削弱了累计奖励之间的数值差异。
Wasserstein距离，又称作搬土距离（Earth Mover's Distance，EMD），
是衡量两个区域上的概率分布之间差异的度量方法~\cite{DBLP:conf/iccv/RubnerTG98}。
\autoref{eq:Wasserstein}定义了Wasserstein距离。
\begin{equation}
    \label{eq:Wasserstein}
    \begin{aligned}
    l_1(u, v)=\inf _{\pi \in \Gamma(u, v)} \int_{\mathbb{R} \times \mathbb{R}}|x-y| \mathrm{d} \pi(x, y)
    \end{aligned}
\end{equation}
其中，$\Gamma(u,v)$ 是定义在 $\mathbb{R} \times \mathbb{R}$ 上的分布集合，其边际分布分别为 $u$ 和 $v$。
由于累积奖励是根据经验数据收集的，即确切的分布属性是未知的，
因此审计员应该使用位于指定值处的狄拉克Delta函数的加权和。
Wasserstein距离反映了累积奖励的数值和位置偏差，很好地适应了审计需求。
因此，我们默认推荐使用Wasserstein距离，并在\autoref{sec:evaluation}中比较不同的距离度量方法。

\mypara{假设检验方法}
在选择距离度量之后，审计人员会使用 $\mathbb{Q}_j^i$ 和 $\mathbb{Q}_j^s$ 与 $\bar{\mathbb{Q}}_j$ 的距离进行假设检验。
\begin{equation}
    \begin{aligned}
    &H_0: d(\mathbb{Q}_j^s, \bar{\mathbb{Q}}_j) \text{is not an outlier. }   \nonumber \\
    &H_1: d(\mathbb{Q}_j^s, \bar{\mathbb{Q}}_j) \text{is an outlier. }   \nonumber
    \end{aligned}
\end{equation}

一个直观的方法是利用 $3\sigma$ 原则，即正态样本应该在距离均值 $\mu_{d}$ 三倍标准差 $\sigma_{d}$ 的范围内分布。
$3\sigma$ 原则是一种高效的假设检验方法，但是均值 $\mu_{d}$ 很容易被离群值误导。
与 $3\sigma$ 原则相比，Grubbs 检验~\cite{grubbs1950sample} 是一种更为健壮的单变量数据集中检测离群值的假设检验方法。
如果 Grubbs 检验统计量 $d(\mathbb{Q}_j^s, \bar{\mathbb{Q}}_j)$ 超过基于显著性水平计算得到的检验统计量，
审计人员可以判断 $d(\mathbb{Q}_j^s, \bar{\mathbb{Q}}_j)$ 偏离了均值，即拒绝 $H_0$ 并输出阴性结果。
对于一组样本 $\{d_i \mid i=1,2,\dots,n\}$，Grubbs 检验通过一系列步骤确定离群值。

\begin{itemize}
    \setlength{\itemsep}{0pt}
    \setlength{\parsep}{0pt}
    \setlength{\parskip}{0pt}
    \item [1)] 计算均值 $\mu_{d}$ 和标准差 $\sigma_{d}$。
    \item [2)] 将 $\{d_i \mid i=1,2,\dots,n \}$ 中的 $d_i$ 按升序重新编号得到 $d^{(i)}$。
    \item [3)] 通过 $G=\frac{|d(\mathbb{Q}_j^s, \bar{\mathbb{Q}}_j)-\mu_{d}|}{\sigma_{d}}$ 计算 Grubbs 检验统计量。
    \item [4)] 如果 $G>\frac{n-1}{\sqrt{n}} \sqrt{\frac{t_{\alpha /(n), n-2}^2}{n-2+t_{\alpha /(n), n-2}^2}}$，则拒绝 $H_0$，即该轨迹没有用于训练可疑模型。
    在上式中，$t_{\alpha /(n), n-2}^2$ 表示当自由度为 $n-2$、显著性水平为 $\frac{\alpha}{n}$ 时 $t$-分布的上界值。
\end{itemize}

两种假设检验方法都基于假设距离值遵循高斯分布。
由于\sysnameo 场景中审核员的样本数量较少且实际分布未知，因此安德森-达宁检验（Anderson-Darling test）~\cite{stephens1974edf} 十分适合本文所提方法的需求。
在部署\sysnameo 时，我们首先采用安德森-达宁检验~\cite{stephens1974edf} 来确保影子模型的距离值满足高斯分布，之后进行假设检验。


\section{性能评估}
\label{sec:orl evaluation}
在本节中，我们评估了\sysname 在行为克隆和三个离线强化学习模型上的有效性，
即策略约束方法（BCQ）\cite{DBLP:conf/icml/FujimotoMP19}、值函数正则化方法（IQL）\cite{DBLP:conf/iclr/KostrikovNL22}和融合行为克隆的时序差分学习方法（TD3PlusBC）\cite{DBLP:conf/nips/FujimotoG21}。
首先，我们介绍所使用的数据集和实验设置。
接着，我们分别阐述每个实验的设计以及其相应的结果。
\subsection{实验设置}
\label{sec:orl experimental setup}
\subsubsection{任务}
\begin{table}[t]
    \caption{
    任务概述。
    “连续” 和 “离散” 分别说明了状态和动作的数据类型，并给出了相应的维度数。
    }
    \label{tab:overview-of-tasks}
    \setlength{\tabcolsep}{0.8em}
    \renewcommand{\arraystretch}{1.3}
    \footnotesize
    \centering
    \begin{tabular}{ccc}
    \toprule
    \textbf{任务名称}                                                 & \textbf{状态} & \textbf{动作} \\ \hline
    \begin{tabular}[c]{@{}c@{}}月球着陆\\ (连续版本)\end{tabular} & \begin{tabular}[c]{@{}c@{}}连续(6-dim)\\ 离散(2-dim)\end{tabular}          & 连续(2-dim)      \\ \hline
双足机器人行走                                                     & 连续(24-dim)           & 连续(4-dim)           \\ \hline
    % \begin{tabular}[c]{@{}c@{}}LunarLander\\ (Continuous)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Continuous(6-dim)\\ Discrete(2-dim)\end{tabular}          & Continuous(2-dim)      \\ \hline
    蚂蚁机器人行走                                                                & 连续(111-dim)          & 连续(8-dim)      \\ \bottomrule
    \end{tabular}
\end{table}
我们采用了Gym中的月球着陆（Lunar Lander）、双足机器人行走（Bipedal Walker）和蚂蚁机器人行走（Ant）三个任务。
这些任务在现在研究中被广泛使用~\cite{CGZXL21,IUQJAHN20,PTLBC18}，
覆盖离散、连续的状态和动作空间，状态和动作空间的维度从低维度（2 维）到高维度（111 维）。
\autoref{tab:overview-of-tasks}概括了三个任务的特征，接下来我们将详细介绍这三个任务。

\mypara{月球着陆任务}
该任务是将一艘宇宙飞船平稳地降落在目标降落区的两个旗标之间。
降落区中心坐标为（0,0）。
飞船有三个推进器；主发动机向下喷射，另外两个横向发动机向左和向右喷射。
飞船状态是一个八维向量，分别为飞船的x轴坐标、y轴坐标、x轴线速度、y轴线速度、角度、角速度、以及两个表示起落架是否与地面接触的布尔值。
飞船动作由一个两维向量刻画，每一个元素的取值范围为[-1,1]。
动作的第一维控制主发动机，
当值在[-1,0)之间时关闭发动机。
当值从0上升到1时，主发动机油门从50\%增加到100\%。
另外两个横向发动机由动作向量的第二维控制。
当值在[-1.0,-0.5)时，飞船左发动机点火；
当值在[0.5,1)时，飞船右发动机点火；
当值在[-0.5,0.5]时两个横向发动机不运行。
飞船从空中（屏幕顶部）平稳降落到着陆区的奖励约为140分。
如果飞船平稳降落到着陆区外，则只有起落架触地奖励每个10分。
考虑到主发动机运行需要耗费燃料，所以主发动机运行一帧需要扣除0.3分。
如果飞船坠毁，任务直接结束并扣除100分。

\mypara{双足机器人行走任务}
该任任务是操作一个四关节的双足机器人稳定快速地向前移动。
机器人由机身和两条腿组成。
每条腿都有两个关节，一个在髋关节，一个在膝盖处。
双足机器人的状态包括八个连续的物理变量，即机身角速度、角加速度、水平速度、垂直速度、关节位置和关节角速度、腿部接触地面状态，另外还有十个激光雷达测距数据。
双足机器人可以控制其每个髋关节和膝盖上的关节电机速度值（共四个），范围在[-1,1]之间。
该行走机器人从地图左端站立位置向右边的目标移动，平稳到达目的地的奖励约300分。
如果中途机器人摔倒，则扣除100分。
考虑到电机运行需要耗费电能，所以需要根据电机扭矩变化扣除一定分数。
任务的终止条件为机器人身体接触地面或行走机器人到达目的地。

\mypara{蚂蚁机器人行走任务}
在这个任务中，玩家需要操纵一个蚂蚁机器人向地图的右侧移动。
蚂蚁机器人由躯干和四条腿组成，每条腿都有两个关节。 
蚂蚁机器人的状态包括不同身体部位的位置值，然后是对应的速度值。 
默认情况下，状态是一个111维的向量，分别为位置（1-dim），角度（12-dim），速度（14-dim）以及各个接触点力（力矩）的信息（84-dim）。 
玩家可以在每条腿的两个连接件和躯干共八个转轴处上施加扭矩。
所以动作空间是一个八维连续向量，表示施加在关节上的扭矩。 
该任务的奖励由四部分组成：机器人健康奖励、机器人前进奖励、驱动成本和扭矩惩罚。 
所以，总奖励=健康奖励+前进奖励-驱动成本-扭矩惩罚。 
任务的终止条件是蚂蚁的状态不健康或交互达到1000个时间步长。

\subsubsection{数据集和离线强化学习模型生成}
为了获取\autoref{tab:overview-of-tasks}中任务的数据集，我们采用了与现有数据集发布者相同的方法~\cite{DBLP:journals/corr/abs-2006-13888, DBLP:journals/corr/abs-2004-07219, DBLP:journals/corr/abs-2102-00714, tensorflow2015-whitepaper}，
即在交互环境中训练在线RL模型，并记录交互数据构成数据集。
然后，离线RL模型从数据集中学习。 
\autoref{tab:dataset generation and offline model preparation}总结了整个过程。 
对于每个任务，我们使用五个全局随机种子分别训练五个在线RL模型。 
接着，我们使用五个在线RL模型与环境交互并收集数据集，其中每个在线RL模型对应一个数据集。 
为了方便阅读，在线RL模型和其产生的数据集共享相同的名称。 
在每个数据集上，我们使用不同的全局随机种子训练30个离线RL模型。 
所有在线和离线RL模型都由开源RL库实现，并使用默认的超参数设置~\cite{stable-baselines3, d3rlpy}。

\begin{table}[!ht]
    \centering
    \caption{数据集和离线强化学习模型生成的主要步骤以及输入和输出的详细信息。}
    % \mc{Change this into a figure?}}
    \label{tab:dataset generation and offline model preparation}
    \setlength{\tabcolsep}{0.8em}
    \renewcommand{\arraystretch}{1.3}
    % \footnotesize
    \small
    \centering
    \begin{tabular}{c}
    \hline
    % \multicolumn{1}{|c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}The   interactive enviroment of Gym for \\ each task in \autoref{tab:overview-of-tasks}\end{tabular}}}                                                                                              \\
    \multicolumn{1}{|c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}
        对于每个任务和 \\ 离线RL模型的组合\end{tabular}}}                                                                                              \\
    \multicolumn{1}{|c|}{}                                                                                                                                                                                                                                                               \\ \hline
    $\downarrow$                                                                                                                                                                                                                                                                         \\ \hline
    \multicolumn{1}{|c|}{使用5个随机种子训练 $\{0, 1, \cdots , 4\}$}                                                                                                                                                                                                                       \\ \hline
    $\downarrow$                                                                                                                                                                                                                                                                         \\ \hline
    \multicolumn{1}{|c|}{\textbf{5个在线RL模型}的具体信息在 \autoref{tab:the online models}}                                                                                                                                                                             \\ \hline
    $\downarrow$                                                                                                                                                                                                                                                                         \\ \hline
    \multicolumn{1}{|c|}{使用1个随机种子收集数据集$\{0\}$}                                                                                                                                                                                                                                  \\ \hline
    $\downarrow$                                                                                                                                                                                                                                                                         \\ \hline
    \multicolumn{1}{|c|}{\textbf{5x1个离线数据集}的具体信息在 \autoref{tab:the offline datasets}}                                                                                                                                                                          \\ \hline
    $\downarrow$                                                                                                                                                                                                                                                                         \\ \hline
    \multicolumn{1}{|c|}{使用30个随机种子训练$\{42, 43, \cdots , 72\}$}                                                                                                                                                                                                                       \\ \hline
    $\downarrow$                                                                                                                                                                                                                                                                         \\ \hline
    \multicolumn{1}{|c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{5x1x30个离线RL模型}的具体信息在 \\ \autoref{tab:BC models}, \autoref{tab:BCQ models}, \autoref{tab:IQL models}, and \autoref{tab:TD3PlusBC models}\end{tabular}}}  
    \\ 
    \multicolumn{1}{|c|}{}   
    \\ \hline
    % \multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}10x1x30 offline RL models detailed in \autoref{tab:BC models}, \autoref{tab:BCQ models},\autoref{tab:IQL models}, and \autoref{tab:TD3PlusBC models}\end{tabular}} \\ \hline
    \end{tabular}
\end{table}

\begin{table*}[t]
    \small
    % \footnotesize
    \centering
    \caption{
        % The details of the online models for generating the offline datasets. The model performance shows the cumulative reward for 10 separate evaluations. 
        用于生成离线数据集的在线模型信息。其中模型性能展示了 10 次独立测试结果的均值和方差。
        }
    \label{tab:the online models}
    \setlength{\tabcolsep}{0.3em}
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{ccccc} 
    \toprule
    \textbf{任务名称}              & \textbf{在线模型} & \textbf{训练次数}  & \textbf{模型名称} & \textbf{模型性能}  \\ 
    \hline
    \multirow{5}{*}{Lunar Lander}   & \multirow{5}{*}{SAC}  & \multirow{5}{*}{1e6} & 1171                & 275.47$\pm$14.38            \\
                                    &                       &                      & 2094                & 50.79$\pm$65.95             \\
                                    &                       &                      & 4496                & 195.02$\pm$143.15           \\
                                    &                       &                      & 6518                & 246.40$\pm$33.91            \\
                                    &                       &                      & 9906                & 209.33$\pm$91.73            \\ 
    \hline
    \multirow{5}{*}{Bipedal Walker} & \multirow{5}{*}{PPO}  & \multirow{5}{*}{1e6} & 0841                & 285.55$\pm$60.84            \\
                                    &                       &                      & 1203                & 286.94$\pm$53.46            \\
                                    &                       &                      & 2110                & 283.58$\pm$47.35            \\
                                    &                       &                      & 3813                & 235.88$\pm$103.83           \\
                                    &                       &                      & 6558                & 285.16$\pm$65.92            \\ 
    \hline
    \multirow{5}{*}{Ant}            & \multirow{5}{*}{SAC}  & \multirow{5}{*}{2e6} & 2232                & 5377.70$\pm$1653.17         \\
                                    &                       &                      & 3569                & 1924.58$\pm$1180.96         \\
                                    &                       &                      & 4603                & 5531.45$\pm$844.10          \\
                                    &                       &                      & 5766                & 3025.89$\pm$547.36          \\
                                    &                       &                      & 7490                & 5897.37$\pm$477.34          \\
    \bottomrule
    \end{tabular}
    % \begin{tabular}{ccccc}
    % \hline
    % \multirow{2}{*}{\textbf{Task Name}}       & \multirow{2}{*}{\textbf{Online Model}} & \multirow{2}{*}{\textbf{Train Step}} & \multirow{2}{*}{\textbf{Model Name}} & \multirow{2}{*}{\textbf{Model Performance}} \\
    %                                           &                                           &                                      &                                      &                                             \\ \hline
    % \multirow{10}{*}{LunarLander(Continuous)} & \multirow{10}{*}{SAC}                     & \multirow{10}{*}{1e6}                & 1171                                 & 275.47 +/- 14.38                            \\
    %                                           &                                           &                                      & 1395                                 & 255.08 +/- 48.98                            \\
    %                                           &                                           &                                      & 2094                                 & 50.79 +/- 65.95                             \\
    %                                           &                                           &                                      & 2097                                 & 255.96 +/- 28.82                            \\
    %                                           &                                           &                                      & 4496                                 & 195.02 +/- 143.15                           \\
    %                                           &                                           &                                      & 4632                                 & 265.80 +/- 19.75                            \\
    %                                           &                                           &                                      & 6518                                 & 246.40 +/- 33.91                            \\
    %                                           &                                           &                                      & 6620                                 & 100.22 +/- 118.90                           \\
    %                                           &                                           &                                      & 7963                                 & 70.08 +/- 107.04                            \\
    %                                           &                                           &                                      & 9906                                 & 209.33 +/- 91.73                            \\ \hline
    % \multirow{13}{*}{Bipedal Walker}          & \multirow{13}{*}{PPO}                     & \multirow{13}{*}{1e6}                & 0841                                 & 285.55 +/- 60.84                            \\
    %                                           &                                           &                                      & 0977                                 & 263.09 +/- 43.48                            \\
    %                                           &                                           &                                      & 1203                                 & 286.94 +/- 53.46                            \\
    %                                           &                                           &                                      & 1351                                 & 231.29 +/- 94.39                            \\
    %                                           &                                           &                                      & 1351                                 & 226.55 +/- 98.07                            \\
    %                                           &                                           &                                      & 2110                                 & 283.58 +/- 47.35                            \\
    %                                           &                                           &                                      & 2637                                 & 202.91 +/- 121.25                           \\
    %                                           &                                           &                                      & 3813                                 & 235.88 +/- 103.83                           \\
    %                                           &                                           &                                      & 5303                                 & 273.94 +/- 66.54                            \\
    %                                           &                                           &                                      & 6558                                 & 285.16 +/- 65.92                            \\
    %                                           &                                           &                                      & 6862                                 & 264.60 +/- 91.13                            \\
    %                                           &                                           &                                      & 6974                                 & 221.13 +/- 109.71                           \\
    %                                           &                                           &                                      & 7539                                 & 101.95 +/- 175.98                           \\ \hline
    % \multirow{10}{*}{Ant}                     & \multirow{10}{*}{SAC}                     & \multirow{10}{*}{2e6}                & 2232                                 & 5377.70 +/- 1653.17                         \\
    %                                           &                                           &                                      & 3468                                 & 2079.52 +/- 1180.56                         \\
    %                                           &                                           &                                      & 3569                                 & 1924.58 +/- 1180.96                         \\
    %                                           &                                           &                                      & 3883                                 & 4243.71 +/- 1077.31                         \\
    %                                           &                                           &                                      & 4603                                 & 5531.45 +/- 844.10                          \\
    %                                           &                                           &                                      & 5006                                 & 4003.50 +/- 1673.15                         \\
    %                                           &                                           &                                      & 5766                                 & 3025.89 +/- 547.36                          \\
    %                                           &                                           &                                      & 6896                                 & 4762.93 +/- 1487.85                         \\
    %                                           &                                           &                                      & 7490                                 & 5897.37 +/- 477.34                          \\
    %                                           &                                           &                                      & 9865                                 & 5687.55 +/- 1058.92                         \\ \hline
    % \end{tabular}
\end{table*}

\begin{table*}[!ht]
    \caption{离线数据集的详细信息}
    \label{tab:the offline datasets}
    % \scriptsize
    \small
    % Please add the following required packages to your document preamble:
    % \usepackage{multirow}
    % \footnotesize
    \setlength{\tabcolsep}{0.4em}
    \renewcommand{\arraystretch}{1.1}
    \centering
    \begin{tabular}{ccccc} 
        \toprule
        \textbf{任务名称}              & \textbf{迁移数量} & \textbf{数据集名称} & \textbf{轨迹数量} & \textbf{轨迹长度}  \\ 
        \hline
        \multirow{5}{*}{月球着陆任务}   & \multirow{5}{*}{5e5}           & 1171                 & 2175                            & 229.83$\pm$83.51               \\
                                        &                                & 2094                 & 578                             & 864.19$\pm$231.88              \\
                                        &                                & 4496                 & 1252                            & 399.30$\pm$240.88              \\
                                        &                                & 6518                 & 1878                            & 266.13$\pm$99.65               \\
                                        &                                & 9906                 & 1566                            & 319.21$\pm$231.06              \\ 
        \hline
        \multirow{5}{*}{双足机器人行走任务} & \multirow{5}{*}{1e6}           & 0841                 & 1019                            & 981.03$\pm$190.79              \\
                                        &                                & 1203                 & 1027                            & 973.07$\pm$118.42              \\
                                        &                                & 2110                 & 877                             & 1139.55$\pm$151.10             \\
                                        &                                & 3813                 & 887                             & 1126.63$\pm$379.05             \\
                                        &                                & 6558                 & 1041                            & 959.77$\pm$146.13              \\ 
        \hline
        \multirow{5}{*}{蚂蚁机器人行走任务}            & \multirow{5}{*}{2e6}           & 2232                 & 2093                            & 955.46$\pm$177.72              \\
                                        &                                & 3569                 & 3497                            & 571.66$\pm$375.40              \\
                                        &                                & 4603                 & 2096                            & 954.01$\pm$175.82              \\
                                        &                                & 5766                 & 2217                            & 901.84$\pm$236.93              \\
                                        &                                & 7490                 & 2103                            & 951.02$\pm$187.93              \\
        \bottomrule
    \end{tabular}
\end{table*}


\subsubsection{评论家模型}
我们采用全连接神经网络构建评论家模型，该模型有四个隐藏层，每层有1024个神经元。
我们使用Adam优化器以学习率为0.001优化评论家模型。
整个训练过程需要150次数据集迭代，并且学习率每50次迭代衰减一半。

\subsubsection{评估指标}
回顾\autoref{fig:application scenario}中\sysnameo 的应用场景，对于单个嫌疑模型，审计准确率可以很好地表征\sysnameo 的性能，即正确审计轨迹数与总审计轨迹数的比率。
在实验中，阳性模型（在目标数据集上训练）和阴性模型（在非目标数据集上训练）是随机混合的，所以单纯的准确率指标可能受到样本分布的影响。
因此，我们还提供了真正率（TPR）和真负率（TNR）。

\subsubsection{方法设置}
在实验中，我们使用四种距离度量方法，即 $\ell_1$ 范数、$\ell_2$ 范数、余弦距离和Wasserstein距离，
提供了3$\sigma$原则和Grubbs检验的审计性能。

\subsubsection{实验部署平台}
我们分别使用 stable-baselines3~\cite{stable-baselines3} 和 d3rlpy~\cite{d3rlpy} 来实现在线和离线 RL 模型。
所有审计方法均使用 Python 3.8 实现，并在配备有8个 NVIDIA GeForce RTX 3090 和512GB内存的服务器上运行。

\subsection{整体审计性能}
\label{sec:overall performance}
\begin{table*}[!ht]
    \centering
    \caption{
    % The TPR and TNR results based on Grubbs test. 
    % The mean and standard deviation of TPR and TNR in each row represent the audit results for one combination of task and model by four distance metrics. 
    % Bold indicates the highest sum of TPR and TNR, \ie, accuracy, in a row. 
    % Each pair of TPR and TNR is derived from the diagonal
    % and non-diagonal values of the corresponding heatmap in \autoref{fig:audit result on lunarlander}, \autoref{fig:audit result on bipedalwalker} and \autoref{fig:audit result on ant}.
    基于Grubbs检验的TPR和TNR结果。
    每行的TPR和TNR的均值和标准差代表了在四个距离度量下每个任务和模型组合的审计结果。
    加粗表示行中TPR和TNR之和最高，即精度最高。
    每个TPR和TNR的配对都来自于相应热图中对角线和非对角线的值，
    分别对应\autoref{fig:audit result on lunarlander}, \autoref{fig:audit result on bipedalwalker} and \autoref{fig:audit result on ant}中的结果。
    }
    \label{tab:overall audit accuracy based on Grubbs}
        \setlength{\tabcolsep}{0.3em}
        \renewcommand{\arraystretch}{1.1}
        \scriptsize
        \centering
    \begin{tabular}{cccccccccc} 
    \toprule
    \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Task}\\\textbf{ Name}\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Offline}\\\textbf{ Model}\end{tabular}} & \multicolumn{2}{c}{\textbf{L1 Norm}}                                                 & \multicolumn{2}{c}{\textbf{L2 Norm}}                               & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Cosine}\\\textbf{ Distance}\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Wasserstein}\\\textbf{ Distance}\end{tabular}}  \\ 
    \cline{3-10}
                                                                                           &                                                                                            & \multicolumn{1}{c}{TPR}                  & \multicolumn{1}{c}{TNR}                   & \multicolumn{1}{c}{TPR}         & \multicolumn{1}{c}{TNR}          & \multicolumn{1}{c}{TPR}                  & \multicolumn{1}{c}{TNR}                              & \multicolumn{1}{c}{TPR}                  & \multicolumn{1}{c}{TNR}                                    \\ 
    \hline
    \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Lunar\\ Lander\end{tabular}}                & BC                                                                                         & \textbf{99.01$\pm$0.46} & \textbf{100.00$\pm$0.00} & 96.96$\pm$0.73 & 100.00$\pm$0.00 & 96.93$\pm$0.77          & 100.00$\pm$0.00                     & 98.40$\pm$0.74          & 99.94$\pm$0.16                            \\
                                                                                           & BCQ                                                                                        & \textbf{98.29$\pm$1.14} & \textbf{100.00$\pm$0.00} & 96.03$\pm$1.15 & 100.00$\pm$0.00 & 95.97$\pm$1.07          & 99.99$\pm$0.04                      & 97.57$\pm$1.17          & 99.91$\pm$0.14                            \\
                                                                                           & IQL                                                                                        & \textbf{98.61$\pm$1.51} & \textbf{99.91$\pm$0.32}  & 97.52$\pm$2.51 & 99.97$\pm$0.12  & 97.49$\pm$2.56          & 99.92$\pm$0.19                      & 98.32$\pm$1.79          & 97.10$\pm$5.66                            \\
                                                                                           & TD3PlusBC                                                                                  & \textbf{98.29$\pm$2.04} & \textbf{99.48$\pm$0.79}  & 96.35$\pm$3.01 & 99.89$\pm$0.22  & 96.27$\pm$3.16          & 99.91$\pm$0.23                      & 98.53$\pm$1.25          & 95.59$\pm$3.77                            \\ 
    \cline{2-10}
    \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Bipedal\\ Walker\end{tabular}}              & BC                                                                                         & 99.20$\pm$1.47          & 100.00$\pm$0.00          & 98.40$\pm$2.70 & 100.00$\pm$0.00 & 98.56$\pm$2.68          & 100.00$\pm$0.00                     & \textbf{99.31$\pm$1.32} & \textbf{100.00$\pm$0.00}                  \\
                                                                                           & BCQ                                                                                        & 99.52$\pm$0.77          & 100.00$\pm$0.00          & 98.16$\pm$2.89 & 100.00$\pm$0.00 & 99.87$\pm$0.15          & 100.00$\pm$0.00                     & \textbf{99.89$\pm$0.13} & \textbf{100.00$\pm$0.00}                  \\
                                                                                           & IQL                                                                                        & 95.10$\pm$7.41          & 100.00$\pm$0.00          & 95.04$\pm$5.45 & 100.00$\pm$0.00 & \textbf{99.84$\pm$0.32} & \textbf{100.00$\pm$0.00}            & 95.01$\pm$6.72          & 100.00$\pm$0.00                           \\
                                                                                           & TD3PlusBC                                                                                  & \textbf{99.36$\pm$1.28} & \textbf{94.77$\pm$19.42} & 97.15$\pm$5.71 & 93.36$\pm$21.46 & 96.96$\pm$5.82          & 91.98$\pm$21.75                     & 98.08$\pm$3.84          & 88.26$\pm$25.34                           \\ 
    \cline{2-10}
    \multirow{4}{*}{Ant}                                                                   & BC                                                                                         & 97.42$\pm$1.66          & 99.94$\pm$0.11           & 96.48$\pm$1.66 & 99.90$\pm$0.36  & 99.20$\pm$1.08          & 85.66$\pm$28.23                     & \textbf{98.00$\pm$1.19} & \textbf{99.92$\pm$0.14}                   \\
                                                                                           & BCQ                                                                                        & 97.17$\pm$2.96          & 99.80$\pm$0.43           & 95.68$\pm$2.54 & 99.84$\pm$0.43  & 99.66$\pm$0.43          & 86.70$\pm$26.89                     & \textbf{98.67$\pm$1.65} & \textbf{99.79$\pm$0.46}                   \\
                                                                                           & IQL                                                                                        & 97.20$\pm$2.33          & 99.66$\pm$0.73           & 96.61$\pm$2.50 & 99.69$\pm$0.59  & 99.57$\pm$0.79          & 86.25$\pm$27.90                     & \textbf{99.36$\pm$0.42} & \textbf{99.63$\pm$0.78}                   \\
                                                                                           & TD3PlusBC                                                                                  & 98.53$\pm$1.80          & 99.18$\pm$1.72           & 97.17$\pm$1.79 & 99.35$\pm$1.74  & 99.72$\pm$0.40          & 87.79$\pm$26.43                     & \textbf{99.25$\pm$1.24} & \textbf{99.14$\pm$1.81}                   \\
    \bottomrule
    \end{tabular}
    \end{table*}
    
\begin{table*}[!ht]
    \caption{
    % The TPR and TNR results based on 3$\sigma$ principle. 
    % The mean and standard deviation of TPR and TNR in each row represent the audit results for one combination of task and model by four distance metrics. 
    % Bold indicates the highest sum of TPR and TNR, \ie, accuracy, in a row. 
    基于3$\sigma$原则的TPR和TNR结果。
    每行的TPR和TNR的均值和标准差代表了在四个距离度量下每个任务和模型组合的审计结果。
    加粗表示行中TPR和TNR之和最高，即精度最高。
    }
    \label{tab:overall audit accuracy based on 3sigma}
    \centering
    \setlength{\tabcolsep}{0.3em}
    \renewcommand{\arraystretch}{1.1}
    \scriptsize
    \begin{tabular}{cccccccccc} 
    \toprule
    \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Task}\\\textbf{ Name}\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Offline}\\\textbf{ Model}\end{tabular}} & \multicolumn{2}{c}{\textbf{L1 Norm}}               & \multicolumn{2}{c}{\textbf{L2 Norm}}              & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Cosine}\\\textbf{ Distance}\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Wasserstein}\\\textbf{ Distance}\end{tabular}}  \\ 
    \cline{3-10}
                                                                                           &                                                                                            & TPR                     & TNR                      & TPR                     & TNR                     & TPR                     & TNR                                                                   & TPR                     & TNR                                                                         \\ 
    \hline
    \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Lunar\\ Lander\end{tabular}}                & BC                                                                                         & \textbf{96.53$\pm$1.36} & \textbf{100.00$\pm$0.00} & 95.47$\pm$2.81          & 100.00$\pm$0.00         & 95.73$\pm$2.58          & 100.00$\pm$0.00                                                       & 96.13$\pm$2.02          & 100.00$\pm$0.00                                                             \\
                                                                                           & BCQ                                                                                        & 96.13$\pm$3.01          & 100.00$\pm$0.00          & 94.80$\pm$3.18          & 100.00$\pm$0.00         & 94.67$\pm$2.92          & 100.00$\pm$0.00                                                       & \textbf{96.40$\pm$2.92} & \textbf{99.95$\pm$0.36}                                                     \\
                                                                                           & IQL                                                                                        & \textbf{97.20$\pm$3.24} & \textbf{99.97$\pm$0.28}  & 96.27$\pm$2.44          & 100.00$\pm$0.00         & 96.53$\pm$2.40          & 100.00$\pm$0.00                                                       & 96.53$\pm$4.14          & 98.90$\pm$3.33                                                              \\
                                                                                           & TD3PlusBC                                                                                  & \textbf{95.60$\pm$4.39} & \textbf{99.54$\pm$2.53}  & 92.80$\pm$5.07          & 99.91$\pm$0.47          & 93.33$\pm$5.40          & 99.93$\pm$0.40                                                        & 96.67$\pm$2.88          & 96.86$\pm$7.24                                                              \\ 
    \cline{2-10}
    \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Bipedal\\ Walker\end{tabular}}              & BC                                                                                         & 96.56$\pm$4.27          & 100.00$\pm$0.00          & 95.78$\pm$4.58          & 100.00$\pm$0.00         & \textbf{98.33$\pm$2.50} & \textbf{100.00$\pm$0.00}                                              & 97.22$\pm$4.05          & 100.00$\pm$0.00                                                             \\
                                                                                           & BCQ                                                                                        & 96.67$\pm$4.20          & 100.00$\pm$0.00          & 94.78$\pm$7.43          & 100.00$\pm$0.00         & \textbf{98.67$\pm$1.63} & \textbf{100.00$\pm$0.00}                                              & 97.11$\pm$3.69          & 100.00$\pm$0.00                                                             \\
                                                                                           & IQL                                                                                        & 94.33$\pm$7.45          & 100.00$\pm$0.00          & 93.78$\pm$7.25          & 100.00$\pm$0.00         & \textbf{98.89$\pm$2.17} & \textbf{100.00$\pm$0.00}                                              & 94.00$\pm$9.45          & 100.00$\pm$0.00                                                             \\
                                                                                           & TD3PlusBC                                                                                  & \textbf{97.00$\pm$4.46} & \textbf{99.90$\pm$1.16}  & 94.11$\pm$8.63          & 97.80$\pm$12.09         & 95.33$\pm$6.66          & 97.78$\pm$12.19                                                       & 96.44$\pm$5.95          & 93.87$\pm$19.73                                                             \\ 
    \cline{2-10}
    \multirow{4}{*}{Ant}                                                                   & BC                                                                                         & 90.67$\pm$5.30          & 100.00$\pm$0.00          & 93.33$\pm$4.62          & 100.00$\pm$0.00         & 99.20$\pm$0.88          & 88.00$\pm$27.55                                                       & \textbf{95.20$\pm$2.99} & \textbf{99.99$\pm$0.07}                                                     \\
                                                                                           & BCQ                                                                                        & 90.40$\pm$8.68          & 99.96$\pm$0.42           & \textbf{94.13$\pm$3.83} & \textbf{99.94$\pm$0.56} & 98.00$\pm$2.00          & 88.47$\pm$26.83                                                       & 93.47$\pm$6.81          & 99.95$\pm$0.49                                                              \\
                                                                                           & IQL                                                                                        & 90.67$\pm$6.93          & 100.00$\pm$0.00          & 89.60$\pm$3.99          & 100.00$\pm$0.00         & 97.20$\pm$3.89          & 88.30$\pm$27.38                                                       & \textbf{91.20$\pm$9.03} & \textbf{100.00$\pm$0.00}                                                    \\
                                                                                           & TD3PlusBC                                                                                  & 95.62$\pm$5.19          & 99.74$\pm$1.79           & 94.12$\pm$5.03          & 99.35$\pm$2.58          & 99.08$\pm$1.54          & 88.52$\pm$26.25                                                       & \textbf{97.74$\pm$2.66} & \textbf{99.60$\pm$2.13}                                                     \\
    \bottomrule
    \end{tabular}
\end{table*}

\begin{figure*}[!ht]
    \includegraphics[width=\hsize]{figure/orl_auditor/overall_sac_lunarlander_20230130181510409.pdf}
    \caption{
    % The audit accuracy between every two Lunar Lander datasets. 
    % The caption of each plot demonstrates the offline DRL model's type, the task and the distance metric. 
    % The x labels are the names of datasets to be audit, \ie, the target datasets. 
    % The y labels are the names of datasets the suspect models learned, \ie, the actual datasets. 
    % Thus, the diagonal values show the audit accuracy when the actual dataset is same with the target dataset, \ie, TPR, and the non-diagonal values are the TNR results. 
    % The positions without value mean 100\% accuracy. 
    每两个月球着陆任务的数据集之间的交叉审计准确性。
    每个图的标题说明了离线DRL模型的类型、任务和使用的距离度量。
    x轴标签是要审计的数据集的名称，即目标数据集。
    y轴标签是可疑模型学习的数据集的名称，即实际数据集。
    因此，对角线值显示了实际数据集与目标数据集相同时的审计准确性，即TPR，而非对角线值则是TNR结果。
    没有值的位置表示100\%的准确性。
    }
    % \mc{What do the numbers in the x-axis and y-axis mean? Does the blank mean that there is no result for the specific setting? If there is no sufficient space and the conclusion is consistent, move the results of the other two datasets to the appendix.}}
    \label{fig:audit result on lunarlander}
\end{figure*}
    
\begin{figure*}[!ht]
    \includegraphics[width=\hsize]{figure/orl_auditor/overall_sac_bipedalwalker_20230130183936090.pdf}
    \caption{
    % The audit accuracy between every two Bipedal Walker datasets. 
    % The caption of each plot demonstrates the offline DRL model's type, the task and the distance metric. 
    % The x labels are the names of datasets to be audit, \ie, the target datasets. 
    % The y labels are the names of datasets the suspect models learned, \ie, the actual datasets. 
    % Thus, the diagonal values show the audit accuracy when the actual dataset is same with the target dataset, \ie, TPR, and the non-diagonal values are the TNR results. 
    % The positions without value mean 100\% accuracy. 
    每两个双足机器人行走任务的数据集之间的交叉审计准确性。
    每个图的标题说明了离线DRL模型的类型、任务和使用的距离度量。
    x轴标签是要审计的数据集的名称，即目标数据集。
    y轴标签是可疑模型学习的数据集的名称，即实际数据集。
    因此，对角线值显示了实际数据集与目标数据集相同时的审计准确性，即TPR，而非对角线值则是TNR结果。
    没有值的位置表示100\%的准确性。
    }
    \label{fig:audit result on bipedalwalker}
\end{figure*}
    
\begin{figure*}[!ht]
    \includegraphics[width=\hsize]{figure/orl_auditor/overall_sac_ant-v2_20230130182513257.pdf}
    \caption{
    % The audit accuracy between every two Ant datasets. 
    % The caption of each plot demonstrates the offline DRL model's type, the task and the distance metric. 
    % The x labels are the names of datasets to be audit, \ie, the target datasets. 
    % The y labels are the names of datasets the suspect models learned, \ie, the actual datasets. 
    % Thus, the diagonal values show the audit accuracy when the actual dataset is same with the target dataset, \ie, TPR, and the non-diagonal values are the TNR results. 
    % The positions without value mean 100\% accuracy. 
    每两个蚂蚁机器人行走任务的数据集之间的交叉审计准确性。
    每个图的标题说明了离线DRL模型的类型、任务和使用的距离度量。
    x轴标签是要审计的数据集的名称，即目标数据集。
    y轴标签是可疑模型学习的数据集的名称，即实际数据集。
    因此，对角线值显示了实际数据集与目标数据集相同时的审计准确性，即TPR，而非对角线值则是TNR结果。
    没有值的位置表示100\%的准确性。
    }
    \label{fig:audit result on ant}
\end{figure*}



我们在四种模型和三种任务的共十二种组合下评估了\sysnameo 的性能。

\mypara{实验设置}
根据\autoref{tab:dataset generation and offline model preparation}，
我们在每个数据集上训练了30个离线RL模型，所以在一个实验设置下我们共拥有150个离线DRL模型。
我们分别对5个数据集进行审计，其中审计员随机选择15个目标数据集的模型作为影子模型，其余的15个模型和其他数据集的120个模型是构成阳性可疑模型和阴性可疑模型。
对于目标数据集，我们随机选择了50个轨迹进行审计。
由于正样本和负样本的数量不平衡，
我们在\autoref{tab:overall audit accuracy based on Grubbs}中展示了每个实验设置的TPR和TNR的结果平均值及其标准差，
并在\autoref{fig:audit result on lunarlander}（Lunar Lander），\autoref{fig:audit result on bipedalwalker}（Bipedal Walker）和\autoref{fig:audit result on ant}（Ant）中提供了每两个数据集之间的交叉审计结果。
在\autoref{tab:overall audit accuracy based on Grubbs}中，每对TPR和TNR都来自相应热图的对角线和非对角线值。
作为比较，我们在\autoref{tab:overall audit accuracy based on 3sigma}中展示了3$\sigma$原则的审计结果。

\mypara{实验结果}
我们从\autoref{tab:overall audit accuracy based on Grubbs}和\autoref{tab:overall audit accuracy based on 3sigma}中得到以下观察结果。
1）大多数TPR和TNR结果都超过$95\%$，这意味着\sysnameo 是离线DRL模型学习的数据集审计的有效解决方案。
例如，在所有实验设置中，\sysnameo 使用$\ell_1$范数的结果都超过了$94\%$。
2）\sysnameo 在四种距离度量上获得了不同的审计精度。
$\ell_1$范数和Wasserstein距离的审计效果优于$\ell_2$范数和余弦距离。
在\autoref{tab:overall audit accuracy based on Grubbs}和\autoref{tab:overall audit accuracy based on 3sigma}中，使用Wasserstein距离的\sysnameo 审计精度始终处于前两位。
而使用$\ell_2$范数时，其结果通常落后于其他三种距离度量。
回顾\autoref{sec:the details of audit process}，Wasserstein距离同时表征了累计奖励的数值和位置偏差，所以对于偏差更加敏感。
由于累计奖励之间的数值差异很小，例如，在我们的实验中一般为0.01到0.1，所以$\ell_2$范数可能会忽略这些微小但重要的差异。
3）使用Grubbs检验的审计准确性优于3$\sigma$原则。
3$\sigma$原则是一种经验方法，容易受到影子模型的累计奖励异常值的影响。
回顾\autoref{sec:the details of audit process}，Grubbs检验首先计算统计量$G$，
并将$G$与浮动的检测阈值进行比较，其中样本数量也考虑在假设检验中。
因此，Grubbs检验对潜在的异常值更加稳健。

\subsection{累计奖励可视化}
\label{sec:visualization of cumulative rewards}

为进一步解释第 \ref{sec:overall performance} 节中的审计结果，
我们使用 t-SNE 方法分别可视化阳性模型和阴性模型的累计奖励，即 $\mathbb{Q}_j^i$ 和 $\mathbb{Q}_j^s$。

\mypara{实验设置}
在 \autoref{fig:visualization of cumulative rewards by t-SNE} 中，每个图的标题说明所使用的数据集和离线 DRL 模型。
图中每个点为单个 $\mathbb{Q}_j^i$（阳性样本）或 $\mathbb{Q}_j^s$（阴性样本）的可视化结果。
在每个图中，我们展示了每个任务其中一个数据集的三条轨迹的可视化结果。
例如，在标题为“Lunar Lander，BC”的图中，目标数据集是 \autoref{tab:the offline datasets} 中的数据集“1171”。
每条轨迹的三十个阳性样本是从训练于数据集“1171”上的模型中收集的，
而三十个阴性样本是从训练于其他四个数据集的模型中随机收集的。

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\hsize]{figure/orl_auditor/tsne-num_episode_3-20230202182507986.pdf}
    \caption{
        % Visualization of cumulative rewards by t-SNE. 
        % The caption of each plot demonstrates the offline DRL model's type and task. 
        % In a single plot, we randomly select three trajectories from the first dataset for the task, \ie, Lunar Lander dataset 1171, Bipedal Walker dataset 0841, and Ant dataset 2232 in \autoref{tab:the offline datasets}, and then show the cumulative rewards from 30 positive models and 30 negative models for each trajectory. 
        累计奖励的 t-SNE 可视化。
        每个图的标题展示了离线 DRL 模型的类型和任务。
        在每张图中，我们从任务的第一个数据集中随机选择了三条轨迹，即在 \autoref{tab:the offline datasets} 中的 Lunar Lander 数据集 1171、Bipedal Walker 数据集 0841 和 Ant 数据集 2232，然后展示了每条轨迹中 30 个阳性模型和 30 个阴性模型的累计奖励。
        }
    \label{fig:visualization of cumulative rewards by t-SNE}
\end{figure*}



\mypara{实验结果}
根据\autoref{fig:visualization of cumulative rewards by t-SNE}的可视化结果，
我们得出以下观察结论。
1）对于目标数据集中的轨迹，
来自阴影模型和可疑模型的累计奖励明显地被分成两组，
这意味着评论家模型输出的累计奖励很好地反映了模型行为之间的差异。
因此，由评论家模型生成的累计奖励可以作为轨迹级审计的依据。
2）代表不同轨迹的点分布特性有所不同。
比如，来自Lunar Lander数据集的轨迹1比其他两个轨迹更难聚类。
我们认为轨迹1代表了一种基本策略。
例如一个常见的局部最优策略是让飞船的推进器一直保持开启，相似的轨迹可能存在所有数据集中。
由于RL问题中最优策略的非唯一性以及模型训练过程中随机性的影响，
大多数情况下收集到的轨迹具有独特的特征。
因此，剩余代表着其他轨迹的点被清晰地分开。

\subsection{阴影模型数量的影响}
\label{sec:impact of shadow models' amount}
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\hsize]{figure/orl_auditor/overall_result_20230126181443130-number_of_shadow_model.pdf}
    \caption{
    % Impact of shadow models’ amount. 
    % The change value of TPR and TNR when the number of shadow models varies to 9 and 21 compared to the default 15 shadow models. 
    % The caption of each plot demonstrates the offline DRL model's type and task. 
    % The x labels display the four distance metrics. 
    % The y labels show the absolute fluctuating values of TPR and TNR. 
    影子模型数量的影响。
    相比于15个影子模型，当影子模型的数量变化为9和21时TPR和TNR的变化量。
    每个图的标题展示了所用的离线DRL模型和任务。
    x轴标签显示了四种距离度量。
    y轴标签显示了TPR和TNR的变化的绝对值。
    } 

    \label{fig:impact of shadow models' amount}
\end{figure*}

\begin{table*}[!ht]
    \centering
    \caption{
        % The overall TPR and TNR results based on Grubbs' test (9 shadow models).
        基于Grubbs检验的TPR和TNR结果（9个影子模型）。
        }
    \label{tab:overall audit accuracy based on Grubbs (9 Shadow Models)}
        % \setlength{\tabcolsep}{0.6em}
        % \renewcommand{\arraystretch}{1.1}
        % \footnotesize
        \centering
        \setlength{\tabcolsep}{0.3em}
        \renewcommand{\arraystretch}{1.1}
        \scriptsize
    
    \begin{tabular}{cccccccccc} 
    \toprule
    \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Task}\\\textbf{ Name}\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Offline}\\\textbf{ Model}\end{tabular}} & \multicolumn{2}{c}{\textbf{L1 Norm}}                                & \multicolumn{2}{c}{\textbf{L2 Norm}}                                & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Cosine}\\\textbf{ Distance}\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Wasserstein}\\\textbf{ Distance}\end{tabular}}  \\ 
    \cline{3-10}
                                                                                           &                                                                                            & TPR                              & TNR                              & TPR                              & TNR                              & TPR                             & TNR                                                           & TPR                             & TNR                                                                 \\ 
    \hline
    \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Lunar\\ Lander\end{tabular}}                & BC                                                                                         & 97.09$\pm$1.09  & 100.00$\pm$0.00 & 94.97$\pm$1.31  & 100.00$\pm$0.00 & 95.09$\pm$1.41 & 100.00$\pm$0.00                              & 96.55$\pm$1.98 & 99.93$\pm$0.26                                     \\
                                                                                           & BCQ                                                                                        & 96.97$\pm$1.65  & 100.00$\pm$0.00 & 94.53$\pm$1.20  & 100.00$\pm$0.00 & 94.48$\pm$1.09 & 99.98$\pm$0.10                               & 97.03$\pm$1.51 & 99.78$\pm$0.38                                     \\
                                                                                           & IQL                                                                                        & 96.89$\pm$1.96  & 99.90$\pm$0.37  & 95.22$\pm$2.85  & 99.98$\pm$0.07  & 95.03$\pm$3.13 & 99.91$\pm$0.22                               & 96.82$\pm$2.14 & 96.85$\pm$6.45                                     \\
                                                                                           & TD3PlusBC                                                                                  & 97.24$\pm$2.17  & 99.32$\pm$0.84  & 93.77$\pm$3.64  & 99.82$\pm$0.45  & 93.81$\pm$3.71 & 99.78$\pm$0.49                               & 97.54$\pm$1.16 & 95.45$\pm$3.71                                     \\ 
    \cline{2-10}
    \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Bipedal\\ Walker\end{tabular}}              & BC                                                                                         & 95.14$\pm$3.54  & 100.00$\pm$0.00 & 89.68$\pm$10.06 & 100.00$\pm$0.00 & 97.70$\pm$3.59 & 100.00$\pm$0.00                              & 94.80$\pm$3.69 & 100.00$\pm$0.00                                    \\
                                                                                           & BCQ                                                                                        & 93.90$\pm$5.98  & 100.00$\pm$0.00 & 95.47$\pm$3.37  & 100.00$\pm$0.00 & 98.69$\pm$0.93 & 100.00$\pm$0.00                              & 95.35$\pm$4.04 & 100.00$\pm$0.00                                    \\
                                                                                           & IQL                                                                                        & 88.55$\pm$10.61 & 100.00$\pm$0.00 & 87.79$\pm$8.56  & 100.00$\pm$0.00 & 98.80$\pm$1.27 & 100.00$\pm$0.00                              & 90.68$\pm$8.87 & 100.00$\pm$0.00                                    \\
                                                                                           & TD3PlusBC                                                                                  & 97.39$\pm$5.22  & 94.30$\pm$20.23 & 96.57$\pm$6.86  & 90.88$\pm$22.63 & 97.30$\pm$4.95 & 88.39$\pm$24.21                              & 96.08$\pm$7.85 & 84.40$\pm$30.65                                    \\ 
    \cline{2-10}
    \multirow{4}{*}{Ant}                                                                   & BC                                                                                         & 90.61$\pm$6.99  & 99.93$\pm$0.16  & 92.25$\pm$4.98  & 99.95$\pm$0.17  & 98.91$\pm$0.99 & 85.17$\pm$28.30                              & 96.23$\pm$3.90 & 99.92$\pm$0.16                                     \\
                                                                                           & BCQ                                                                                        & 92.65$\pm$3.46  & 99.78$\pm$0.50  & 90.00$\pm$5.47  & 99.88$\pm$0.27  & 98.02$\pm$1.01 & 85.88$\pm$28.10                              & 98.11$\pm$1.42 & 99.76$\pm$0.52                                     \\
                                                                                           & IQL                                                                                        & 97.05$\pm$1.06  & 99.58$\pm$0.92  & 94.44$\pm$2.40  & 99.63$\pm$0.72  & 99.12$\pm$0.43 & 85.16$\pm$28.62                              & 98.50$\pm$1.30 & 99.57$\pm$0.93                                     \\
                                                                                           & TD3PlusBC                                                                                  & 93.57$\pm$7.04  & 99.35$\pm$1.27  & 93.15$\pm$4.39  & 99.59$\pm$1.05  & 99.35$\pm$0.75 & 87.99$\pm$26.18                              & 97.86$\pm$1.32 & 99.30$\pm$1.36                                     \\
    \bottomrule
    \end{tabular}
\end{table*}
    
    
\begin{table*}[!ht]
    \centering
    \caption{基于Grubbs检验的TPR和TNR结果（21个影子模型）。
        % The overall TPR and TNR results based on Grubbs' test (21 shadow models).
        }
    \label{tab:overall audit accuracy based on Grubbs (21 Shadow Models)}
        % \setlength{\tabcolsep}{0.6em}
        % \renewcommand{\arraystretch}{1.1}
        % \footnotesize
        \centering
        \setlength{\tabcolsep}{0.3em}
        \renewcommand{\arraystretch}{1.1}
        \scriptsize
    \begin{tabular}{cccccccccc} 
    \toprule
    \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Task}\\\textbf{ Name}\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Offline}\\\textbf{ Model}\end{tabular}} & \multicolumn{2}{c}{\textbf{L1 Norm}}                                & \multicolumn{2}{c}{\textbf{L2 Norm}}                               & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Cosine}\\\textbf{ Distance}\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Wasserstein}\\\textbf{ Distance}\end{tabular}}  \\ 
    \cline{3-10}
                                                                                           &                                                                                            & TPR                              & TNR                              & TPR                             & TNR                              & TPR                             & TNR                                                           & TPR                             & TNR                                                                 \\ 
    \hline
    \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Lunar\\ Lander\end{tabular}}                & BC                                                                                         & 99.25$\pm$0.97  & 100.00$\pm$0.00 & 98.13$\pm$1.88 & 100.00$\pm$0.00 & 98.00$\pm$1.84 & 100.00$\pm$0.00                              & 99.11$\pm$0.93 & 99.96$\pm$0.10                                     \\
                                                                                           & BCQ                                                                                        & 99.56$\pm$0.32  & 100.00$\pm$0.00 & 98.40$\pm$0.68 & 100.00$\pm$0.00 & 98.27$\pm$0.47 & 99.99$\pm$0.04                               & 98.80$\pm$0.78 & 99.75$\pm$0.41                                     \\
                                                                                           & IQL                                                                                        & 97.95$\pm$2.45  & 99.96$\pm$0.17  & 97.11$\pm$3.12 & 99.98$\pm$0.09  & 96.85$\pm$3.40 & 99.92$\pm$0.19                               & 97.11$\pm$3.65 & 97.47$\pm$5.34                                     \\
                                                                                           & TD3PlusBC                                                                                  & 97.87$\pm$3.45  & 99.45$\pm$0.84  & 96.09$\pm$3.91 & 99.73$\pm$0.58  & 95.78$\pm$4.29 & 99.95$\pm$0.14                               & 98.00$\pm$2.60 & 96.27$\pm$3.43                                     \\ 
    \cline{2-10}
    \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Bipedal\\ Walker\end{tabular}}              & BC                                                                                         & 97.07$\pm$3.60  & 100.00$\pm$0.00 & 97.24$\pm$4.49 & 100.00$\pm$0.00 & 97.69$\pm$4.51 & 100.00$\pm$0.00                              & 98.36$\pm$2.67 & 100.00$\pm$0.00                                    \\
                                                                                           & BCQ                                                                                        & 100.00$\pm$0.00 & 100.00$\pm$0.00 & 99.56$\pm$0.69 & 100.00$\pm$0.00 & 99.07$\pm$1.65 & 100.00$\pm$0.00                              & 99.96$\pm$0.09 & 100.00$\pm$0.00                                    \\
                                                                                           & IQL                                                                                        & 95.91$\pm$4.93  & 100.00$\pm$0.00 & 96.36$\pm$4.57 & 100.00$\pm$0.00 & 99.51$\pm$0.26 & 100.00$\pm$0.00                              & 96.44$\pm$4.54 & 100.00$\pm$0.00                                    \\
                                                                                           & TD3PlusBC                                                                                  & 99.91$\pm$0.18  & 95.05$\pm$19.14 & 99.87$\pm$0.27 & 93.96$\pm$20.97 & 99.82$\pm$0.36 & 92.79$\pm$21.27                              & 99.91$\pm$0.18 & 91.78$\pm$21.28                                    \\ 
    \cline{2-10}
    \multirow{4}{*}{Ant}                                                                   & BC                                                                                         & 98.13$\pm$1.55  & 99.91$\pm$0.18  & 97.73$\pm$1.19 & 99.86$\pm$0.41  & 99.73$\pm$0.53 & 86.82$\pm$26.97                              & 97.55$\pm$1.57 & 99.90$\pm$0.21                                     \\
                                                                                           & BCQ                                                                                        & 97.16$\pm$2.73  & 99.81$\pm$0.42  & 96.67$\pm$2.18 & 99.84$\pm$0.42  & 99.69$\pm$0.41 & 87.53$\pm$26.74                              & 98.58$\pm$1.69 & 99.80$\pm$0.43                                     \\
                                                                                           & IQL                                                                                        & 95.91$\pm$3.87  & 99.64$\pm$0.77  & 96.49$\pm$3.89 & 99.68$\pm$0.63  & 99.51$\pm$0.67 & 86.53$\pm$27.80                              & 97.65$\pm$2.19 & 99.64$\pm$0.78                                     \\
                                                                                           & TD3PlusBC                                                                                  & 99.44$\pm$0.60  & 99.23$\pm$1.53  & 98.27$\pm$1.03 & 99.36$\pm$1.64  & 99.76$\pm$0.33 & 88.42$\pm$25.93                              & 99.79$\pm$0.27 & 99.18$\pm$1.65                                     \\
    \bottomrule
    \end{tabular}
\end{table*}
    
% 我们研究了阴影模型数量与审计准确性之间的关系。

\mypara{实验设置}
我们将阴影模型的数量由15分别更改为9和21，其余设置与\autoref{sec:overall performance}相同。 
\autoref{fig:impact of shadow models' amount} 显示了TPR和TNR相对于\autoref{sec:overall performance}结果的变化。
每张图的标题说明了模型和任务的设置，x轴表示四种距离度量，y轴是变化的绝对值。
此外，我们提供了更为详细实验结果，包括\autoref{tab:overall audit accuracy based on Grubbs (9 Shadow Models)} (9个阴影模型)和\autoref{tab:overall audit accuracy based on Grubbs (21 Shadow Models)} (21个阴影模型)。

\mypara{实验结果}
根据\autoref{fig:impact of shadow models' amount}，我们得到以下观察结果。
1）随着阴影模型数量的增加，审计准确性也会增加。
由于阴影模型的值是对数据集中真实$Q\left(s, a\right)$的多次采样，
因此随着阴影模型数量的增加，平均值和标准差会更加精确。
例如，当有9个阴影模型时，\sysnameo 显然会出现TPR下降（超过30\%）。
这是因为缺乏目标数据集上训练的模型多样性的知识，审核人员容易将阳性模型错误地判断为阴性模型。
2）随着阴影模型数量的扩大，审计准确性的增益存在饱和点。
当阴影模型数量从15增加到21时，由于审核人员掌握了更多来自目标数据集训练的模型特性，TPR通常会增加。
然而，在大多数情况下，TPR的上升不是很明显。
这意味着阴影模型集中出现了类似的累计奖励特征，即与15个阴影模型情况相比，模型多样性并没有显著增加。
所以，过多的阴影模型对于提升审计精度提升有限且需要更多的训练开销。



\subsection{显著性水平的影响} 
\label{sec:impact of significance level}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\hsize]{figure/orl_auditor/overall_result_20230130210458739-sigma.pdf}
    \caption{
    % Impact of the significance level. 
    % The change value of TPR and TNR when the significance level varies to 0.001 and 0.0001 compared to the default 0.01. 
    % The caption of each plot demonstrates the offline DRL model's type and task. 
    % The x labels display the four distance metrics. 
    % The y labels show the absolute fluctuating values of TPR and TNR. 
    显著性水平的影响。
    当显著性水平相对于默认值0.01变化为0.001和0.0001时，TPR和TNR的变化量。
    每个图的标题展示了所用的离线DRL模型和任务。
    x轴标签显示了四个距离度量。
    y轴标签显示了TPR和TNR的变化的绝对值。
    }
    \label{fig:impact of significance level}
\end{figure*}

\begin{table*}[!ht]
    \centering
    \caption{基于Grubbs检验的TPR和TNR结果（$\sigma=0.001$）。}
    \label{tab:overall audit accuracy based on Grubbs (sigma=0.001)}
        % \setlength{\tabcolsep}{0.6em}
        % \renewcommand{\arraystretch}{1.1}
        % \footnotesize
    \centering
    \setlength{\tabcolsep}{0.3em}
    \renewcommand{\arraystretch}{1.1}
    \scriptsize
    \begin{tabular}{cccccccccc}
    \toprule
    \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Task}\\\textbf{ Name}\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Offline}\\\textbf{ Model}\end{tabular}} & \multicolumn{2}{c}{\textbf{L1 Norm}}                               & \multicolumn{2}{c}{\textbf{L2 Norm}}                               & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Cosine}\\\textbf{ Distance}\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Wasserstein}\\\textbf{ Distance}\end{tabular}}  \\ 
    \cline{3-10}
                                                                                           &                                                                                            & TPR                             & TNR                              & TPR                             & TNR                              & TPR                             & TNR                                                           & TPR                              & TNR                                                                \\ 
    \hline
    \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Lunar\\ Lander\end{tabular}}                & BC                                                                                         & 99.63$\pm$0.19 & 100.00$\pm$0.00 & 98.21$\pm$0.55 & 100.00$\pm$0.00 & 98.21$\pm$0.63 & 100.00$\pm$0.00                              & 99.31$\pm$0.38  & 99.84$\pm$0.38                                    \\
                                                                                           & BCQ                                                                                        & 99.15$\pm$0.67 & 100.00$\pm$0.00 & 97.60$\pm$1.13 & 100.00$\pm$0.00 & 97.63$\pm$1.04 & 99.97$\pm$0.13                               & 98.59$\pm$0.95  & 99.61$\pm$0.53                                    \\
                                                                                           & IQL                                                                                        & 99.31$\pm$0.90 & 99.83$\pm$0.40  & 98.56$\pm$1.51 & 99.96$\pm$0.16  & 98.51$\pm$1.59 & 99.79$\pm$0.51                               & 99.04$\pm$1.21  & 94.88$\pm$8.21                                    \\
                                                                                           & TD3PlusBC                                                                                  & 99.20$\pm$1.10 & 99.22$\pm$0.89  & 97.47$\pm$2.33 & 99.61$\pm$0.55  & 97.55$\pm$2.22 & 99.75$\pm$0.51                               & 99.49$\pm$0.56  & 92.45$\pm$5.32                                    \\ 
    \cline{2-10}
    \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Bipedal\\ Walker\end{tabular}}              & BC                                                                                         & 99.97$\pm$0.05 & 100.00$\pm$0.00 & 98.67$\pm$2.67 & 100.00$\pm$0.00 & 98.64$\pm$2.66 & 100.00$\pm$0.00                              & 100.00$\pm$0.00 & 100.00$\pm$0.00                                   \\
                                                                                           & BCQ                                                                                        & 99.95$\pm$0.06 & 100.00$\pm$0.00 & 99.73$\pm$0.34 & 100.00$\pm$0.00 & 99.95$\pm$0.06 & 100.00$\pm$0.00                              & 99.97$\pm$0.05  & 100.00$\pm$0.00                                   \\
                                                                                           & IQL                                                                                        & 97.04$\pm$5.47 & 100.00$\pm$0.00 & 95.81$\pm$5.06 & 100.00$\pm$0.00 & 99.87$\pm$0.27 & 100.00$\pm$0.00                              & 97.68$\pm$4.51  & 100.00$\pm$0.00                                   \\
                                                                                           & TD3PlusBC                                                                                  & 99.92$\pm$0.16 & 89.69$\pm$22.99 & 97.17$\pm$5.65 & 85.59$\pm$27.05 & 97.20$\pm$5.60 & 82.52$\pm$30.48                              & 99.68$\pm$0.64  & 80.18$\pm$35.21                                   \\ 
    \cline{2-10}
    \multirow{4}{*}{Ant}                                                                   & BC                                                                                         & 99.52$\pm$0.50 & 99.86$\pm$0.25  & 98.48$\pm$0.80 & 99.88$\pm$0.40  & 99.55$\pm$0.66 & 80.58$\pm$33.24                              & 99.36$\pm$0.49  & 99.85$\pm$0.25                                    \\
                                                                                           & BCQ                                                                                        & 98.91$\pm$1.68 & 99.71$\pm$0.60  & 97.97$\pm$1.43 & 99.81$\pm$0.47  & 99.87$\pm$0.15 & 81.88$\pm$31.60                              & 99.52$\pm$0.64  & 99.69$\pm$0.63                                    \\
                                                                                           & IQL                                                                                        & 98.88$\pm$1.27 & 99.53$\pm$0.97  & 98.42$\pm$1.80 & 99.60$\pm$0.73  & 99.71$\pm$0.52 & 80.90$\pm$32.60                              & 99.92$\pm$0.06  & 99.49$\pm$1.05                                    \\
                                                                                           & TD3PlusBC                                                                                  & 99.62$\pm$0.46 & 98.92$\pm$2.08  & 98.73$\pm$0.97 & 99.24$\pm$1.91  & 99.79$\pm$0.36 & 84.36$\pm$28.07                              & 99.71$\pm$0.58  & 98.78$\pm$2.15                                    \\
    \bottomrule
    \end{tabular}
\end{table*}
    
    
\begin{table*}[!ht]
    \centering
    \caption{基于Grubbs检验的TPR和TNR结果（$\sigma=0.0001$）。}
    \label{tab:overall audit accuracy based on Grubbs (sigma=0.0001)}
        % \setlength{\tabcolsep}{0.6em}
        % \renewcommand{\arraystretch}{1.1}
        % \footnotesize
        \centering
        \setlength{\tabcolsep}{0.3em}
        \renewcommand{\arraystretch}{1.1}
        \scriptsize
    \begin{tabular}{cccccccccc} 
    \toprule
    \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Task}\\\textbf{ Name}\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Offline}\\\textbf{ Model}\end{tabular}} & \multicolumn{2}{c}{\textbf{L1 Norm}}                                & \multicolumn{2}{c}{\textbf{L2 Norm}}                               & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Cosine}\\\textbf{ Distance}\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Wasserstein}\\\textbf{ Distance}\end{tabular}}  \\ 
    \cline{3-10}
                                                                                           &                                                                                            & TPR                              & TNR                              & TPR                             & TNR                              & TPR                              & TNR                                                          & TPR                              & TNR                                                                \\ 
    \hline
    \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Lunar\\ Lander\end{tabular}}                & BC                                                                                         & 99.87$\pm$0.12  & 100.00$\pm$0.00 & 98.93$\pm$0.34 & 100.00$\pm$0.00 & 99.04$\pm$0.35  & 100.00$\pm$0.00                             & 99.79$\pm$0.22  & 99.56$\pm$0.79                                    \\
                                                                                           & BCQ                                                                                        & 99.49$\pm$0.46  & 100.00$\pm$0.00 & 98.48$\pm$0.79 & 100.00$\pm$0.00 & 98.48$\pm$0.82  & 99.95$\pm$0.20                              & 99.33$\pm$0.54  & 98.88$\pm$1.54                                    \\
                                                                                           & IQL                                                                                        & 99.71$\pm$0.52  & 99.66$\pm$0.55  & 98.99$\pm$1.04 & 99.91$\pm$0.21  & 98.91$\pm$1.17  & 99.49$\pm$1.15                              & 99.52$\pm$0.70  & 91.46$\pm$10.53                                   \\
                                                                                           & TD3PlusBC                                                                                  & 99.55$\pm$0.56  & 98.80$\pm$1.27  & 98.48$\pm$1.48 & 98.96$\pm$1.24  & 98.56$\pm$1.49  & 99.38$\pm$0.87                              & 99.84$\pm$0.16  & 88.16$\pm$6.84                                    \\ 
    \cline{2-10}
    \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Bipedal\\ Walker\end{tabular}}              & BC                                                                                         & 100.00$\pm$0.00 & 100.00$\pm$0.00 & 98.67$\pm$2.67 & 100.00$\pm$0.00 & 98.80$\pm$2.40  & 100.00$\pm$0.00                             & 100.00$\pm$0.00 & 100.00$\pm$0.00                                   \\
                                                                                           & BCQ                                                                                        & 100.00$\pm$0.00 & 100.00$\pm$0.00 & 99.81$\pm$0.26 & 100.00$\pm$0.00 & 100.00$\pm$0.00 & 100.00$\pm$0.00                             & 100.00$\pm$0.00 & 100.00$\pm$0.00                                   \\
                                                                                           & IQL                                                                                        & 98.53$\pm$2.87  & 100.00$\pm$0.00 & 96.27$\pm$4.74 & 100.00$\pm$0.00 & 99.87$\pm$0.27  & 100.00$\pm$0.00                             & 98.67$\pm$2.67  & 100.00$\pm$0.00                                   \\
                                                                                           & TD3PlusBC                                                                                  & 100.00$\pm$0.00 & 79.08$\pm$38.26 & 97.23$\pm$5.55 & 75.43$\pm$39.10 & 97.33$\pm$5.33  & 74.60$\pm$39.23                             & 99.97$\pm$0.05  & 73.98$\pm$38.98                                   \\ 
    \cline{2-10}
    \multirow{4}{*}{Ant}                                                                   & BC                                                                                         & 99.95$\pm$0.11  & 99.80$\pm$0.41  & 99.15$\pm$0.67 & 99.85$\pm$0.43  & 99.73$\pm$0.41  & 77.36$\pm$35.68                             & 99.73$\pm$0.22  & 99.79$\pm$0.42                                    \\
                                                                                           & BCQ                                                                                        & 99.33$\pm$1.21  & 99.56$\pm$0.76  & 98.75$\pm$1.11 & 99.80$\pm$0.48  & 99.92$\pm$0.11  & 78.87$\pm$33.94                             & 99.81$\pm$0.23  & 99.52$\pm$0.79                                    \\
                                                                                           & IQL                                                                                        & 99.73$\pm$0.29  & 99.35$\pm$1.25  & 98.91$\pm$1.45 & 99.56$\pm$0.80  & 99.89$\pm$0.21  & 77.78$\pm$34.93                             & 100.00$\pm$0.00 & 99.32$\pm$1.31                                    \\
                                                                                           & TD3PlusBC                                                                                  & 99.96$\pm$0.05  & 98.65$\pm$2.42  & 99.35$\pm$0.50 & 99.11$\pm$2.07  & 99.92$\pm$0.11  & 81.33$\pm$29.74                             & 99.90$\pm$0.19  & 98.06$\pm$3.18                                    \\
    \bottomrule
    \end{tabular}
\end{table*}

显著性水平代表审计员对判定结果的置信度。
在\autoref{sec:overall performance}中，我们采用显著性水平$\alpha=0.01$，这意味着审核人员对所做的判断有99\%的置信度。
显著性水平代表\sysnameo 的审计能力上限，而不是超参数设置，因为它一般是数据集所有者的审计要求。

\mypara{实验设置}
我们要求审计员输出更加确信的判定结果，
其中单次判定的错误概率应该控制在 1\textperthousand 和 0.1\textperthousand 之间，即$\alpha=0.001$和$\alpha=0.0001$。
\autoref{fig:impact of significance level}显示了TPR和TNR相对于显著性水平$\alpha=0.01$时的变化量。
每个图的标题说明所使用的离线DRL模型和任务。
x轴表示四个距离度量指标，y轴表示变化的绝对值。
每两个数据集之间的审计结果在\autoref{tab:overall audit accuracy based on Grubbs (sigma=0.001)}（$\alpha=0.001$）和\autoref{tab:overall audit accuracy based on Grubbs (sigma=0.0001)}（$\alpha=0.0001$）中。

\mypara{实验结果}
从\autoref{fig:impact of significance level}中，我们得到以下观察结果。
1）对于一个复杂的任务，我们建议审核人员选择一个较大的显著性水平。
任务的复杂度影响\sysnameo 的显著性水平的下界。
例如，当显著性水平降低到0.001时，Lunar Lander任务上的TPR和TNP几乎没有变化，而在Ant任务上则大幅缩小。
从\autoref{tab:overview-of-tasks}可以看出，Ant的状态和动作空间比Lunar Lander更大。
当审核人员利用评论家模型将每个模型的状态和动作对压缩为累计奖励时，
在Ant任务上$Q_j^i$和$Q_j^s$（回顾\autoref{fig:framework}）之间的偏差更加难以察觉。
2）对于性能不佳的可疑模型，\sysnameo 应采用较大的显著性水平以保证审计准确性。
例如，在标题为“Bipedal Walker，TD3PlusBC”的图中，当$\alpha$降至0.001和0.0001时，四个距离度量的TNR结果均下降。
从\autoref{tab:TD3PlusBC models}可知，TD3PlusBC模型完成双足机器人行走任务的表现在-100分左右，
这意味着TD3PlusBC模型未完全掌握数据集的知识。
因此，它们行为反映出的数据集特征是模糊的，这削弱了阳性样本和阴性样本之间的差异。
同时，置信区间（即\autoref{fig:intuition}中的$\Delta$）随着显著性水平的降低而扩大。
由于以上两个原因，TD3PlusBC模型在双足机器人行走任务上的TNR结果比$\alpha=0.01$时下降了10\%以上。

综上，$\alpha=0.01$是\sysnameo 的较为安全的显著性边界，
而较低的$\alpha$可能会突破\sysnameo 的审计性能上限，导致审计员将阴性模型错误地判定为阳性。

\subsection{鲁棒性测试}
\label{sec:robustness}
在实践中，可疑模型可能会通过扰动输出来掩盖其数据集盗用行为。
因此，我们评估\sysnameo 针对输出扰动的鲁棒性。

\mypara{实验设置}
因为动作的每一个维度均在-1到1之间变化，
所以我们利用均值为0，标准差分别为0.01和0.1的高斯分布分别生成弱扰动和强扰动。 
\autoref{fig:robustness}展示了使用或不使用输出扰动的审计结果。
每个图的标题中显示了所用的离线DRL模型和任务。
x轴表示四个距离度量指标，y轴表示变化的绝对值。
每两个数据集之间的详细结果分别在\autoref{fig:robustness 0.01 on lunarlander}，\autoref{fig:robustness 0.1 on lunarlander}（月球着陆任务），\autoref{fig:robustness 0.01 on bipedalwalker}，\autoref{fig:robustness 0.1 on bipedalwalker}（双足机器人行走任务），\autoref{fig:robustness 0.01 on ant}和\autoref{fig:robustness 0.1 on ant}（蚂蚁机器人行走任务）中展示。

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\hsize]{figure/orl_auditor/overall_result_20230201162017541-output_gaussian_noise.pdf}
    \caption{
    % Robustness against output perturbation. 
    % The change value of TPR and TNR when the suspect model adds Gaussian noise parameterized with $(\mu=0, \sigma=0.01)$ and $(\mu=0, \sigma=0.1)$ to its output. 
    % The caption of each plot demo nstrates the offline DRL model's type and task. 
    % The x labels display the four distance metrics. 
    % The y labels show the absolute fluctuating values of TPR and TNR. 
    所提方法对抗输出扰动的鲁棒性。
    当可疑模型向其输出分别添加 $(\mu=0, \sigma=0.01)$ 和 $(\mu=0, \sigma=0.1)$ 的高斯噪声时，TPR 和 TNR 的变化量。
    每个图标题展示了离线 DRL 模型和任务。
    x轴标签显示四个距离度量。
    y轴标签显示 TPR 和 TNR 变化的绝对值。
    }
    \label{fig:robustness}
\end{figure*}

\begin{figure*}[!t]
    \includegraphics[width=\hsize]{figure/orl_auditor/overall_sac_lunarlander_20230201160601003.pdf}
    \caption{
    % The audit accuracy with Gaussian ($\mu=0, \sigma=0.01$) perturbation on the suspect models' output for Lunar Lander.
    % The caption of each plot demonstrates the offline DRL model's type, the task, the distance metric and the noise strength. 
    % The x labels are the names of datasets to be audit, \ie, the target datasets. 
    % The y labels are the names of datasets the suspect models learned, \ie, the actual datasets. 
    % Thus, the diagonal values show the audit accuracy when the actual dataset is same with the target dataset, \ie, TPR, and the non-diagonal values are the TNR results. 
    % The positions without value mean 100\% accuracy. 
    对于月球着陆任务，可疑模型使用高斯噪声 ($\mu=0, \sigma=0.01$) 对输出进行扰动后的审计准确性。
    每个绘图的标题展示了离线 DRL 模型、任务、距离度量和噪声强度。
    x轴标签是要审计的数据集名称，即目标数据集。
    y轴标签是嫌疑模型学习的数据集名称，即实际数据集。
    因此，对角线上的值显示了实际数据集与目标数据集相同时的审计准确性，即TPR，非对角线上的值则是 TNR 的结果。
    没有数值的位置表示 100\% 的准确率。
    }
    \label{fig:robustness 0.01 on lunarlander}
    \end{figure*}
    
    \begin{figure*}[!t]
    \includegraphics[width=\hsize]{figure/orl_auditor/overall_sac_lunarlander_20230201161413988.pdf}
    \caption{
    % The audit accuracy with Gaussian ($\mu=0, \sigma=0.1$) perturbation on the suspect models' output for Lunar Lander.
    % The caption of each plot demonstrates the offline DRL model's type, the task, the distance metric and the noise strength. 
    % The x labels are the names of datasets to be audit, \ie, the target datasets. 
    % The y labels are the names of datasets the suspect models learned, \ie, the actual datasets. 
    % Thus, the diagonal values show the audit accuracy when the actual dataset is same with the target dataset, \ie, TPR, and the non-diagonal values are the TNR results. 
    % The positions without value mean 100\% accuracy. 
    对于月球着陆任务，可疑模型使用高斯噪声 ($\mu=0, \sigma=0.1$) 对输出进行扰动后的审计准确性。
    每个绘图的标题展示了离线 DRL 模型、任务、距离度量和噪声强度。
    x轴标签是要审计的数据集名称，即目标数据集。
    y轴标签是嫌疑模型学习的数据集名称，即实际数据集。
    因此，对角线上的值显示了实际数据集与目标数据集相同时的审计准确性，即TPR，非对角线上的值则是 TNR 的结果。
    没有数值的位置表示 100\% 的准确率。
    }
    \label{fig:robustness 0.1 on lunarlander}
    \end{figure*}
    
\begin{figure*}[!ht]
    \includegraphics[width=\hsize]{figure/orl_auditor/overall_sac_bipedalwalker_20230201161034943.pdf}
    \caption{
    % The audit accuracy with Gaussian ($\mu=0, \sigma=0.01$) perturbation on the suspect models' output for Bipedal Walker.
    % The caption of each plot demonstrates the offline DRL model's type, the task, the distance metric and the noise strength. 
    % The x labels are the names of datasets to be audit, \ie, the target datasets. 
    % The y labels are the names of datasets the suspect models learned, \ie, the actual datasets. 
    % Thus, the diagonal values show the audit accuracy when the actual dataset is same with the target dataset, \ie, TPR, and the non-diagonal values are the TNR results. 
    % The positions without value mean 100\% accuracy. 
    对于双足机器人行走任务，可疑模型使用高斯噪声 ($\mu=0, \sigma=0.01$) 对输出进行扰动后的审计准确性。
    每个绘图的标题展示了离线 DRL 模型、任务、距离度量和噪声强度。
    x轴标签是要审计的数据集名称，即目标数据集。
    y轴标签是嫌疑模型学习的数据集名称，即实际数据集。
    因此，对角线上的值显示了实际数据集与目标数据集相同时的审计准确性，即TPR，非对角线上的值则是 TNR 的结果。
    没有数值的位置表示 100\% 的准确率。
    }
    \label{fig:robustness 0.01 on bipedalwalker}
    \end{figure*}
    
\begin{figure*}[!t]
    \includegraphics[width=\hsize]{figure/orl_auditor/overall_sac_bipedalwalker_20230201161849913.pdf}
    \caption{
    % The audit accuracy with Gaussian ($\mu=0, \sigma=0.1$) perturbation on the suspect models' output for Bipedal Walker.
    % The caption of each plot demonstrates the offline DRL model's type, the task, the distance metric and the noise strength. 
    % The x labels are the names of datasets to be audit, \ie, the target datasets. 
    % The y labels are the names of datasets the suspect models learned, \ie, the actual datasets. 
    % Thus, the diagonal values show the audit accuracy when the actual dataset is same with the target dataset, \ie, TPR, and the non-diagonal values are the TNR results. 
    % The positions without value mean 100\% accuracy. 
    对于双足机器人行走任务，可疑模型使用高斯噪声 ($\mu=0, \sigma=0.1$) 对输出进行扰动后的审计准确性。
    每个绘图的标题展示了离线 DRL 模型、任务、距离度量和噪声强度。
    x轴标签是要审计的数据集名称，即目标数据集。
    y轴标签是嫌疑模型学习的数据集名称，即实际数据集。
    因此，对角线上的值显示了实际数据集与目标数据集相同时的审计准确性，即TPR，非对角线上的值则是 TNR 的结果。
    没有数值的位置表示 100\% 的准确率。
    }
    \label{fig:robustness 0.1 on bipedalwalker}
\end{figure*}
    
    
\begin{figure*}[!ht]
    \includegraphics[width=\hsize]{figure/orl_auditor/overall_sac_ant-v2_20230201160818087.pdf}
    \caption{
    % The audit accuracy with Gaussian ($\mu=0, \sigma=0.01$) perturbation on the suspect models' output for Ant.
    % The caption of each plot demonstrates the offline DRL model's type, the task, the distance metric and the noise strength. 
    % The x labels are the names of datasets to be audit, \ie, the target datasets. 
    % The y labels are the names of datasets the suspect models learned, \ie, the actual datasets. 
    % Thus, the diagonal values show the audit accuracy when the actual dataset is same with the target dataset, \ie, TPR, and the non-diagonal values are the TNR results. 
    % The positions without value mean 100\% accuracy. 
    对于蚂蚁机器人行走任务，可疑模型使用高斯噪声 ($\mu=0, \sigma=0.01$) 对输出进行扰动后的审计准确性。
    每个绘图的标题展示了离线 DRL 模型、任务、距离度量和噪声强度。
    x轴标签是要审计的数据集名称，即目标数据集。
    y轴标签是嫌疑模型学习的数据集名称，即实际数据集。
    因此，对角线上的值显示了实际数据集与目标数据集相同时的审计准确性，即TPR，非对角线上的值则是 TNR 的结果。
    没有数值的位置表示 100\% 的准确率。
    }
    \label{fig:robustness 0.01 on ant}
    \end{figure*}
    
\begin{figure*}[!ht]
    \includegraphics[width=\hsize]{figure/orl_auditor/overall_sac_ant-v2_20230201161631240.pdf}
    \caption{
    % The audit accuracy with Gaussian ($\mu=0, \sigma=0.1$) perturbation on the suspect models' output for Ant.
    % The caption of each plot demonstrates the offline DRL model's type, the task, the distance metric and the noise strength. 
    % The x labels are the names of datasets to be audit, \ie, the target datasets. 
    % The y labels are the names of datasets the suspect models learned, \ie, the actual datasets. 
    % Thus, the diagonal values show the audit accuracy when the actual dataset is same with the target dataset, \ie, TPR, and the non-diagonal values are the TNR results. 
    % The positions without value mean 100\% accuracy. 
    对于蚂蚁机器人行走任务，可疑模型使用高斯噪声 ($\mu=0, \sigma=0.1$) 对输出进行扰动后的审计准确性。
    每个绘图的标题展示了离线 DRL 模型、任务、距离度量和噪声强度。
    x轴标签是要审计的数据集名称，即目标数据集。
    y轴标签是嫌疑模型学习的数据集名称，即实际数据集。
    因此，对角线上的值显示了实际数据集与目标数据集相同时的审计准确性，即TPR，非对角线上的值则是 TNR 的结果。
    没有数值的位置表示 100\% 的准确率。
    }
    \label{fig:robustness 0.1 on ant}
\end{figure*}

\mypara{实验结果}
根据上述结果，我们得出以下观察结论。
1）\sysnameo 能够抵抗来自可疑模型的输出扰动，尤其\sysnameo 使用余弦距离度量。
从 \autoref{fig:robustness} 中可以看出，
在大部分的弱噪声设置下，TPR 和 TNR 变化较小，其中余弦距离的最大准确率下降不超过 3\%。
我们推测，余弦距离在计算两个累计奖励序列的内积时具有噪声抑制能力。
另一方面，弱噪声可能有助于数据集审计，因为它会扩大阴性样本与阳性样本集合之间的距离。
2）\sysname 在只使用单一距离度量时，对于强扰动的抵抗能力较弱。
当强烈扰动彻底改变模型输出的分布时，目标数据集上训练的可疑模型的累计奖励与审计员的阴影模型不同，
因此审计员不能仅依靠单一的距离度量来区分阳性模型和阴性模型。
从 \autoref{fig:robustness 0.1 on ant} 中可以看出，余弦距离适用于区分阳性模型（结果在对角线上），
而 Wasserstein 距离则适用于区分阴性模型（结果在非对角线上）。
因此，对于强输出噪声，使用多种距离度量的组合可以增强 \sysname 的审计鲁棒性。
此外，需要注意的是，强噪声也会破坏模型的正常行为。
例如，在 \autoref{tab:IQL models} 中，噪声会导致 IQL 模型的性能下降高达 25\%。
并且模型质量越好，性能下降越明显。

\subsection{实际应用案例}
\label{sec:real-world application}
在本节中，我们将\sysnameo 应用于DeepMind~\cite{DBLP:conf/nips/Gulcehre0NPCZAM20}和Google~\cite{DBLP:conf/nips/Pomerleau88}的开源数据集进行审核。
我们选择了两者都发布的“halfcheetah”任务，其中操作员控制一个由9个连接件和8个连接它们的关节（包括两个爪子）组成的二维猎豹机器人，使猎豹尽可能快地向前（右侧）奔跑。
halfcheetah数据集和离线DRL模型的详细信息在\autoref{tab:details of the halfcheetah dataset}和\autoref{tab:details of the models trained on the halfcheetah dataset}中。
所有实验设置保持和\autoref{sec:overall performance}的设置一致。
\begin{table*}[!ht]
    \small
    \caption{HalfCheetah数据集的详细信息}
    \label{tab:details of the halfcheetah dataset}
    \centering
    \begin{tabular}{ccccc}
    \toprule
    \textbf{任务名称}          & \textbf{迁移数量} & \textbf{数据集名称} & \textbf{轨迹数量} & \textbf{轨迹长度} \\ \hline
    \multirow{4}{*}{Half Cheetah} & 1e6                            & D4RL   Expert        & 1001                            & 998.00   +/- 0.06             \\
                                  & 1e6                            & D4RL   Medium        & 1001                            & 997.90   +/- 3.13             \\
                                  & 1e6                            & D4RL   Random        & 1001                            & 998.00 +/- 0.00               \\
                                  & 3.003e5                        & RL Unplugged         & 300                             & 1001.00 +/- 0.00              \\ \bottomrule
    \end{tabular}
\end{table*}
    
    % Please add the following required packages to your document preamble:
    % \usepackage{multirow}
\begin{table*}[!ht]
    \small
    \centering
    \caption{
        % The details of the models trained on the HalfCheetah dataset. The model performance shows the cumulative reward for 10 separate evaluations. 
        在HalfCheetah训练的模型信息，其中模型性能展示了 10 次独立测试结果的均值和方差
        }
    \label{tab:details of the models trained on the halfcheetah dataset}
    \centering
    \begin{tabular}{ccccc}
    \toprule
    \textbf{任务名称}           & \textbf{训练次数}   & \textbf{离线模型}     & \textbf{数据集名称} & \textbf{模型性能} \\ \hline
    \multirow{16}{*}{Half Cheetah} & \multirow{16}{*}{5e5} & \multirow{4}{*}{BC}        & D4RL   Expert        & 12620.94 +/- 307.84        \\
                                   &                       &                            & D4RL   Medium        & 4223.77 +/- 134.67         \\
                                   &                       &                            & D4RL   Random        & -0.33 +/- 0.24             \\
                                   &                       &                            & RL Unplugged         & -427.50 +/- 113.42         \\ \cline{3-5} 
                                   &                       & \multirow{4}{*}{BCQ}       & D4RL   Expert        & 10974.19 +/- 842.10        \\
                                   &                       &                            & D4RL   Medium        & 4765.24 +/- 98.75          \\
                                   &                       &                            & D4RL   Random        & -1.13 +/- 0.43             \\
                                   &                       &                            & RL Unplugged         & -421.91 +/- 212.36         \\ \cline{3-5} 
                                   &                       & \multirow{4}{*}{IQL}       & D4RL   Expert        & 10163.20 +/- 1106.70       \\
                                   &                       &                            & D4RL   Medium        & 4808.11 +/- 46.99          \\
                                   &                       &                            & D4RL   Random        & 1649.55 +/- 518.47         \\
                                   &                       &                            & RL Unplugged         & -378.74 +/- 151.65         \\ \cline{3-5} 
                                   &                       & \multirow{4}{*}{TD3PlusBC} & D4RL   Expert        & 12712.69 +/- 383.33        \\
                                   &                       &                            & D4RL   Medium        & 4969.74 +/- 56.31          \\
                                   &                       &                            & D4RL   Random        & 1046.23 +/- 226.61         \\
                                   &                       &                            & RL Unplugged         & -181.50 +/- 205.29         \\ \bottomrule
    \end{tabular}
\end{table*}

从\autoref{tab:audit result on halfcheetah}中，我们可以得出以下观察结果。

1）\sysnameo 在现实世界应用中是有效的。
当使用$\ell_1$范数和Wasserstein距离作为距离度量，\sysnameo 的TPR和TNR均超过95\%，这意味着\sysnameo 对现在已开源的数据集有效。
2）Wasserstein距离在实验和现实世界数据集上具有稳定的性能。
使用Wasserstein距离，\sysnameo 的整体准确性高于其他三个距离度量。

\begin{table*}[!ht]
    \centering
    \caption{
    在HalfCheetah数据集上进行审计的TPR和TNR结果。
    每行的TPR和TNR的均值和标准差代表了在四个距离度量下每个任务和模型组合的审计结果。
    加粗表示行中TPR和TNR之和最高，即精度最高。
    每个TPR和TNR的配对都来自于\autoref{fig:audit result on halfcheetah}中对角线和非对角线的值。
    }
    \label{tab:audit result on halfcheetah}
    \setlength{\tabcolsep}{0.3em}
    \renewcommand{\arraystretch}{1.1}
    \scriptsize
    \begin{tabular}{cccccccccc} 
    \hline
    \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{任务}\\\textbf{名称}\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{离线}\\\textbf{模型}\end{tabular}} & \multicolumn{2}{c}{\textbf{$\ell_1$ Norm}}                               & \multicolumn{2}{c}{\textbf{$\ell_2$ Norm}}                               & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Cosine}\\\textbf{ Distance}\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Wasserstein}\\\textbf{ Distance}\end{tabular}}  \\ 
    \cline{3-10}
                                                                                           &                                                                                            & TPR                             & TNR                              & TPR                             & TNR                              & TPR                             & TNR                                                           & TPR                                      & TNR                                                        \\ 
    \hline
    \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Half\\Cheetah\end{tabular}}                 & BC                                                                                         & 96.07$\pm$3.15 & 100.00$\pm$0.00 & 96.07$\pm$2.34 & 100.00$\pm$0.00 & 99.80$\pm$0.35 & 68.62$\pm$42.47                              & \textbf{98.47$\pm$1.13} & \textbf{100.00$\pm$0.00}                  \\
                                                                                           & BCQ                                                                                        & 95.37$\pm$0.55 & 100.00$\pm$0.00 & 95.83$\pm$1.20 & 100.00$\pm$0.00 & 99.57$\pm$0.47 & 70.14$\pm$41.14                              & \textbf{97.47$\pm$1.35} & \textbf{100.00$\pm$0.00}                  \\
                                                                                           & IQL                                                                                        & 95.47$\pm$0.77 & 100.00$\pm$0.00 & 95.68$\pm$1.02 & 100.00$\pm$0.00 & 99.78$\pm$0.23 & 71.38$\pm$41.05                              & \textbf{97.12$\pm$2.70} & \textbf{100.00$\pm$0.00}                  \\
                                                                                           & TD3PlusBC                                                                                  & 95.00$\pm$2.87 & 100.00$\pm$0.00 & 95.50$\pm$1.99 & 100.00$\pm$0.00 & 99.87$\pm$0.16 & 70.57$\pm$40.85                              & \textbf{98.27$\pm$1.09} & \textbf{100.00$\pm$0.00}                  \\
    \cline{1-10}
    \end{tabular}
    \end{table*}
    
    \begin{figure*}[t]
    \includegraphics[width=\hsize]{figure/orl_auditor/overall_rluply_d4rl_halfcheetah_20230130184110602.pdf}
    \caption{
    每两个HalfCheetah数据集之间的交叉审计准确性。
    每个绘图的标题展示了离线 DRL 模型、任务、距离度量和噪声强度。
    x轴标签是要审计的数据集名称，即目标数据集。
    y轴标签是嫌疑模型学习的数据集名称，即实际数据集。
    因此，对角线上的值显示了实际数据集与目标数据集相同时的审计准确性，即TPR，非对角线上的值则是 TNR 的结果。
    没有数值的位置表示 100\% 的准确率。
    }
    \label{fig:audit result on halfcheetah}
\end{figure*}







\begin{table*}[t]
    \centering
    \caption{
        % The details of the BC offline models. The model performance shows the cumulative reward for 10 separate evaluations. 
        BC模型的详细信息，其中模型性能展示了10次独立测试结果的均值和方差。
        }
    \label{tab:BC models}
    \setlength{\tabcolsep}{0.3em}
    \renewcommand{\arraystretch}{1.1}
    % \scriptsize
    \footnotesize
    \begin{tabular}{ccccccc} 
        \toprule
        \textbf{离线模型}       & \textbf{训练次数}            & \textbf{任务名称}                       & \textbf{数据集名称} & \begin{tabular}[c]{@{}c@{}}\textbf{模型性能}\\ ~\textbf{(无输出扰动)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{模型性能}\\ ~\textbf{(Gauss. 0.01)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{模型性能}\\ ~\textbf{(Gauss. 0.1)}\end{tabular}  \\ 
        \hline
        \multirow{15}{*}{BC} & \multirow{15}{*}{5e4} & \multirow{5}{*}{Lunar Lander}   & 1171        & 272.06$\pm$5.14                                                                & 270.84$\pm$6.38                                                            & 269.01$\pm$11.12                                                           \\
                             &                       &                                 & 2094        & 39.04$\pm$34.07                                                                & 55.03$\pm$34.09                                                            & 53.99$\pm$30.89                                                            \\
                             &                       &                                 & 4496        & 173.62$\pm$58.93                                                               & 161.95$\pm$54.27                                                           & 177.23$\pm$35.35                                                           \\
                             &                       &                                 & 6518        & 211.85$\pm$43.52                                                               & 223.84$\pm$41.63                                                           & 219.14$\pm$44.10                                                           \\
                             &                       &                                 & 9906        & 225.45$\pm$32.91                                                               & 215.19$\pm$35.72                                                           & 215.76$\pm$33.81                                                           \\ 
        \cline{3-7}
                             &                       & \multirow{5}{*}{Bipedal Walker} & 0841        & 264.92$\pm$29.11                                                               & 257.99$\pm$26.73                                                           & 268.83$\pm$25.55                                                           \\
                             &                       &                                 & 1203        & 288.85$\pm$15.39                                                               & 287.64$\pm$16.36                                                           & 285.91$\pm$15.71                                                           \\
                             &                       &                                 & 2110        & 276.80$\pm$26.03                                                               & 283.05$\pm$20.17                                                           & 286.19$\pm$14.48                                                           \\
                             &                       &                                 & 3813        & 164.56$\pm$46.14                                                               & 160.48$\pm$56.24                                                           & 182.20$\pm$55.79                                                           \\
                             &                       &                                 & 6558        & 281.02$\pm$48.65                                                               & 284.69$\pm$22.77                                                           & 268.39$\pm$39.12                                                           \\ 
        \cline{3-7}
                             &                       & \multirow{5}{*}{Ant}            & 2232        & 5479.72$\pm$354.79                                                             & 5324.99$\pm$441.27                                                         & 4332.23$\pm$589.30                                                         \\
                             &                       &                                 & 3569        & 1493.77$\pm$413.96                                                             & 1460.97$\pm$436.37                                                         & 1412.06$\pm$391.99                                                         \\
                             &                       &                                 & 4603        & 5424.74$\pm$422.83                                                             & 5470.37$\pm$473.25                                                         & 4679.76$\pm$496.30                                                         \\
                             &                       &                                 & 5766        & 2806.80$\pm$286.57                                                             & 2899.11$\pm$313.43                                                         & 2458.92$\pm$272.67                                                         \\
                             &                       &                                 & 7490        & 5514.17$\pm$441.78                                                             & 5451.01$\pm$430.96                                                         & 4417.19$\pm$687.25                                                         \\
        \bottomrule
    \end{tabular}
    % \begin{tabular}{ccccccc} 
    % \hline
    % Offline Model        & Train Step            & Task Name                                  & Dataset Name & \begin{tabular}[c]{@{}c@{}}Model Performance\\ ~(no perturbation)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Model Performance\\ ~(Gauss. 0.01)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Model Performance\\ ~(Gauss. 0.1)\end{tabular}  \\ 
    % \hline
    % \multirow{32}{*}{BC} & \multirow{32}{*}{5e4} & \multirow{10}{*}{LunarLander (Continuous)} & 1171        & 270.69$\pm$8.51                                                                & 268.41$\pm$13.91                                                           & 270.25$\pm$13.45                                                           \\
    %                      &                       &                                            & 1395        & 260.33$\pm$15.90                                                               & 260.13$\pm$18.00                                                           & 261.77$\pm$14.79                                                           \\
    %                      &                       &                                            & 2094        & 52.67$\pm$26.38                                                                & 57.80$\pm$30.64                                                            & 55.73$\pm$28.36                                                            \\
    %                      &                       &                                            & 2097        & 257.76$\pm$24.54                                                               & 250.97$\pm$22.24                                                           & 261.81$\pm$20.87                                                           \\
    %                      &                       &                                            & 4496        & 166.13$\pm$57.37                                                               & 188.89$\pm$38.33                                                           & 191.19$\pm$46.30                                                           \\
    %                      &                       &                                            & 4632        & 257.10$\pm$15.33                                                               & 256.94$\pm$14.58                                                           & 258.05$\pm$17.07                                                           \\
    %                      &                       &                                            & 6518        & 234.99$\pm$30.93                                                               & 236.13$\pm$33.25                                                           & 235.19$\pm$25.16                                                           \\
    %                      &                       &                                            & 6620        & 66.03$\pm$41.46                                                                & 61.30$\pm$37.79                                                            & 72.06$\pm$37.24                                                            \\
    %                      &                       &                                            & 7963        & 75.74$\pm$43.40                                                                & 82.06$\pm$36.98                                                            & 87.33$\pm$52.03                                                            \\
    %                      &                       &                                            & 9906        & 243.74$\pm$24.43                                                               & 237.51$\pm$31.45                                                           & 233.42$\pm$34.97                                                           \\ 
    % \cline{3-7}
    %                      &                       & \multirow{12}{*}{Bipedal Walker}           & 0841        & 228.05$\pm$43.06                                                               & 247.17$\pm$37.65                                                           & 249.94$\pm$28.21                                                           \\
    %                      &                       &                                            & 0977        & 255.92$\pm$16.77                                                               & 265.16$\pm$12.88                                                           & 259.89$\pm$14.76                                                           \\
    %                      &                       &                                            & 1203        & 269.87$\pm$28.93                                                               & 276.78$\pm$21.02                                                           & 281.59$\pm$17.69                                                           \\
    %                      &                       &                                            & 1351        & 255.41$\pm$37.92                                                               & 249.42$\pm$43.78                                                           & 255.94$\pm$34.60                                                           \\
    %                      &                       &                                            & 2110        & 281.34$\pm$18.56                                                               & 270.97$\pm$24.22                                                           & 270.78$\pm$26.49                                                           \\
    %                      &                       &                                            & 2637        & 179.25$\pm$53.95                                                               & 192.80$\pm$45.72                                                           & 230.02$\pm$58.41                                                           \\
    %                      &                       &                                            & 3813        & 166.87$\pm$55.81                                                               & 177.90$\pm$52.03                                                           & 185.86$\pm$45.97                                                           \\
    %                      &                       &                                            & 5303        & 245.13$\pm$23.99                                                               & 243.16$\pm$33.70                                                           & 217.26$\pm$47.37                                                           \\
    %                      &                       &                                            & 6558        & 271.52$\pm$57.34                                                               & 275.55$\pm$30.70                                                           & 262.56$\pm$43.95                                                           \\
    %                      &                       &                                            & 6862        & 288.59$\pm$21.52                                                               & 268.93$\pm$36.34                                                           & 286.69$\pm$26.42                                                           \\
    %                      &                       &                                            & 6974        & 108.11$\pm$63.91                                                               & 109.33$\pm$64.93                                                           & 168.19$\pm$49.39                                                           \\
    %                      &                       &                                            & 7539        & 233.21$\pm$61.69                                                               & 230.29$\pm$53.09                                                           & 239.07$\pm$40.07                                                           \\ 
    % \cline{3-7}
    %                      &                       & \multirow{10}{*}{Ant}                      & 2232        & 3844.45$\pm$875.84                                                             & 3587.01$\pm$816.81                                                         & 2514.55$\pm$772.19                                                         \\
    %                      &                       &                                            & 3468        & 1811.85$\pm$502.98                                                             & 1686.32$\pm$539.30                                                         & 1518.59$\pm$438.37                                                         \\
    %                      &                       &                                            & 3569        & 1032.55$\pm$327.13                                                             & 1013.29$\pm$283.76                                                         & 942.25$\pm$266.46                                                          \\
    %                      &                       &                                            & 3883        & 3360.82$\pm$595.13                                                             & 3230.56$\pm$522.61                                                         & 2530.34$\pm$517.81                                                         \\
    %                      &                       &                                            & 4603        & 4554.06$\pm$676.32                                                             & 4480.04$\pm$639.38                                                         & 3412.76$\pm$804.53                                                         \\
    %                      &                       &                                            & 5006        & 3225.00$\pm$540.93                                                             & 3375.72$\pm$625.77                                                         & 2824.84$\pm$481.92                                                         \\
    %                      &                       &                                            & 5766        & 2583.27$\pm$268.12                                                             & 2640.93$\pm$323.35                                                         & 2031.98$\pm$293.49                                                         \\
    %                      &                       &                                            & 6896        & 2883.44$\pm$727.46                                                             & 2578.26$\pm$793.64                                                         & 2480.54$\pm$546.79                                                         \\
    %                      &                       &                                            & 7490        & 3653.48$\pm$1108.85                                                            & 3552.11$\pm$1115.43                                                        & 2432.82$\pm$892.02                                                         \\
    %                      &                       &                                            & 9865        & 3371.78$\pm$1006.27                                                            & 3449.00$\pm$1048.00                                                        & 2437.03$\pm$813.01                                                         \\
    % \hline
    % \end{tabular}
\end{table*}

\begin{table*}[t]
    \centering
    \caption{
        % The details of the BCQ offline models. The model performance shows the cumulative reward for 10 separate evaluations. 
        BCQ模型的详细信息，其中模型性能展示了10次独立测试结果的均值和方差。
        }
    \label{tab:BCQ models}
    \setlength{\tabcolsep}{0.3em}
    \renewcommand{\arraystretch}{1.1}
    % \scriptsize
    \footnotesize
    \begin{tabular}{ccccccc} 
    \toprule
    \textbf{离线模型}       & \textbf{训练次数}            & \textbf{任务名称}                       & \textbf{数据集名称} & \begin{tabular}[c]{@{}c@{}}\textbf{模型性能}\\ ~\textbf{(无输出扰动)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{模型性能}\\ ~\textbf{(Gauss. 0.01)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{模型性能}\\ ~\textbf{(Gauss. 0.1)}\end{tabular}  \\ 
    \hline
    \multirow{15}{*}{BCQ} & \multirow{15}{*}{5e4} & \multirow{5}{*}{Lunar Lander}   & 1171        & 270.69$\pm$8.51                                                                & 268.41$\pm$13.91                                                           & 270.25$\pm$13.45                                                           \\
                          &                       &                                 & 2094        & 52.67$\pm$26.38                                                                & 57.80$\pm$30.64                                                            & 55.73$\pm$28.36                                                            \\
                          &                       &                                 & 4496        & 166.13$\pm$57.37                                                               & 188.89$\pm$38.33                                                           & 191.19$\pm$46.30                                                           \\
                          &                       &                                 & 6518        & 234.99$\pm$30.93                                                               & 236.13$\pm$33.25                                                           & 235.19$\pm$25.16                                                           \\
                          &                       &                                 & 9906        & 243.74$\pm$24.43                                                               & 237.51$\pm$31.45                                                           & 233.42$\pm$34.97                                                           \\ 
    \cline{3-7}
                          &                       & \multirow{5}{*}{Bipedal Walker} & 0841        & 228.05$\pm$43.06                                                               & 247.17$\pm$37.65                                                           & 249.94$\pm$28.21                                                           \\
                          &                       &                                 & 1203        & 269.87$\pm$28.93                                                               & 276.78$\pm$21.02                                                           & 281.59$\pm$17.69                                                           \\
                          &                       &                                 & 2110        & 281.34$\pm$18.56                                                               & 270.97$\pm$24.22                                                           & 270.78$\pm$26.49                                                           \\
                          &                       &                                 & 3813        & 166.87$\pm$55.81                                                               & 177.90$\pm$52.03                                                           & 185.86$\pm$45.97                                                           \\
                          &                       &                                 & 6558        & 271.52$\pm$57.34                                                               & 275.55$\pm$30.70                                                           & 262.56$\pm$43.95                                                           \\ 
    \cline{3-7}
                          &                       & \multirow{5}{*}{Ant}            & 2232        & 3844.45$\pm$875.84                                                             & 3587.01$\pm$816.81                                                         & 2514.55$\pm$772.19                                                         \\
                          &                       &                                 & 3569        & 1032.55$\pm$327.13                                                             & 1013.29$\pm$283.76                                                         & 942.25$\pm$266.46                                                          \\
                          &                       &                                 & 4603        & 4554.06$\pm$676.32                                                             & 4480.04$\pm$639.38                                                         & 3412.76$\pm$804.53                                                         \\
                          &                       &                                 & 5766        & 2583.27$\pm$268.12                                                             & 2640.93$\pm$323.35                                                         & 2031.98$\pm$293.49                                                         \\
                          &                       &                                 & 7490        & 3653.48$\pm$1108.85                                                            & 3552.11$\pm$1115.43                                                        & 2432.82$\pm$892.02                                                         \\
    \bottomrule
    \end{tabular}
    % \begin{tabular}{ccccccc} 
    % \hline
    % Offline Model         & Train Step            & Task Name                                  & Dataset Name & \begin{tabular}[c]{@{}c@{}}Model Performance\\ ~(no perturbation)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Model Performance\\ ~(Gauss. 0.01)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Model Performance\\ ~(Gauss. 0.1)\end{tabular}  \\ 
    % \hline
    % \multirow{32}{*}{BCQ} & \multirow{32}{*}{5e4} & \multirow{10}{*}{LunarLander (Continuous)} & 1171        & 270.69$\pm$8.51                                                                & 268.41$\pm$13.91                                                           & 270.25$\pm$13.45                                                           \\
    %                       &                       &                                            & 1395        & 260.33$\pm$15.90                                                               & 260.13$\pm$18.00                                                           & 261.77$\pm$14.79                                                           \\
    %                       &                       &                                            & 2094        & 52.67$\pm$26.38                                                                & 57.80$\pm$30.64                                                            & 55.73$\pm$28.36                                                            \\
    %                       &                       &                                            & 2097        & 257.76$\pm$24.54                                                               & 250.97$\pm$22.24                                                           & 261.81$\pm$20.87                                                           \\
    %                       &                       &                                            & 4496        & 166.13$\pm$57.37                                                               & 188.89$\pm$38.33                                                           & 191.19$\pm$46.30                                                           \\
    %                       &                       &                                            & 4632        & 257.10$\pm$15.33                                                               & 256.94$\pm$14.58                                                           & 258.05$\pm$17.07                                                           \\
    %                       &                       &                                            & 6518        & 234.99$\pm$30.93                                                               & 236.13$\pm$33.25                                                           & 235.19$\pm$25.16                                                           \\
    %                       &                       &                                            & 6620        & 66.03$\pm$41.46                                                                & 61.30$\pm$37.79                                                            & 72.06$\pm$37.24                                                            \\
    %                       &                       &                                            & 7963        & 75.74$\pm$43.40                                                                & 82.06$\pm$36.98                                                            & 87.33$\pm$52.03                                                            \\
    %                       &                       &                                            & 9906        & 243.74$\pm$24.43                                                               & 237.51$\pm$31.45                                                           & 233.42$\pm$34.97                                                           \\ 
    % \cline{3-7}
    %                       &                       & \multirow{12}{*}{Bipedal Walker}           & 0841        & 228.05$\pm$43.06                                                               & 247.17$\pm$37.65                                                           & 249.94$\pm$28.21                                                           \\
    %                       &                       &                                            & 0977        & 255.92$\pm$16.77                                                               & 265.16$\pm$12.88                                                           & 259.89$\pm$14.76                                                           \\
    %                       &                       &                                            & 1203        & 269.87$\pm$28.93                                                               & 276.78$\pm$21.02                                                           & 281.59$\pm$17.69                                                           \\
    %                       &                       &                                            & 1351        & 255.41$\pm$37.92                                                               & 249.42$\pm$43.78                                                           & 255.94$\pm$34.60                                                           \\
    %                       &                       &                                            & 2110        & 281.34$\pm$18.56                                                               & 270.97$\pm$24.22                                                           & 270.78$\pm$26.49                                                           \\
    %                       &                       &                                            & 2637        & 179.25$\pm$53.95                                                               & 192.80$\pm$45.72                                                           & 230.02$\pm$58.41                                                           \\
    %                       &                       &                                            & 3813        & 166.87$\pm$55.81                                                               & 177.90$\pm$52.03                                                           & 185.86$\pm$45.97                                                           \\
    %                       &                       &                                            & 5303        & 245.13$\pm$23.99                                                               & 243.16$\pm$33.70                                                           & 217.26$\pm$47.37                                                           \\
    %                       &                       &                                            & 6558        & 271.52$\pm$57.34                                                               & 275.55$\pm$30.70                                                           & 262.56$\pm$43.95                                                           \\
    %                       &                       &                                            & 6862        & 288.59$\pm$21.52                                                               & 268.93$\pm$36.34                                                           & 286.69$\pm$26.42                                                           \\
    %                       &                       &                                            & 6974        & 108.11$\pm$63.91                                                               & 109.33$\pm$64.93                                                           & 168.19$\pm$49.39                                                           \\
    %                       &                       &                                            & 7539        & 233.21$\pm$61.69                                                               & 230.29$\pm$53.09                                                           & 239.07$\pm$40.07                                                           \\ 
    % \cline{3-7}
    %                       &                       & \multirow{10}{*}{Ant}                      & 2232        & 3844.45$\pm$875.84                                                             & 3587.01$\pm$816.81                                                         & 2514.55$\pm$772.19                                                         \\
    %                       &                       &                                            & 3468        & 1811.85$\pm$502.98                                                             & 1686.32$\pm$539.30                                                         & 1518.59$\pm$438.37                                                         \\
    %                       &                       &                                            & 3569        & 1032.55$\pm$327.13                                                             & 1013.29$\pm$283.76                                                         & 942.25$\pm$266.46                                                          \\
    %                       &                       &                                            & 3883        & 3360.82$\pm$595.13                                                             & 3230.56$\pm$522.61                                                         & 2530.34$\pm$517.81                                                         \\
    %                       &                       &                                            & 4603        & 4554.06$\pm$676.32                                                             & 4480.04$\pm$639.38                                                         & 3412.76$\pm$804.53                                                         \\
    %                       &                       &                                            & 5006        & 3225.00$\pm$540.93                                                             & 3375.72$\pm$625.77                                                         & 2824.84$\pm$481.92                                                         \\
    %                       &                       &                                            & 5766        & 2583.27$\pm$268.12                                                             & 2640.93$\pm$323.35                                                         & 2031.98$\pm$293.49                                                         \\
    %                       &                       &                                            & 6896        & 2883.44$\pm$727.46                                                             & 2578.26$\pm$793.64                                                         & 2480.54$\pm$546.79                                                         \\
    %                       &                       &                                            & 7490        & 3653.48$\pm$1108.85                                                            & 3552.11$\pm$1115.43                                                        & 2432.82$\pm$892.02                                                         \\
    %                       &                       &                                            & 9865        & 3371.78$\pm$1006.27                                                            & 3449.00$\pm$1048.00                                                        & 2437.03$\pm$813.01                                                         \\
    % \hline
    % \end{tabular}
\end{table*}

\begin{table*}[t]
    \centering
    \caption{
        % The details of the IQL offline models. The model performance shows the cumulative reward for 10 separate evaluations. 
        IQL模型的详细信息，其中模型性能展示了10次独立测试结果的均值和方差。
        }
    \label{tab:IQL models}
    \setlength{\tabcolsep}{0.3em}
    \renewcommand{\arraystretch}{1.1}
    % \scriptsize
    \footnotesize
    \begin{tabular}{ccccccc} 
        \toprule
        \textbf{离线模型}       & \textbf{训练次数}            & \textbf{任务名称}                       & \textbf{数据集名称} & \begin{tabular}[c]{@{}c@{}}\textbf{模型性能}\\ ~\textbf{(无输出扰动)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{模型性能}\\ ~\textbf{(Gauss. 0.01)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{模型性能}\\ ~\textbf{(Gauss. 0.1)}\end{tabular}  \\ 
        \hline
        \multirow{15}{*}{IQL} & \multirow{15}{*}{5e4} & \multirow{5}{*}{Lunar Lander}   & 1171        & 268.55$\pm$10.81                                                               & 266.55$\pm$13.58                                                           & 265.35$\pm$14.22                                                           \\
                              &                       &                                 & 2094        & 57.92$\pm$31.14                                                                & 49.96$\pm$28.60                                                            & 46.82$\pm$29.83                                                            \\
                              &                       &                                 & 4496        & 181.48$\pm$40.09                                                               & 194.79$\pm$43.22                                                           & 181.89$\pm$38.97                                                           \\
                              &                       &                                 & 6518        & 226.85$\pm$43.37                                                               & 218.41$\pm$45.31                                                           & 245.00$\pm$20.38                                                           \\
                              &                       &                                 & 9906        & 237.33$\pm$29.23                                                               & 245.07$\pm$20.08                                                           & 231.25$\pm$25.67                                                           \\ 
        \cline{3-7}
                              &                       & \multirow{5}{*}{Bipedal Walker} & 0841        & 261.23$\pm$33.91                                                               & 254.18$\pm$35.26                                                           & 264.38$\pm$34.52                                                           \\
                              &                       &                                 & 1203        & 284.63$\pm$17.38                                                               & 285.79$\pm$17.42                                                           & 285.38$\pm$14.02                                                           \\
                              &                       &                                 & 2110        & 285.24$\pm$28.87                                                               & 287.20$\pm$19.29                                                           & 281.40$\pm$23.24                                                           \\
                              &                       &                                 & 3813        & 169.20$\pm$38.88                                                               & 172.41$\pm$43.21                                                           & 163.51$\pm$55.19                                                           \\
                              &                       &                                 & 6558        & 279.97$\pm$33.62                                                               & 284.75$\pm$21.41                                                           & 268.64$\pm$40.07                                                           \\ 
        \cline{3-7}
                              &                       & \multirow{5}{*}{Ant}            & 2232        & 4577.36$\pm$865.63                                                             & 4678.37$\pm$804.33                                                         & 3420.01$\pm$912.08                                                         \\
                              &                       &                                 & 3569        & 1406.45$\pm$447.39                                                             & 1421.81$\pm$459.03                                                         & 1239.03$\pm$328.98                                                         \\
                              &                       &                                 & 4603        & 5248.48$\pm$477.42                                                             & 5232.36$\pm$536.94                                                         & 4135.40$\pm$708.39                                                         \\
                              &                       &                                 & 5766        & 2846.64$\pm$295.47                                                             & 2879.16$\pm$262.72                                                         & 2338.67$\pm$263.68                                                         \\
                              &                       &                                 & 7490        & 4814.81$\pm$556.16                                                             & 4877.90$\pm$707.65                                                         & 3461.92$\pm$694.92                                                         \\
        \bottomrule
    \end{tabular}
    % \begin{tabular}{ccccccc} 
    % \hline
    % Offline Model         & Train Step            & Task Name                                  & Dataset Name & \begin{tabular}[c]{@{}c@{}}Model Performance\\ ~(no perturbation)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Model Performance\\ ~(Gauss. 0.01)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Model Performance\\ ~(Gauss. 0.1)\end{tabular}  \\ 
    % \hline
    % \multirow{32}{*}{IQL} & \multirow{32}{*}{5e4} & \multirow{10}{*}{LunarLander (Continuous)} & 1171        & 268.55$\pm$10.81                                                               & 266.55$\pm$13.58                                                           & 265.35$\pm$14.22                                                           \\
    %                       &                       &                                            & 1395        & 261.39$\pm$13.32                                                               & 258.68$\pm$16.29                                                           & 256.32$\pm$18.39                                                           \\
    %                       &                       &                                            & 2094        & 57.92$\pm$31.14                                                                & 49.96$\pm$28.60                                                            & 46.82$\pm$29.83                                                            \\
    %                       &                       &                                            & 2097        & 250.90$\pm$25.95                                                               & 257.39$\pm$25.54                                                           & 262.07$\pm$15.05                                                           \\
    %                       &                       &                                            & 4496        & 181.48$\pm$40.09                                                               & 194.79$\pm$43.22                                                           & 181.89$\pm$38.97                                                           \\
    %                       &                       &                                            & 4632        & 253.47$\pm$15.81                                                               & 255.02$\pm$13.92                                                           & 259.44$\pm$13.04                                                           \\
    %                       &                       &                                            & 6518        & 226.85$\pm$43.37                                                               & 218.41$\pm$45.31                                                           & 245.00$\pm$20.38                                                           \\
    %                       &                       &                                            & 6620        & 67.54$\pm$36.24                                                                & 87.40$\pm$50.10                                                            & 73.84$\pm$39.58                                                            \\
    %                       &                       &                                            & 7963        & 81.28$\pm$38.67                                                                & 82.48$\pm$36.99                                                            & 80.91$\pm$49.52                                                            \\
    %                       &                       &                                            & 9906        & 237.33$\pm$29.23                                                               & 245.07$\pm$20.08                                                           & 231.25$\pm$25.67                                                           \\ 
    % \cline{3-7}
    %                       &                       & \multirow{12}{*}{Bipedal Walker}           & 0841        & 261.23$\pm$33.91                                                               & 254.18$\pm$35.26                                                           & 264.38$\pm$34.52                                                           \\
    %                       &                       &                                            & 0977        & 266.38$\pm$14.96                                                               & 263.84$\pm$18.56                                                           & 262.69$\pm$16.46                                                           \\
    %                       &                       &                                            & 1203        & 284.63$\pm$17.38                                                               & 285.79$\pm$17.42                                                           & 285.38$\pm$14.02                                                           \\
    %                       &                       &                                            & 1351        & 221.53$\pm$37.53                                                               & 213.91$\pm$44.44                                                           & 241.19$\pm$40.69                                                           \\
    %                       &                       &                                            & 2110        & 285.24$\pm$28.87                                                               & 287.20$\pm$19.29                                                           & 281.40$\pm$23.24                                                           \\
    %                       &                       &                                            & 2637        & 163.45$\pm$55.85                                                               & 158.31$\pm$53.31                                                           & 190.92$\pm$49.62                                                           \\
    %                       &                       &                                            & 3813        & 169.20$\pm$38.88                                                               & 172.41$\pm$43.21                                                           & 163.51$\pm$55.19                                                           \\
    %                       &                       &                                            & 5303        & 257.84$\pm$31.36                                                               & 267.08$\pm$21.77                                                           & 236.03$\pm$35.74                                                           \\
    %                       &                       &                                            & 6558        & 279.97$\pm$33.62                                                               & 284.75$\pm$21.41                                                           & 268.64$\pm$40.07                                                           \\
    %                       &                       &                                            & 6862        & 291.76$\pm$17.17                                                               & 286.38$\pm$19.40                                                           & 288.46$\pm$24.63                                                           \\
    %                       &                       &                                            & 6974        & 147.18$\pm$45.72                                                               & 135.76$\pm$55.74                                                           & 188.04$\pm$45.50                                                           \\
    %                       &                       &                                            & 7539        & 216.18$\pm$46.21                                                               & 196.25$\pm$59.59                                                           & 173.12$\pm$53.43                                                           \\ 
    % \cline{3-7}
    %                       &                       & \multirow{10}{*}{Ant}                      & 2232        & 4577.36$\pm$865.63                                                             & 4678.37$\pm$804.33                                                         & 3420.01$\pm$912.08                                                         \\
    %                       &                       &                                            & 3468        & 1926.17$\pm$465.73                                                             & 1898.48$\pm$382.56                                                         & 1558.55$\pm$313.90                                                         \\
    %                       &                       &                                            & 3569        & 1406.45$\pm$447.39                                                             & 1421.81$\pm$459.03                                                         & 1239.03$\pm$328.98                                                         \\
    %                       &                       &                                            & 3883        & 3397.69$\pm$722.67                                                             & 3376.63$\pm$615.78                                                         & 2770.21$\pm$614.99                                                         \\
    %                       &                       &                                            & 4603        & 5248.48$\pm$477.42                                                             & 5232.36$\pm$536.94                                                         & 4135.40$\pm$708.39                                                         \\
    %                       &                       &                                            & 5006        & 3717.58$\pm$493.53                                                             & 3865.07$\pm$456.61                                                         & 3305.61$\pm$467.69                                                         \\
    %                       &                       &                                            & 5766        & 2846.64$\pm$295.47                                                             & 2879.16$\pm$262.72                                                         & 2338.67$\pm$263.68                                                         \\
    %                       &                       &                                            & 6896        & 3422.65$\pm$730.98                                                             & 3659.65$\pm$635.95                                                         & 2826.90$\pm$596.03                                                         \\
    %                       &                       &                                            & 7490        & 4814.81$\pm$556.16                                                             & 4877.90$\pm$707.65                                                         & 3461.92$\pm$694.92                                                         \\
    %                       &                       &                                            & 9865        & 5289.18$\pm$624.37                                                             & 5399.35$\pm$434.64                                                         & 4370.70$\pm$598.37                                                         \\
    % \hline
    % \end{tabular}
\end{table*}
    


\begin{table*}[t]
    \centering
    \caption{
        % The details of the TD3PlusBC offline models. The model performance shows the cumulative reward for 10 separate evaluations. 
        TD3PlusBC模型的详细信息，其中模型性能展示了10次独立测试结果的均值和方差。
        }
    \label{tab:TD3PlusBC models}
    \setlength{\tabcolsep}{0.3em}
    \renewcommand{\arraystretch}{1.1}
    % \scriptsize
    \footnotesize
    \begin{tabular}{ccccccc} 
        \toprule
        \textbf{离线模型}       & \textbf{训练次数}            & \textbf{任务名称}                       & \textbf{数据集名称} & \begin{tabular}[c]{@{}c@{}}\textbf{模型性能}\\ ~\textbf{(无输出扰动)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{模型性能}\\ ~\textbf{(Gauss. 0.01)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{模型性能}\\ ~\textbf{(Gauss. 0.1)}\end{tabular}  \\ 
        \hline
        \multirow{15}{*}{TD3PlusBC} & \multirow{15}{*}{5e4} & \multirow{5}{*}{Lunar Lander}   & 1171        & 263.65$\pm$21.47                                                               & 267.15$\pm$12.96                                                           & 265.12$\pm$10.36                                                           \\
                                    &                       &                                 & 2094        & 99.72$\pm$47.21                                                                & 100.67$\pm$34.95                                                           & 90.74$\pm$37.66                                                            \\
                                    &                       &                                 & 4496        & 201.58$\pm$42.49                                                               & 207.07$\pm$28.13                                                           & 194.69$\pm$42.59                                                           \\
                                    &                       &                                 & 6518        & 242.58$\pm$21.54                                                               & 238.51$\pm$21.01                                                           & 243.96$\pm$16.41                                                           \\
                                    &                       &                                 & 9906        & 235.98$\pm$25.48                                                               & 230.41$\pm$34.05                                                           & 229.55$\pm$36.81                                                           \\ 
        \cline{3-7}
                                    &                       & \multirow{5}{*}{Bipedal Walker} & 0841        & -102.88$\pm$56.03                                                              & -101.93$\pm$54.70                                                          & -97.05$\pm$73.10                                                           \\
                                    &                       &                                 & 1203        & -86.65$\pm$22.94                                                               & -87.64$\pm$22.35                                                           & -86.95$\pm$25.50                                                           \\
                                    &                       &                                 & 2110        & -101.94$\pm$22.86                                                              & -101.02$\pm$23.63                                                          & -98.78$\pm$26.75                                                           \\
                                    &                       &                                 & 3813        & -114.96$\pm$14.97                                                              & -113.97$\pm$12.93                                                          & -119.21$\pm$10.67                                                          \\
                                    &                       &                                 & 6558        & 154.70$\pm$148.81                                                              & 165.64$\pm$136.38                                                          & 168.47$\pm$68.50                                                           \\ 
        \cline{3-7}
                                    &                       & \multirow{5}{*}{Ant}            & 2232        & 259.94$\pm$116.75                                                              & 243.30$\pm$121.39                                                          & 222.48$\pm$139.45                                                          \\
                                    &                       &                                 & 3569        & 549.14$\pm$192.30                                                              & 579.86$\pm$213.98                                                          & 495.19$\pm$160.81                                                          \\
                                    &                       &                                 & 4603        & 374.17$\pm$199.78                                                              & 372.37$\pm$194.00                                                          & 367.64$\pm$279.00                                                          \\
                                    &                       &                                 & 5766        & 396.13$\pm$130.69                                                              & 334.74$\pm$172.22                                                          & 361.40$\pm$117.68                                                          \\
                                    &                       &                                 & 7490        & 314.48$\pm$222.06                                                              & 365.24$\pm$212.09                                                          & 275.81$\pm$130.30                                                          \\
        \bottomrule
    \end{tabular}
\end{table*}



% \begin{table*}[t]
% \centering
% \caption{The overall TPR and TNR results based on Grubbs' test (21 shadow models).}
% \label{tab:overall audit accuracy based on Grubbs (21 Shadow Models)}
%     % \setlength{\tabcolsep}{0.6em}
%     % \renewcommand{\arraystretch}{1.1}
%     % \footnotesize
%     \centering
%     \setlength{\tabcolsep}{0.3em}
%     \renewcommand{\arraystretch}{1.1}
%     \scriptsize
% \begin{tabular}{cccccccccc} 
% \toprule
% \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Task}\\\textbf{ Name}\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Offline}\\\textbf{ Model}\end{tabular}} & \multicolumn{2}{c}{\textbf{L1 Norm}}                                & \multicolumn{2}{c}{\textbf{L2 Norm}}                               & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Cosine}\\\textbf{ Distance}\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Wasserstein}\\\textbf{ Distance}\end{tabular}}  \\ 
% \cline{3-10}
%                                                                                        &                                                                                            & TPR                              & TNR                              & TPR                             & TNR                              & TPR                             & TNR                                                           & TPR                             & TNR                                                                 \\ 
% \hline
% \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Lunar\\ Lander\end{tabular}}                & BC                                                                                         & 99.25$\pm$0.97  & 100.00$\pm$0.00 & 98.13$\pm$1.88 & 100.00$\pm$0.00 & 98.00$\pm$1.84 & 100.00$\pm$0.00                              & 99.11$\pm$0.93 & 99.96$\pm$0.10                                     \\
%                                                                                        & BCQ                                                                                        & 99.56$\pm$0.32  & 100.00$\pm$0.00 & 98.40$\pm$0.68 & 100.00$\pm$0.00 & 98.27$\pm$0.47 & 99.99$\pm$0.04                               & 98.80$\pm$0.78 & 99.75$\pm$0.41                                     \\
%                                                                                        & IQL                                                                                        & 97.95$\pm$2.45  & 99.96$\pm$0.17  & 97.11$\pm$3.12 & 99.98$\pm$0.09  & 96.85$\pm$3.40 & 99.92$\pm$0.19                               & 97.11$\pm$3.65 & 97.47$\pm$5.34                                     \\
%                                                                                        & TD3PlusBC                                                                                  & 97.87$\pm$3.45  & 99.45$\pm$0.84  & 96.09$\pm$3.91 & 99.73$\pm$0.58  & 95.78$\pm$4.29 & 99.95$\pm$0.14                               & 98.00$\pm$2.60 & 96.27$\pm$3.43                                     \\ 
% \cline{2-10}
% \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Bipedal\\ Walker\end{tabular}}              & BC                                                                                         & 97.07$\pm$3.60  & 100.00$\pm$0.00 & 97.24$\pm$4.49 & 100.00$\pm$0.00 & 97.69$\pm$4.51 & 100.00$\pm$0.00                              & 98.36$\pm$2.67 & 100.00$\pm$0.00                                    \\
%                                                                                        & BCQ                                                                                        & 100.00$\pm$0.00 & 100.00$\pm$0.00 & 99.56$\pm$0.69 & 100.00$\pm$0.00 & 99.07$\pm$1.65 & 100.00$\pm$0.00                              & 99.96$\pm$0.09 & 100.00$\pm$0.00                                    \\
%                                                                                        & IQL                                                                                        & 95.91$\pm$4.93  & 100.00$\pm$0.00 & 96.36$\pm$4.57 & 100.00$\pm$0.00 & 99.51$\pm$0.26 & 100.00$\pm$0.00                              & 96.44$\pm$4.54 & 100.00$\pm$0.00                                    \\
%                                                                                        & TD3PlusBC                                                                                  & 99.91$\pm$0.18  & 95.05$\pm$19.14 & 99.87$\pm$0.27 & 93.96$\pm$20.97 & 99.82$\pm$0.36 & 92.79$\pm$21.27                              & 99.91$\pm$0.18 & 91.78$\pm$21.28                                    \\ 
% \cline{2-10}
% \multirow{4}{*}{Ant}                                                                   & BC                                                                                         & 98.13$\pm$1.55  & 99.91$\pm$0.18  & 97.73$\pm$1.19 & 99.86$\pm$0.41  & 99.73$\pm$0.53 & 86.82$\pm$26.97                              & 97.55$\pm$1.57 & 99.90$\pm$0.21                                     \\
%                                                                                        & BCQ                                                                                        & 97.16$\pm$2.73  & 99.81$\pm$0.42  & 96.67$\pm$2.18 & 99.84$\pm$0.42  & 99.69$\pm$0.41 & 87.53$\pm$26.74                              & 98.58$\pm$1.69 & 99.80$\pm$0.43                                     \\
%                                                                                        & IQL                                                                                        & 95.91$\pm$3.87  & 99.64$\pm$0.77  & 96.49$\pm$3.89 & 99.68$\pm$0.63  & 99.51$\pm$0.67 & 86.53$\pm$27.80                              & 97.65$\pm$2.19 & 99.64$\pm$0.78                                     \\
%                                                                                        & TD3PlusBC                                                                                  & 99.44$\pm$0.60  & 99.23$\pm$1.53  & 98.27$\pm$1.03 & 99.36$\pm$1.64  & 99.76$\pm$0.33 & 88.42$\pm$25.93                              & 99.79$\pm$0.27 & 99.18$\pm$1.65                                     \\
% \bottomrule
% \end{tabular}
% \end{table*}
























\section{本章小结}
在本章中，我们提出了一种新的轨迹级的离线强化学习场景的数据集审计方法。
该方法指出累计奖励可以作为数据集的内在指纹存在于目标数据集上训练的所有模型中。
通过在四个离线DRL模型和三个任务上进行评估，\sysnameo 的真正率和真负率均超过90\%，
证明了\sysnameo 是一种数据集审计的有效解决方案，可以用于保护数据集所有者的知识产权。
通过研究阴影模型数量和假设检验显著性水平的对于所提方法审计精度的影响，
我们得出了一些有助于实际应用\sysnameo 的重要结论。
鲁棒性评估表明，\sysnameo 可以抵抗可疑模型的输出扰动，
同时整合多个距离度量可以进一步提高\sysnameo 对扰动的鲁棒性。
最后，我们利用来自Google~\cite{DBLP:conf/nips/Pomerleau88}和DeepMind~\cite{DBLP:conf/nips/Gulcehre0NPCZAM20}的开源数据集来检验\sysnameo 的在实际场景中的有效性，
并展示其在已发布数据集上的出色表现。


% \begin{algorithm}[h]
%     \begin{algorithmic} % enter the algorithmic environment
%         \REQUIRE $n \geq 0 \vee x \neq 0$
%         \ENSURE $y = x^n$
%         \STATE $y \Leftarrow 1$
%         \IF{$n < 0$}
%             \STATE $X \Leftarrow 1 / x$
%             \STATE $N \Leftarrow -n$
%         \ELSE
%             \STATE $X \Leftarrow x$
%             \STATE $N \Leftarrow n$
%         \ENDIF
%         \WHILE{$N \neq 0$}
%             \IF{$N$ is even}
%                 \STATE $X \Leftarrow X \times X$
%                 \STATE $N \Leftarrow N / 2$
%             \ELSE[$N$ is odd]
%                 \STATE $y \Leftarrow y \times X$
%                 \STATE $N \Leftarrow N - 1$
%             \ENDIF
%         \ENDWHILE
%     \end{algorithmic}
%     \caption{\label{alg:sample}算法样例}
% \end{algorithm}


\chapter{数据模型超参数泄露风险评估方法}
% 原题目：强化学习模型关键参数泄露风险评估
本章摘要: 
深度强化学习（DRL）已被应用于许多关键系统，例如智能电网、交通控制、自动驾驶等。
为了保护知识产权和减少潜在的安全风险，实际中部署的DRL模型通常只具有黑盒访问权限，即输入环境状态并给出相应的执行动作。
例如，模型神经网络的结构和优化过程中的参数通常不会向用户公开，以免侵犯模型拥有者的知识产权或增加系统的脆弱性。
所以，探究模型的输入状态和输出动作是否具有泄露了超参数设置的风险非常重要。
为了有效地评估模型参数泄露风险，我们面临两个主要挑战：
1）从具有黑盒访问权限的模型中获得的信息有限；
2）不同超参数对DRL模型的行为影响具有强耦合性。

我们提出了一种针对DRL模型的超参数窃取攻击（\sysname），可以用来评估基于不同框架的模型的参数泄露风险。
我们观察到DRL模型在不同的超参数设置下在相同任务上具有不同的行为偏好。
基于该事实，我们设计了两种状态生成机制来刺激DRL模型的产生行为分歧，
并训练了一个攻击模型来刻画DRL模型的行为和超参数设置之间的关系。
通过使用多个DRL模型和环境，我们全面地展示了\sysname 的超参数窃取效果，
例如，在PPO模型和CartPole环境上，所提方法推断准确度超过$90\%$。
另外，\sysname 可以作为增强其他下游攻击的基础，例如\sysname 可以有效提高对抗样本攻击的在不同模型之间的可迁移性。

关键词：模型关键参数；数据泄露；强化学习

\section{引言}

深度强化学习（Deep Reinforcement Learning, DRL）结合了深度神经网络（Deep Neuron Network, DNN）强大的拟合能力，
成为一种具有广阔应用前景的机器学习范式。
例如AlphaGo Zero~\cite{SSSAHGHBLB17}，
在三天内通过自我对弈而无需人类数据，获得了击败人类冠军的能力。
DRL还成功地应用于许多复杂的决策任务，如自动驾驶~\cite{FHOL18}、机器人控制~\cite{A17}、金融交易~\cite{DBKRD17}、电子竞技~\cite{SSSAHGHBLB17,BBCCDDFFHHJGOPPPRSSSSSTWZ19}等。

DRL模型通常只开放输入、输出接口以提供服务，即黑盒访问权限，
一方面，因为泄露模型参数设置的细节可能会使模型更容易受到数据层面的恶意攻击，
如对抗样本攻击~\cite{HPGDA17, BM17, LHLSLS17, KS17}和模型后门攻击~\cite{KWSL20,YIRV19,KWJL20,WSLMJ21,WJWGXS21}。
另一方面，DRL的模型参数设置属于知识产权，泄露将会造成模型所有者的财产损失。
综上，研究在黑盒访问权限下进行DRL模型参数设置泄露风险的评估是至关重要的。

现有文献已经探究了针对有监督深度学习模型的参数设置窃取攻击~\cite{OASF18,WG18}。
但是，这些技术不能直接应用于DRL模型。
在有监督学习场景中，攻击者可以使用训练数据集作为高质量的输入进行模型参数嗅探。
因为DRL模型通常是从与环境的交互中学习，DRL攻击者没有现成的嗅探数据。


在本章中，我们对DRL模型的超参数泄露风险进行评估。
评估的关键依据是DRL模型在某些状态下不一致的行为（行为分歧）将暴露它们的超参数配置信息。
基于这个观察，我们的方法首先构建一系列状态，收集DRL模型的行为分歧，然后利用分类器根据状态和预测的行动来揭示受害者黑盒DRL模型的架构和优化过程。由于行为分歧包含更多的超参数设置知识，所以在具有高质量状态序列的情况下，窃取攻击将变得更加有效，即刺激更多的行为分歧。

我们提出了两种状态生成机制来构建状态序列。 
被动状态生成方法基于被动记录DRL模型与环境之间的交互来构建状态序列，解决了上面提到的第一个挑战。
然而，仅直接从环境中获取状态只能弱化DRL模型生成分歧行动的刺激。
为了使具有相同超参数设置的智能体表现出相似的行为，而具有不同超参数设置的智能体表现出不同的行为，我们进一步提出了一种主动状态生成方法。
具体来说，除了训练分类器预测超参数设置外，主动方法还反向传播分类器模型的部分损失并优化状态。
优化后的状态可以引导具有不同超参数设置的DRL模型生成更多的分歧行动。
此外，我们经验证明，不断调整的状态可以减缓分类器的过拟合，并进一步提高准确性。

我们的实验结果表明，在多个DRL模型和环境下，超参数激活函数的窃取准确率超过$60\%$，甚至在PPO智能体上超过$90\%$。
我们进一步评估了影响窃取攻击性能的四个代表性影响因素，即受害DRL模型的奖励标准，超参数耦合，状态-动作序列的长度以及影子DRL模型的数量。
总之，我们对\sysname 的实际采用获得了三个重要观察结果。
首先，具有更高奖励标准的DRL模型存在更高的超参数泄漏风险，这警示DRL从业者，因为随着DRL性能的提高，超参数隐私变得更加容易受到攻击。
其次，超参数对模型动作偏好有不同的影响强度。
例如，攻击者可以轻松地从DRL模型的行为中区分激活函数或优化器，而很难区分学习率。
第三，主动窃取攻击方法以更少的状态生成和DRL模型训练成本获得了表现优异的结果，这使\sysname 成为一种高效的超参数窃取机制。
此外，我们评估了一种潜在的防御方法，即仅发布预测动作而非置信度值（后验概率），以减少对攻击者可访问的信息。
然而，在某些设置中，主动窃取攻击方法的窃取准确率仍比基线高$20\%$。
最后，我们以对抗攻击为案例研究，并展示了针对黑盒DRL模型的对抗性示例在有或无超参数信息的情况下的可转移性改进，以证明我们的窃取攻击的必要性。

综上，我们的贡献可分为三个方面：
\begin{itemize}
\item 我们提出了首个针对DRL模型的超参数窃取框架\sysname，揭示了当前DRL模型参数泄露风险。
\item 我们在六种不同的DRL模型和环境组合上展示了\sysname 的有效性。我们系统地分析了各种实验因素，并总结出了一些重要的关于实践中采用\sysname 的观察结果。
\item 通过显著提高对抗状态的可迁移性，我们展示了提出的\sysname 可以作为进一步攻击的基础，这提醒人们注意超参数窃取攻击的隐私风险。
\end{itemize}

% \section{预备知识}
% \subsection{强化学习问题}
% \label{sec:Reinforcement Learning Problem}
% 在强化学习问题中，模型的目标是寻找一种最优或接近最优的策略，以最大化完成一段序列决策后累积的“奖励函数”或其他用户提供的奖励信号。
% 模型的学习过程与动物心理学中的条件反射实验相似。
% 例如，假设在投喂食物（奖励）之前播放音乐，
% 那么狗倾向于将声音与食物联系起来，并在仅听到声音刺激时流口水~\cite{PG41}。

% 强化学习问题可以被建模为一个马尔科夫决策过程（Markov Decision Process，MDP）问题，通常由四个变量组成：
% \begin{itemize}
% % \setlength\itemsep{-0.3em}
% \item 状态空间 $\mathbb{S}$ 包含模型的所有可能状态，其中 $s_t \in \mathbb{S}$ 表示时间步 $t$ 时的模型状态。
% \item 动作空间 $\mathbb{A}$ 包含模型的所有可能动作，其中 $a_t \in \mathbb{A}$ 表示时间步 $t$ 时的模型动作。
% \item 状态转移函数 $\mathcal{T}$ 定义为从状态-动作对到新状态的概率映射 $\mathcal{T}: (\mathbb{S} \times \mathbb{A}) \times \mathbb{S} \rightarrow [0, 1]$。
% \item 奖励函数 $\mathcal{R}: \mathbb{S} \times \mathbb{A} \rightarrow \mathbb{R}$ 输出在状态 $s_t$ 上采取动作 $a_t$ 时预期的奖励 $r_t \in \mathbb{R}$。
% \end{itemize}
% 训练后的模型学习到一个策略 $\pi$，
% 它将状态-动作对映射到概率分布 $\pi: \mathbb{S} \times \mathbb{A} \rightarrow [0, 1]$，以最大化累计奖励。
% \begin{equation}
%     \pi^{*}=\underset{\pi}{\arg \max } \sum_{t=t_{0}}^{T} \sum_{a_{t} \in \mathbb{A}} \gamma^{t-t_{0}} \mathcal{R}\left(s_{t}, a_{t}\right) \pi\left(s_{t}, a_{t}\right)
% \end{equation}
% 其中，折扣因子 $\gamma$ 被用于折算累计奖励中的未来奖励，$T$ 是一个游戏回合的终止时间步，$t_0$ 是初始时间步。

% \mypara{示例}
% \begin{figure}[!t]
%     \center
%     \includegraphics[width=0.6\hsize]{figure/drl_hypertheif/fig-drl-examplech.pdf}
%     \caption{强化学习问题示例}
%     \label{fig:running example}
% \end{figure}
% \autoref{fig:running example} 给出了一个运行示例，
% 其中模型与Flappy Bird游戏环境\footnote{\url{https://flappybird.io}} 进行互动。
% 状态包括鸟和障碍物的位置。
% 模型选择是否点击（动作）来让鸟向上飞或下落。
% 奖励为每次操作之后获得的分数。
% 模型从环境中获取状态和奖励，然后在每个时间步从动作空间中选择一个动作。
% 然后，环境更新状态并给予奖励。
% 基于环境状态、执行动作和奖励，模型训练一个策略（神经网络）来最大化累计奖励，即游戏结束时的总分数。

% \subsection{强化学习模型}
% \label{sec:Deep Reinforcement Learning model}
% 强化学习的目标是找到一种最优行为策略以获取高奖励。
% 基于不同的策略优化方法，我们将现有的强化学习模型分为三类。

% \mypara{基于值的强化学习模型}
% Q-learning是一种主要的基于值的强化学习模型 \cite{watkins1989learning}，
% 通过最大化连续步骤中总奖励的期望值来找到最优策略。
% Q-learning通过训练拟合一个用于计算状态-动作组合质量的函数：
% $
% \mathcal{Q}: \mathbb{S} \times \mathbb{A} \rightarrow \mathbb{R}.
% $
% 对于基本的Q-learning算法，$\mathcal{Q}$ 在训练初始化阶段被定义为一张充满随机值的表格。
% 在每个时间步 $t$，模型基于当前的环境状态，选择一个执行动作 $a_t$，并收到奖励 $r_t$。
% 然后，环境更新状态至 $s_{t+1}$（取决于先前的状态 $s_t$ 和选择的动作 $a_t$），
% 模型根据交互过程中的状态、动作和奖励数据更新 $\mathcal{Q}$ 中的值。
% $\mathcal{Q}$每次更新过程如下所示。
% \begin{equation}
%     \label{eq:q-learning-obj}
%     \mathcal{Q}^{\text{new}} \leftarrow \mathcal{Q} + 
%     \alpha \cdot\left(r_{t}+\gamma \max_{a} \mathcal{Q}\left(s_{t+1}, a\right)-\mathcal{Q}\left(s_{t}, a_{t}\right)\right), 
% \end{equation}
% 其中，$\alpha \in (0,1]$ 是学习率，$\mathcal{Q}^{\text{new}}$ 的变化速度随着 $\alpha$ 的增大而加快。

% 如\autoref{eq:q-learning-obj}所示，Q learning使用原始的 $\mathcal{Q}$ 值和通过交互获取的新信息的加权平均值来进行更新。

% 然而，当状态和动作空间十分庞大时，Q-learning 很难维护和存储一个巨大的$\mathcal{Q}$。
% 为了解决这个问题，深度Q-learning网络（Deep Q-learning Network, DQN)~\cite{MKSGAWR13} 将深度神经网络用来拟合$\mathcal{Q}$。
% 凭借着神经网络在学习高维特征表示和函数逼近方面的卓越性能，DQN在围棋 \cite{SSSAHGHBLB17} 和 Atari 游戏上达到了人类专家水平~\cite{MKSGAWR13}。

% \mypara{基于策略更新的强化学习模型}
% 相比于Q-learning通过和环境交互拟合$\mathcal{Q}$，基于策略更新的强化学习模型直接学习状态和动作之间的映射关系。
% 例如，REINFORCE~\cite{W92}通过梯度下降更新神经网络参数的以获得交互策略。
% 目前，基于策略更新的强化学习模型倾向于使用深度神经网络，如多层感知机 \cite{XLZP18} 或循环神经网络 \cite{ZMFLA16}）作为策略$\pi$的载体。
% 基于策略更新的强化学习模型的目标函数为最大化交互累计奖励。
% \begin{equation}
%     \label{eq:pg-obj}
%     J(\theta)=\mathbb{E}_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta}}\left[\sum_{t=0}^{\infty} \gamma^{t} r_{t}\right]. 
% \end{equation}
% 根据文献{\rm\parencite{KT99}}，目标函数的梯度可写成如下方程：
% \begin{equation}
%     \nabla_{\theta} J(\theta)=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) v_t\right], 
% \end{equation}
% 其中 $v_t$ 是 $Q_{\pi_{\theta}}(s,a)$ 的采样值，$Q_{\pi_{\theta}}(s,a)$则表示MDP问题中真实的累计奖励函数，而 $\pi\left(s, a\right)$ 则是策略网络。

% \mypara{Actor-Critic强化学习模型}
% Actor-Critic 强化学习模型 \cite{MBMGLHSK16} 由两个主要的部分构成：Actor 和 Critic，分别表示策略和动作值函数。
% Actor 负责决定采取什么动作，而 Critic 则对 Actor 采取的动作进行评分。
% Actor 训练过程与基于策略更新的强化学习模型相似，
% Critic 通过计算动作值函数来评估 Actor 采取的动作。
% 由于动作值函数有助于策略更新过程，
% 因此 Actor-Critic 强化学习模型可以减少参数更新过程中的梯度方差，加速模型收敛过程。
% Actor-Critic 强化学习模型有两个目标函数，分别用于 Actor 和 Critic 网络。
% 对于 Actor，其目标函数形式类似于 \autoref{eq:pg-obj}，但是使用的是 $\mathcal{A}_{\pi_{\theta}}(s, a)$ 而不是 $Q_{\pi_{\theta}}(s, a)$。
% \begin{equation}
%     \mathcal{A}_{\pi_{\theta}}(s, a)=Q_{\pi_{\theta}}(s, a)-V_{\pi_{\theta}}(s), 
% \end{equation}
% 其中 $V_{\pi_{\theta}}(s)$ 表示状态 $s$ 的平均值。
% 当 $\mathcal{A}{\pi{\theta}}(s, a) = 0$ 时，调整动作概率不会带来额外的收益，
% 此时 Actor 的参数将不再更新。
% Critic 的目标函数是最小化估计动作值函数和真实动作值函数之间的误差。

% 为了加速 Actor-Critic 强化学习模型的训练过程\cite{SLAJM15}，
% 文献{\rm\parencite{SWDRK17,HTSLMWTEWERS17}}将一个新的目标函数称为“裁剪代理目标函数”（clipped surrogate objective function）整合到了 Actor-Critic 算法中，
% 新的算法被称为近端策略优化（proximal policy optimization, PPO）。
% 由于PPO易用性，OpenAI将PPO作为其默认的深度强化学习算法\footnote{\url{https://openai.com/blog/openai-baselines-ppo/}}。

% \subsubsection{深度强化学习模型超参数}
% \begin{table}[!t]
%     \caption{超参数总结}
%     \label{tab:hyper-parameter settings}
%     \centering
%     \resizebox{0.5\linewidth}{!}{
%     \setlength{\tabcolsep}{0.6em}
%     \begin{tabular}{ccc}
%     % \hline
%     \toprule
%     \textbf{参数类型}  & \textbf{参数名称} & \textbf{参数集合}             \\ %\hline
%     \toprule
%     \multirow{3}{*}{\textbf{模型结构}} & \textbf{激活函数}      & ReLU, ELU, Tanh    \\ 
%     % \rowcolor{mygray}
%      & \textbf{隐含层数}   & 2, 3, 4            \\
%      & \textbf{神经元数}   & 64, 128, 256       \\ 
%     % \rowcolor{mygray}
%     \toprule
%     \multirow{2}{*}{\textbf{模型优化}} &\textbf{优化器}       & SGD, Adam, RMSprop \\
%      & \textbf{学习率}   & 1e-3, 5e-4, 1e-4   \\ 
%     % \rowcolor{mygray}
%     \toprule
%     \textbf{RL特有参数} & \textbf{Gamma}           & 0.9, 0.95, 0.99    \\ %\hline
%     \bottomrule
%     \end{tabular}
%     }
%     % \vspace{-0.3cm}
% \end{table}
% DRL模型通过调整DNN模型来拟合行为策略，
% 因此，按照超参数对DRL模型的不同影响，
% DRL模型的超参数可以分为三个部分：DNN模型结构超参数、DNN模型优化超参数以及RL模型特有参数。
% 在本章中，我们主要关注六种DRL模型超参数（参见\autoref{tab:hyper-parameter settings}），
% % 其中包括先前DNN超参数窃取攻击\cite{OASF18}中的超参数和RL模型的折扣因子$\gamma$。
% 这些超参数常作为DRL模型训练的默认可调参数。
% 例如，亚马逊公司的DeepRacer实验平台\cite{BMGGDKRSTTCMK19}开放这些超参数的配置接口供用户调整DRL模型。

% 我们在本章没有考虑更复杂的神经网络结构，主要原因是过大或过深的神经网络对会使DRL模型难以收敛~\cite{ota2020can}。
% 首先，相比于监督学习可以在整个训练结束后使用已训练好的模型，深度强化学习模型需要少量的训练后立即使用刚刚训练好的策略，这限制了DRL只能使用相对较浅的神经网络以确保快速拟合。
% 其次，深度强化学习模型的训练数据不如监督学习稳定，并且不能将训练数据分出验证集以避免过拟合。
% 因此，DRL模型不能使用过于宽阔的网络以避免由于参数过度冗余而产生的过拟合。
% 最后，DRL模型对超参数的选择比较敏感，一定程度限制了超参数调整的范围~\cite{henderson2018deep}。

\section{问题建模}
\label{sec:overview}

\begin{figure}[!ht]
    \center
    \includegraphics[width=0.6\hsize]{figure/drl_hypertheif/fig-drl-systemch.pdf}
    \caption{本文算法应用场景}
    \label{fig:application scenario}
\end{figure}
\subsection{攻击者能力假设}
我们考虑一个对受害者DRL模型具有黑盒访问权限的攻击者，这是最符合实际情况的情景。
当受害者DRL模型与环境交互时，攻击者可以观察其状态和动作。
另外，攻击者可以将受害者DRL模型部署在自己构建的环境中，因此攻击者可以在合法范围内调整环境状态。
本章主要评估模型超参数的泄露风险，对于DRL模型所使用框架的泄露风险，
可以基于文献{\rm\parencite{CGZXL21}}所提方法进行评估。
% 实际上，攻击者可以计算平均累计奖励以估计受害者的奖励标准（性能）从模型与环境的交互中。例如，如果DRL模型经过多个周期后的平均累计奖励为300，则该模型的奖励标准为300。

\subsection{攻击目的}
% 攻击者旨在推断DRL模型的超参数设置，
攻击者的目标是以较高的准确率推断受害者DRL模型所使用的超参数设置，例如精准判断模型的非线性激活函数和优化器的类型。

\subsection{攻击场景}
\autoref{fig:application scenario} 描述了一个典型的应用场景，
即DRL模型服务供应商训练了一个DRL模型，然后提供API给用户使用。
一个恶意用户想要从受害者DRL模型中推断出超参数设置，
以窃取知识产权或将其作为攻击其他顾客模型的基础。


\section{本文方法（\sysname）}
\subsection{工作流程}
\label{sec:workflow}
\begin{figure*}[!ht]
    \includegraphics[width=\hsize]{figure/drl_hypertheif/fig-overview-v3.pdf}
    \caption{\sysname 算法主要由四个步骤组成：阴影模型生成、状态-动作序列收集、攻击模型训练和模型超参数窃取。
    \sysname 基于受害者DRL模型的状态和动作中推断其超参数设置。
        % Workflow of \sysname. The four steps in the \sysname algorithm, \ie, shadow model generation, state-action sequence collection, attack model training, and model hyper-parameter stealing. \sysname infers the policy network information from the state and action of the victim DRL model.
    }
    \label{fig:inferring the policy network information}
\end{figure*}

在\autoref{fig:inferring the policy network information}中，
我们以训练DRL模型玩Flappy Bird游戏环境作为例子，
来详细说明DRL超参数推断的工作流程。

\mypara{第一步：阴影模型生成}
在\autoref{fig:inferring the policy network information}的左虚线框中，
攻击者利用Flappy Bird环境生成一组具有不同超参数组合的阴影DRL模型集合。
例如，为了窃取受害DRL模型的激活函数，
攻击者需要训练一组使用不同种类的激活函数的阴影DRL模型。

\mypara{第二步：状态-动作序列收集}
在第二个虚线框中，阴影DRL模型观测环境状态并做出动作。
具体来说，在阴影DRL模型和环境交互过程中，
攻击者记录每个阴影DRL模型的接收的环境状态$s_t$和动作$a^{n}_{t}$，
其中$a^{n}_{t}$表示第$n$个DRL模型在第$t$步的动作。
基于阴影DRL模型动作的差异，
攻击者可以从状态-动作序列中推断出阴影DRL模型的超参数信息。
% 直观地说，攻击者会在收集的状态-动作序列长度越长时，发现更多模型行为的差异。
% 因此，在\autoref{sec:ablation study}中，我们也使用不同长度的状态-动作序列来评估盗取效果。

\mypara{第三步：攻击模型训练}
在完成前两步之后，攻击者获得了一组状态-动作序列以及相应的超参数设置。
对于所有状态-动作序列，攻击者将每一步的状态和动作拼接成一个$(|s|+|a|)$维的特征向量。
这些特征向量沿时间维度拼接成一个$T \times (|s|+|a|)$维的特征矩阵$x$，攻击者得到一个带有超参数设置标签集$Y$的特征集$X$。
然后，可以使用监督分类器（攻击模型）来拟合特征$X$和标签$Y$之间的关系。
攻击模型的目标函数是
\begin{equation}
    \min _{\theta} \underset{x \sim \mathcal{X}}{\mathbb{E}}\left[
        \mathcal{L}\left(f_{\theta}^{p}\left(\left[x\right]_{i=1}^{n}\right), y^{p}\right)\right]\text{,}  
\end{equation}
\mypara{第四步：模型超参数窃取}
攻击者构造状态$s_t$并记录受害者模型的对应行动$a^{v}_t$。
经过第三步的状态和动作预处理之后，攻击者将状态-动作特征输入攻击模型，获得超参数预测结果。

在其他设置不变的情况下，不同的状态$s_t$会导致推断精度的差异。
例如，当小鸟的位置低于障碍物时，所有影子DRL模型都采取了相同的动作，那么攻击模型将无法从状态-动作序列中获得任何模型超参数的信息。
相反，如果攻击者构造的状态能够刺激DRL模型产生更多的行为分歧，攻击模型的推断将更加有效。



\subsection{攻击状态构造}
\label{sec:attack state generation}
在攻击过程中，状态序列在推断受害模型的超参数设置方面起着重要作用。
接下来，我们介绍两种状态构造方法。
\begin{algorithm}[!t]
    \caption{状态扰动}
    \label{alg:state perturbation}
    \begin{algorithmic}[1]
    \REQUIRE 环境 $E$, 状态 $s$
    \ENSURE 扰动后的状态 $s^{\prime}$
    \STATE $//$ 获得环境状态的合法取值范围 of $E$
    \STATE low\_bound, high\_bound = $E$.spaces.Box()
    \STATE $//$ 限制扰动噪声的幅值
    \STATE low\_bound, high\_bound = $\frac{\text{low\_bound}}{10}$, $\frac{\text{high\_bound}}{10}$
    \FOR {$i$-th dimention in $E$.spaces}
    \STATE ${\sigma}$ = min$\left(\lvert\text{low\_bound}\rvert, \lvert\text{high\_bound}\rvert\right)$
    \WHILE {True}
    \STATE $//$ 判断扰动后的状态是否在合法取值范围
    \STATE $s^{\prime}$[$i$] = $s$[$i$] + $N\left(0, {\sigma}^2 \right)$
    \IF {$s^{\prime}$[$i$] in $E$.spaces.Box(dim=$i$)}
    \STATE break
    \ENDIF
    \ENDWHILE
    \ENDFOR
    \STATE 返回 $s^{\prime}$
    \end{algorithmic}
\end{algorithm}


\mypara{被动状态构造机制}
\begin{algorithm}[!t]
    \caption{被动状态构造方法}
    \label{alg:Generation Mechanism 1}
    \begin{algorithmic}[1]
    \REQUIRE Shadow DRL models $M$, environment $E$, the max time step $T$
    \ENSURE State sequence $S = {s_1, s_2, ..., s_T}$
    \STATE Randomly select a DRL model with policy $\pi_{\theta}$
    \STATE Let $S = \{\}$
    \STATE Initialize $E$ randomly with $s_0$ and done\_signal = False
    % \STATE done_signal = False
    \FOR {time step $t$ from $1$ to $T$}
    \IF {done\_signal == True}
    \STATE $s_0^{\prime}$ = \autoref{alg:state perturbation}~($E$, $s_0$)
    \STATE Re-initialize $E$ randomly with $s_0^{\prime}$
    \STATE $//$ Get the action from the DRL model
    \STATE $a_t = \underset{a \in \mathbb{A}}{\arg \max \text{}} \pi_{\theta}(s_0^{\prime}, a) $
    \ELSE
    \STATE $a_t = \underset{a \in \mathbb{A}}{\arg \max \text{~}} \pi_{\theta}(s_{t-1}, a) $
    \ENDIF
    \STATE $//$ Get the new state $s_t$ and the game over signal
    \STATE $s_t, \text{done\_signal} = E(a_t) $
    \STATE Insert the $s_t$ into $S$
    \ENDFOR
    \STATE Return $S$
    \end{algorithmic}
\end{algorithm}
获取状态序列的一种直观方法是直接从影子深度强化学习模型与环境的交互中收集。
\autoref{alg:Generation Mechanism 1} 显示了详细的状态生成过程。
攻击者随机选择一个带有策略$\pi_{\theta}$的深度强化学习模型，然后使用状态空间中的任意$s_0$初始化环境。
例如，在flappy bird游戏中，位于小鸟位于屏幕中央是一个有效状态。
DRL模型将$s_0$放入策略网络$\pi_{\theta}$，然后获取动作$a_1$。
环境转移到下一个状态$s_1$，并判断游戏是否结束。
攻击者将$s_1$插入状态序列$S$中。如果$\text{done\_signal}$是$\text{True}$，则$s_t$是环境的终止状态。
攻击者检查时间步$t$是否等于达到最大步长$T$，
如果$t<T$，攻击者通过\autoref{alg:state perturbation} 生成一个新的初始状态$s^{\prime}_0$，并重新初始化环境。
然后，攻击者继续收集状态，直到时间步长达到$T$。
如果状态维度之间存在物理约束，
我们只需使用\autoref{alg:state perturbation} 扰动部分维度，并通过约束获得其余维度。

被动方法是一种有效的生成状态序列的策略。
然而，它也存在两个缺陷：
1) 没有充分利用白盒影子模型，例如白盒模型的模型权重、更新梯度信息；
2) 攻击模型容易因为静态状态序列而在影子模型上过拟合。


\mypara{主动状态构造机制}
\begin{algorithm}[!t]
    \caption{主动状态构造机制}
    \label{alg:Generation Mechanism 2}
    \begin{algorithmic}[1]
    \REQUIRE 影子模型集合 $M$, 环境 $E$, 最大步长 $T$, 超参数设置集合 $Y$, 攻击模型 $f_{\theta}^{p}$
    \ENSURE 优化后的状态 $S = \{s_1, s_2, ..., s_T\}$
    \STATE $//$ 获取初始状态
    \STATE $S$ = \autoref{alg:Generation Mechanism 1}~($M$, $E$, $T$)
    \STATE state\_update = False
    \FOR {epoch $i$ from $1$ to $n$}
    % \State $//$ BUILD FEATURE MATRIX SET $X$ AND HYPER-PARAMETER SETTING LABEL SET $Y$ BASED ON STEP 2 AND STEP 3 IN \autoref{sec:workflow}
    \IF {$i$ \% $m$ == 0 and state\_update == False}
    \STATE $f_{\theta}^{p}$.loss = $f_{\theta}^{p}$.loss\_calculation($X$, $Y$)
    \STATE $//$ 更新$S$
    \STATE 基于$f_{\theta}^{p}$.loss 反向传播来优化 $S$
    % \State BUILD THE NEW FEATURE SET $X^{\prime}$, AND LET $X=X^{\prime}$
    \STATE state\_update = True
    \ENDIF
    \IF {$i$ == 1 or state\_update == True}
    \STATE $//$ 记录影子模型的动作
    \STATE $X$ = []
    \FOR {$M$中的第$j$个影子模型}
    \STATE $\{a_1, a_2, ..., a_T\}_j$ = $M[j](S)$
    \STATE $X$.append(concatenate($\{a_1, a_2, ..., a_T\}_j$, $S$))
    \STATE 将$X$转化为tensor
    \ENDFOR
    \STATE state\_update = False
    \ENDIF
    \STATE $//$ 更新攻击模型
    \STATE $f_{\theta}^{p}$.train($X$, $Y$)
    \ENDFOR
    \STATE 返回 $S$
    \end{algorithmic}
\end{algorithm}
攻击者利用阴影深度强化学习模型的梯度来生成更有效的状态序列，即主动状态生成方法。
由于阴影深度强化学习模型是白盒的，攻击者可以访问其超参数设置和策略网络中的梯度信息。
与静态状态序列的 \autoref{alg:Generation Mechanism 1} 相比，
\autoref{alg:Generation Mechanism 2} 在攻击模型的训练过程中更新状态序列，即动态状态序列。
我们在训练攻击模型时优化状态以最小化模型损失函数，从而加强状态-动作序列与超参数之间的关系。
\begin{equation}
    \min _{S} \underset{M \sim \mathcal{M}}{\mathbb{E}}\left[ \mathcal{L}\left(f_{\theta}^{p}\left(\left(S, M\left(S\right)\right)\right), y^{p}\right)\right], 
\end{equation}
其中，$S$是状态序列，$\mathcal{M}$是DRL阴影模型集合，$y^{p}$是超参数$p$的真实标签，$\mathcal{L}$是交叉熵损失。
如\autoref{alg:Generation Mechanism 2}第8行所示，
在训练过程中，攻击模型$f_{\theta}^{p}$的损失会在每$m$步反向传播以优化状态序列$S$。
攻击者基于更新后的状态序列$S$重建一个新的特征集$X$，然后使用$X$和$Y$训练分类器，直到下一个状态更新。
攻击者使用最新的状态序列从受害者DRL模型中收集动作，进行超参数推断。

\subsection{讨论}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.6\hsize]{figure/drl_hypertheif/fig-drl-challengech.pdf}
    \caption{针对强化学习模型进行超参数推断攻击中的新挑战。与分类中的显式标签相比，强化学习问题通常有许多等效策略，即最优策略的非唯一性。
    例如，路径1和路径2都是最短路径，这会对推断攻击的造成干扰。} 
    % \mc{I suggest using a shallow grid color to observe the different actions easily.}}
    \label{fig:drl-challenge}
\end{figure}

\sysname 与文献{\rm\parencite{OASF18}}中的逆向工程策略具有相似之处，即一种用于图像分类器模型的超参数推断方法。
文献{\rm\parencite{OASF18}}的逆向工程策略也具有以下特点：
1）从一系列查询中推断出DNN模型的内部信息，
2）生成更有效的输入以增强对黑盒模型超参数的推断精度。
然而，\sysname 在几个关键方面与文献{\rm\parencite{OASF18}}不同，这些差异主要是因为这两种方法针对的受害模型不同。
文献{\rm\parencite{OASF18}}是针对DNN框架下的图像分类器模型，而\sysname 适用于序列决策制定的DRL模型。

\begin{itemize}
    \item \mypara{缺乏良好的数据集}
    与DNN训练场景不同，Oh等人~\cite{OASF18}可以使用训练样本作为高质量的查询输入，而DRL模型则是从环境的交互中进行学习。
    因此，在攻击之前\sysname 没有现成的查询输入。
    我们需要根据训练好的阴影模型构建状态。
    \item \mypara{序列决策任务} 
    文献{\rm\parencite{OASF18}}通过一个单独的查询输入来从DNN模型中提取信息，即通常在每个步骤中处理一个图像样本。
    由于DRL模型是为了优化累积回报而设计的，每个步骤中包含的内部信息是有限的（回顾\autoref{sec:Reinforcement Learning Problem}）。
    因此，\sysname 需要构建一系列状态来推断模型超参数。
    \item \mypara{新的超参数} 
    除了文献{\rm\parencite{OASF18}}探究的超参数外，\sysname 还考虑了DRL的特有超参数，即折扣因子$\gamma$。
    \item \mypara{最优策略的非唯一性} 
    如\autoref{fig:drl-challenge}所示，\sysname 在DRL情景中面临新的挑战。
    对于分类问题，每个样本只有一个正确的标签。
    然而，在RL问题中通常存在多个最优策略，这会对推断准确性造成干扰。
\end{itemize}

\section{性能评估}
\label{sec:evaluation}
在本节，我们在三种深度强化学习模型DQN、PG和PPO，以及两个Gym~\cite{BCPSSTZ16}环境CartPole和Acrobot验证\sysname 的有效性。
这些模型和环境在现有研究~\cite{WJWGXS21,CGZXL21,PWZLYS19,GDWKLR20,WGWX21}中被广泛使用。
通过实验评估，我们计划回答下列问题。

\begin{itemize}
    \item 问题 1：能否通过 DRL 模型的行为推断出其超参数设置？
    \item 问题 2：哪种状态更有利于推断出超参数设置？
    \item 问题 3：其他实验因素，例如受害者 DRL 模型的性能、超参数耦合、状态-动作序列的长度以及阴影DRL模型的数量，如何影响攻击？
    \item 问题 4：是否有防御方法可以有效抵御\sysname ？
\end{itemize}

\subsection{环境}
\label{subsec:environments}

我们选择了 Gym 中的“CartPole”环境，该环境同样被其他最新的DRL模型窃取攻击的工作采用~\cite{CGZXL21}。
除了“CartPole”，我们还在“Acrobot”环境上评估所提方法。
我们基于这些环境评估 \sysname 的性能，
并证明了所提方法在不同 DRL 模型和环境中的有效性。
我们认为这些环境是具有代表性的：
1）Gym 环境是主流开源研究平台之一，其提供了统一的接口来训练和测试 DRL 模型；
2）所选的两个环境常用于评估针对 DRL 模型的攻击的有效性~\cite{CGZXL21,IUQJAHN20,PTLBC18}。

\subsection{实验设置}
\label{sec:experimental setup}
\mypara{DRL模型}
我们在 DQN、PG 和 PPO 模型上评估了 \sysname 。
这三个模型代表了 RL 模型的三种基本思想，即基于值的强化学习模型、基于策略更新的强化学习模型和Actor-Critic强化学习模型。
% 大多数现有的无模型 RL 方法都是从这三种方法演变而来的。
DQN 和 PG 通常作为复杂 DRL 模型的基础模块，
PPO 模型代表着最新的DRL模型。
因为卓越的模型性能和训练收敛性，PPO被OpenAI设置为默认的DRL模型，并在现有文献中广泛使用~\cite{WJWGXS21,CGZXL21,GDWKLR20,WGWX21}。

\mypara{目标超参数}
如\autoref{tab:hyper-parameter settings}所示。
表中前三个超参数涉及深度强化学习策略网络架构，
其他超参数与模型优化过程有关。
对于每个超参数，我们选择了三个不同的值。
这些值通常是开源框架（如Baselines和Stable-baselines）的默认选择，例如优化器选项中的ReLU和Adam。

\mypara{影子模型构造}
我们考虑三个DRL模型（DQN，PG和PPO）和两个Gym环境（CartPole和Acrobot）的六种实验组合。
对于每种实验组合，我们在每个超参数设置下训练20个影子DRL模型（共$20\times3^6$个）。
然后，我们将这些影子模型按照4:1分成一个训练模型集合（$16\times3^6$）和一个测试模型集合（$4\times3^6$）。
由于在某些超参数设置下很难训练出和受害者模型性能相同的影子模型\cite{MBMGLHSK16}，
因此\autoref{tab:hyper-parameter settings}中的超参数分布在影子DRL模型集合中是不均匀的。

\mypara{性能指标}
我们使用超参数设置的预测准确性来评估推断攻击的效果\cite{PWZLYS19,CGZXL21,OASF18}。
具体来说，我们利用训练模型集合中的影子模型生成状态-动作序列并训练攻击模型。
然后，我们将测试模型集合中的影子模型作为受害模型，并基于状态-动作序列推断其超参数设置。

\mypara{对比方法}
我们比较了被动和主动状态构造方法的推断性能差异。
其中，“passive\_env”（\autoref{alg:Generation Mechanism 1}）直接使用DRL模型和环境之间的交互得到的状态。
“active\_env”在“passive\_env”构造状态的基础上，利用\autoref{alg:Generation Mechanism 2}对状态执行进一步的优化。
在实验中，这两种方法的默认序列长度为200。
基线方法“majority\_vote”采用影子模型集合中比例较大的超参数设置作为推断结果。

\mypara{攻击模型设置}
我们基于ResNet框架构造攻击模型，其具体结构如\autoref{tab:structure of attack model}所示。
我们对攻击模型进行400个epoch的训练。
对于被动方法，我们使用学习率为0.01的SGD优化器来训练攻击模型。
对于主动方法，我们在攻击模型训练期间每10个epoch使用学习率为0.01的Adam优化器来优化状态。

\begin{table}[!t]
    \caption{攻击模型的网络结构。
    在每个卷积层中，$K_c$（核大小）从7变为3。
    $C_o$是输出通道数，变量\textit{stride}是互相关步长。
    $L_{out}$是AdaptiveAvgPool函数的输出向量长度。
    % \mc{If the space is limited, put this table to appendix.}
    }
    \label{tab:structure of attack model}
    \centering
    \resizebox{0.5\linewidth}{!}{
    \setlength{\tabcolsep}{0.8em}
    \begin{tabular}{cc}
    \toprule
    \textbf{Layer Name} & \textbf{Hyper-parameter}                    \\ \toprule
    \textbf{Conv1\_x}            & $K_c=7, 5, 3$, $C_o=64$, $\text{stride}=1$  \\
    % \rowcolor{mygray}
    \textbf{Conv2\_x}            & $K_c=7, 5, 3$, $C_o=128$, $\text{stride}=1$ \\ 
    \textbf{Conv3\_x}            & $K_c=7, 5, 3$, $C_o=128$, $\text{stride}=1$ \\
    % \rowcolor{mygray}
    \textbf{AdaptiveAvgPool}     & $L_{out}=\text{sequence length}$            \\
    \textbf{Relu}                & -                                           \\
    % \rowcolor{mygray}
    \textbf{Linear\_1}           & $(C_o \times L_{out}, L_{out})$             \\ 
    \textbf{Linear\_2}           & $(L_{out}, \text{\#classes})$               \\
    % \rowcolor{mygray}
    \textbf{Softmax}             & dim=1                                       \\ \bottomrule
    \end{tabular}
    }
\end{table}
% 超参数值的分布情况在\autoref{sec:the distributions of hyper-parameter}中给出。

\mypara{实验部署平台}
我们使用Tianshou库\cite{weng2021tianshou}来实现所有DRL模型。
所有攻击方法采用Python 3.7实现，并在NVIDIA DGX-A100服务器上运行。

\subsection{总体性能}
\label{sec:overall performance}
\mypara{实验设置}
受害者DRL模型在与环境的一回合交互中能够获取的累计奖励分别为500（Cartpole）和-100（Acrobot）。
在\autoref{fig:overall performence}中，
x轴展示了不同的超参数类型，y轴展示了相应的推断准确性。
每个直方图展示了在三次重复实验中推断准确率的平均值和标准差。

\begin{figure*}[!ht]
    \centering
    \subfigure[DQN, CartPole, Reward = 500]{
    \includegraphics[width=0.3\hsize]{figure/drl_hypertheif/overall_acc/dqn-cartpole-reward500-20220731.pdf}
    \label{fig:DQN CartPole 500 all}
    }
    \subfigure[PG, CartPole, Reward = 500]{
    \includegraphics[width=0.3\hsize]{figure/drl_hypertheif/overall_acc/pg-cartpole-reward500-20220731.pdf}
    \label{fig:PG CartPole 500 all}
    }
    \subfigure[PPO, CartPole, Reward = 500]{
    \includegraphics[width=0.3\hsize]{figure/drl_hypertheif/overall_acc/ppo-cartpole-reward500-20220731.pdf}
    \label{fig:PPO CartPole 500 all}
    }
    \\[-2ex]
    \subfigure[DQN, Acrobot, Reward = -100]{
    \includegraphics[width=0.3\hsize]{figure/drl_hypertheif/overall_acc/dqn-acrobot-reward0100-20220731.pdf}
    \label{fig:DQN Acrobot -100 all}
    }
    \subfigure[PG, Acrobot, Reward = -100]{
    \includegraphics[width=0.3\hsize]{figure/drl_hypertheif/overall_acc/pg-acrobot-reward0100-20220731.pdf}
    \label{fig:PG Acrobot -100 all}
    }
    \subfigure[PPO, Acrobot, Reward = -100]{
    \includegraphics[width=0.3\hsize]{figure/drl_hypertheif/overall_acc/ppo-acrobot-reward0100-20220731.pdf}
    \label{fig:PPO Acrobot -100 all}
    }\\[-0.5ex]
    \subfigure{\includegraphics[width=0.4\textwidth]{figure/drl_hypertheif/overall/overall-legend-20220731.pdf}} 
    % \subfigure{
    % \includegraphics[width=0.65\hsize]{figure/drl_hypertheif_others/comparison_1D_legend6.pdf.pdf} 
    % }
    \vspace{-0.4cm}
    \caption{不同方法在DRL模型和环境六种组合上的超参数推断准确性。
    每个图的x轴标签依次对应于激活函数、隐藏层、神经元数、优化器、学习率和gamma超参数。
    y标签显示推断准确率。
    % \mc{This figure is the most important figure, make it as clear as possible.
    % Make the fontsize larger, add one sentence to explain the reward numbers. I am not sure whether ``-100'' is the highest reward?}
    % \mc{Check the name of each method, keep them consistent in the whole paper, including the figures and captions. Check whether the values are consistent with the conclusions. }
    }
    % \vspace{-0.2cm}
    \label{fig:overall performence}
\end{figure*}

\mypara{实验结果}
我们从\autoref{fig:overall performence}总结出以下结论。
1）\sysname 的窃取精度都高于majority\_vote方法，这意味着DRL模型的行为确实可以暴露模型的超参数设置。
例如，激活函数的推断精度超过了60％，在PPO上甚至超过了90％。
2）\sysname 在不同的 DRL 模型上推断准确率不同。
对于优化器和学习率，攻击者在 PG 和 PPO 模型上的推断效果优于 DQN 模型。
如 \autoref{fig:PG CartPole 500 all} 和 \autoref{fig:PPO CartPole 500 all}，\sysname 在优化器和学习率上取得了巨大的准确率提升，而在 \autoref{fig:DQN CartPole 500 all} 中略微超过了基线方法。
回顾 \autoref{sec:Deep Reinforcement Learning model}，DQN 模型在训练过程中使用策略网络来近似$\mathcal{Q}$表，并根据形成的$\mathcal{Q}$表选择动作，即状态是通过影响Q值进而决定行为动作。
PG 和 PPO 则使用策略网络直接建立状态和动作之间的关系，而不是像 DQN 中那样近似$\mathcal{Q}$表，即状态直接决定行为动作。
因此，从 DQN 中推断出与优化相关的超参数更为困难。
3）从\autoref{fig:overall performence}中可以看出，\sysname 对于不同的超参数具有不同的盗取准确度。
通常情况下，\sysname 在激活函数上的推断准确度要高于学习率。
作为结构超参数，不同的激活函数具有独特的特征。
例如，Tanh激活函数在$(0,0)$处对称，而ReLU和ELU激活函数是不对称的。
此外，激活函数的功能特征在构建DRL模型后不会改变，因此激活函数的推断准确性在DRL模型和环境的组合上保持良好性能。
对于学习率，它控制着模型权重的更新速度。
相对于激活函数而言，学习率通过影响模型的权重值来引导模型的行为。
因此，学习率的特征在随着模型训练会变得难以推断。

\subsection{受害者模型性能的影响}
\label{sec:impact of model reward}
\begin{figure*}
    \centering
    \includegraphics[width=\hsize]{figure/drl_hypertheif/agent_reward/agent_reward.pdf}
    \caption{
    % The stealing accuracy on three models and the CartPole environment when varying the reward criteria of the victim. 
    % The x labels show the reward criterion of the victim model. 
    % The y labels show the stealing accuracy.
    % The left six plots show the absolute stealing accuracy, and the right six plots show the relative stealing accuracy.  
    在不同受害者模型性能情况下，所提方法在三个模型框架上的推断准确率。
    x轴展示了受害者模型性能（累计奖励），
    y轴展示了所提方法的推断准确率。
    左侧六个图为绝对推断准确率，
    右侧六个图为相对于基线方法“majority\_vote”推断准确率。
    } 
    % \mc{Maybe this is not necessary. If possible, keep the hyparparameter order same in all figures. Otherwise, the line colors might not consistent.}} 
    \label{fig:impact of model reward}
    % \vspace{-0.2cm}
\end{figure*}
我们研究了受害者模型性能和超参数泄露风险之间的关系。

\mypara{实验设置}
基于CartPole环境，我们探究受害者模型的累计奖励从100到500变化时\sysname 推断准确性的变化。
推断准确性受到两个因素的影响。
一方面，随着累计奖励的提高，能够达到该条件的可行超参数设置会减少，这相当于变相降低了推断的难度。
从文献{\rm\parencite{MBMGLHSK16}}中可知，DRL模型对超参数的选择较为敏感。
例如，A3C模型以接近$10^{-3}$的学习率在五个游戏中获得最高分数\cite{MBMGLHSK16}。
另一方面，当累计奖励标准提高时，模型的行为偏好往往难以区分，因为获得高累计奖励的策略数量通常是有限的。
例如，所有强化学习策略可能会在迷宫游戏中选择相同的最短路径以获得最高累计奖励。
因此，我们提供了两种类型的结果，即绝对推断准确性和相对推断准确性。
前者反映综合两个方面影响的推断效果，而后者反映DRL模型随着累计奖励标准变化时行为的可区分性变化。
相对推断准确性为$\text{acc}_{rel}=\frac{\text{acc} - \text{acc}_{maj}}{\text{acc}_{maj}}$，其中$\text{acc}$是\sysname 的绝对推断准确性，$\text{acc}_{maj}$是majority\_vote方法的准确性。
$\text{acc}_{rel}$显示了相对于基线方法的\sysname 推测准确性的相对增量。
每个子图标题中标注了所使用的状态构成方法。

\mypara{实验结果}
根据\autoref{fig:impact of model reward}，我们可以得出以下观察结果。
1）从绝对的窃取准确度来看，受害者的性能提升趋向于增加超参数泄露的风险。
超参数泄露主要源于模型在较低的累计奖励标准下的行为偏好以及在较高的奖励条件下有限的可行超参数待选集合。
随着累计奖励标准的提高，符合条件的DRL模型超参数会呈现出明显偏向，特别是超参数 gamma。
例如，当累计奖励标准为100时，所有的超参数设置都能够训练出符合要求的PPO模型。
但是，当累计奖励标准提高到500时，绝大多数PPO模型都是采用gamma = 0.99 (71.21\%)或gamma = 0.95 (27.27\%)。
从\autoref{fig:impact of model reward}可得，随着累计奖励标准从100提高到500，大部分超参数种类的相对推断准确性都有所下降。
这意味着随着奖励条件的提高，模型的行为变得更加收敛。
在训练DRL模型时，我们会优化网络权重以接近最优策略。
高奖励条件意味着DRL模型在与环境的互动中具有更强的能力，即更接近最优策略。
因此，它们的策略具有类似的功能，这削弱了模型行为和超参数设置之间的相关性。
2）不同超参数的绝对推断准确度在奖励条件提高时表现出不一致的趋势。
激活函数和优化器的结果相对稳定，其推断准确性在所有奖励条件下均超过75\%。
而隐藏层数、神经元数量和学习率方面的推断准确度则较低。
我们认为激活函数和优化器在策略优化过程中起着更重要的作用，这导致了DRL模型出现明显的行为倾向。
与这些超参数相比，DRL模型对于隐藏层和神经元数量设置的依赖性不强，即在大多数情况下满足拟合策略的复杂度即可。
此外，我们注意到当奖励条件越高时，超参数gamma的推断准确性会提升。
根据\autoref{sec:Reinforcement Learning Problem}，折扣因子$\gamma$影响到DRL模型对未来奖励的关注程度。
因此，高质量的策略倾向于选择更大的gamma以实现更长远的策略视野。
3）主动状态构造机制对奖励条件的变化具有较强的鲁棒性。
相比主动状态构造机制，主动状态构造机制的准确率下降得更慢。
例如，主动状态构造机制的激活函数窃取准确率仍然是基线方法的两倍，而被动状态构造机制与基线方法相同。
由于高质量的DRL模型对于环境的刻画程度更好，所以被动状态构造机制很难使DRL模型的行为出现不一致的情况。
而主动方法调整原始环境状态为较为罕见的环境状态，从而使DRL模型更容易产生行为分歧。

\subsection{超参数之间的耦合影响}
\label{sec:impact of hyper-parameter coupling}
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\hsize]{figure/drl_hypertheif/hyper_couple/hyper-couple-2.pdf}
    \caption{
    不同的超参数组合会影响到模型的准确度。
    其中，“passive\_env”和“active\_env”分别表示被动和主动的状态生成机制。
    每个图的x轴标签表示目标超参数，y轴标签表示耦合超参数。
    % The stealing accuracy with different hyper-parameter pairs. 
    % The ``passive\_env'' and ``active\_env'' illustrate the passive and the active state generation mechanisms. 
    % The x labels of each plot show the target hyper-parameters. 
    % The y labels of each plot show the irrelevant hyper-parameters. 
    }
    \label{fig:hyper_couple}
\end{figure*}

% \begin{table}[!ht]
%     \centering
%     \footnotesize
%     \caption{Acrobot 中阴影模型的超参数分布。}
%     \label{Acrobot-reward500-hyperparameter-ratio}
%     \begin{tabular}{cccc}
%     \toprule
%     \multicolumn{4}{c}{\multirow{2}{*}{\textbf{DQN, Acrobot, Reward=-100}}}  \\
%     \multicolumn{4}{c}{}                                            \\ \toprule
%     activation     & ELU: 61.76     & Tanh: 38.24   & NaN           \\
%     hidden\_layers & 4: 44.12       & 3: 32.35      & 2: 23.53      \\
%     neuron\_number & 256: 47.06     & 128: 32.35    & 64: 20.59     \\
%     optimizer      & SGD: 100.00    & NaN           & NaN           \\
%     learning\_rate & 0.0001: 94.12  & 0.0005: 5.88  & NaN           \\
%     gamma          & 0.9: 35.29     & 0.95: 35.29   & 0.99: 29.41   \\ \toprule
%     \multicolumn{4}{c}{\multirow{2}{*}{\textbf{PG, Acrobot, Reward=-100}}}   \\
%     \multicolumn{4}{c}{}                                            \\ \toprule
%     activation     & ELU: 40.22     & Tanh: 38.04   & ReLU: 21.74   \\
%     hidden\_layers & 2: 38.04       & 4: 31.52      & 3: 30.43      \\
%     neuron\_number & 128: 34.78     & 256: 32.61    & 64: 32.61     \\
%     optimizer      & RMSprop: 58.70 & Adam: 30.43   & SGD: 10.87    \\
%     learning\_rate & 0.0005: 45.65  & 0.001: 31.52  & 0.0001: 22.83 \\
%     gamma          & 0.9: 33.33     & 0.99: 33.33   & 0.95: 33.33   \\ \toprule
%     \multicolumn{4}{c}{\multirow{2}{*}{\textbf{PPO, Acrobot, Reward=-100}}}  \\
%     \multicolumn{4}{c}{}                                            \\ \toprule
%     activation     & Tanh: 39.51    & ELU: 37.74    & ReLU: 22.75   \\
%     hidden\_layers & 3: 34.22       & 2: 32.98      & 4: 32.80      \\
%     neuron\_number & 128: 33.51     & 256: 33.51    & 64: 32.98     \\
%     optimizer      & RMSprop: 35.63 & Adam: 34.92   & SGD: 29.45    \\
%     learning\_rate & 0.001: 35.98   & 0.0005: 35.45 & 0.0001: 28.57 \\
%     gamma          & 0.99: 38.98    & 0.95: 35.63   & 0.9: 25.40    \\ \bottomrule
%     \end{tabular}
% \end{table}

% \begin{table}[!ht]
%     \centering
%     \footnotesize
%     \caption{CartPole 中阴影模型的超参数分布。}
%     \label{cartpole-reward500-hyperparameter-ratio}
%     \begin{tabular}{cccc}
%     \toprule
%     \multicolumn{4}{c}{\multirow{2}{*}{\textbf{DQN, CartPole, Reward=500}}}  \\
%     \multicolumn{4}{c}{}
%     \\ \toprule
%     activation     & Tanh: 48.68    & ELU: 46.05     & ReLU: 5.26   \\
%     hidden\_layers & 2: 38.16       & 4: 36.18       & 3: 25.66     \\
%     neuron\_number & 256: 53.29     & 128: 32.89     & 64: 13.82    \\
%     optimizer      & RMSprop: 67.76 & Adam: 32.24    & NaN          \\
%     learning\_rate & 0.001: 50.00   & 0.0005: 40.79  & 0.0001: 9.21 \\
%     gamma          & 0.99: 36.18    & 0.95: 32.24    & 0.9: 31.58   \\ \toprule
%     \multicolumn{4}{c}{\multirow{2}{*}{\textbf{PG, CartPole, Reward=500}}}   \\
%     \multicolumn{4}{c}{} 
%     \\ \toprule
%     activation     & ELU: 39.77     & Tanh: 37.50    & ReLU: 22.73  \\
%     hidden\_layers & 2: 35.23       & 3: 34.09       & 4: 30.68     \\
%     neuron\_number & 256: 42.05     & 64: 29.55      & 128: 28.41   \\
%     optimizer      & Adam: 51.14    & RMSprop: 48.86 & NaN          \\
%     learning\_rate & 0.001: 52.27   & 0.0005: 42.05  & 0.0001: 5.68 \\
%     gamma          & 0.99: 94.32    & 0.95: 5.68     & NaN          \\ \toprule
%     \multicolumn{4}{c}{\multirow{2}{*}{\textbf{PPO, CartPole, Reward=500}}}   \\
%     \multicolumn{4}{c}{}
%     \\ \toprule
%     activation     & Tanh: 40.91    & ReLU: 33.33    & ELU: 25.76   \\
%     hidden\_layers & 4: 41.67       & 3: 36.36       & 2: 21.97     \\
%     neuron\_number & 256: 35.61     & 128: 34.09     & 64: 30.30    \\
%     optimizer      & RMSprop: 59.85 & Adam: 40.15    & NaN          \\
%     learning\_rate & 0.001: 54.55   & 0.0005: 43.94  & 0.0001: 1.52 \\
%     gamma          & 0.99: 71.21    & 0.95: 27.27    & 0.9: 1.52    \\ \bottomrule
%     \end{tabular}
% \end{table}
\begin{table}[!ht]
    \centering
    \footnotesize
    \caption{Acrobot 中阴影模型的超参数分布。}
    \label{Acrobot-reward0100-hyperparameter-ratio}
    \begin{tabular}{cccc}
    \toprule
    \multicolumn{4}{c}{\multirow{2}{*}{\textbf{DQN, Acrobot, Reward=-100}}}  \\
    \multicolumn{4}{c}{}                                            \\ \toprule
    激活函数     & ELU: 61.76     & Tanh: 38.24   & NaN           \\
    隐含层数 & 4: 44.12       & 3: 32.35      & 2: 23.53      \\
    神经元数 & 256: 47.06     & 128: 32.35    & 64: 20.59     \\
    优化器      & SGD: 100.00    & NaN           & NaN           \\
    学习率 & 0.0001: 94.12  & 0.0005: 5.88  & NaN           \\
    gamma          & 0.9: 35.29     & 0.95: 35.29   & 0.99: 29.41   \\ \toprule
    \multicolumn{4}{c}{\multirow{2}{*}{\textbf{PG, Acrobot, Reward=-100}}}   \\
    \multicolumn{4}{c}{}                                            \\ \toprule
    激活函数     & ELU: 40.22     & Tanh: 38.04   & ReLU: 21.74   \\
    隐含层数 & 2: 38.04       & 4: 31.52      & 3: 30.43      \\
    神经元数 & 128: 34.78     & 256: 32.61    & 64: 32.61     \\
    优化器      & RMSprop: 58.70 & Adam: 30.43   & SGD: 10.87    \\
    学习率 & 0.0005: 45.65  & 0.001: 31.52  & 0.0001: 22.83 \\
    gamma          & 0.9: 33.33     & 0.99: 33.33   & 0.95: 33.33   \\ \toprule
    \multicolumn{4}{c}{\multirow{2}{*}{\textbf{PPO, Acrobot, Reward=-100}}}  \\
    \multicolumn{4}{c}{}                                            \\ \toprule
    激活函数     & Tanh: 39.51    & ELU: 37.74    & ReLU: 22.75   \\
    隐含层数 & 3: 34.22       & 2: 32.98      & 4: 32.80      \\
    神经元数 & 128: 33.51     & 256: 33.51    & 64: 32.98     \\
    优化器      & RMSprop: 35.63 & Adam: 34.92   & SGD: 29.45    \\
    学习率 & 0.001: 35.98   & 0.0005: 35.45 & 0.0001: 28.57 \\
    gamma          & 0.99: 38.98    & 0.95: 35.63   & 0.9: 25.40    \\ \bottomrule
    \end{tabular}
\end{table}

\begin{table}[!ht]
    \centering
    \footnotesize
    \caption{CartPole 中阴影模型的超参数分布。}
    \label{cartpole-reward500-hyperparameter-ratio}
    \begin{tabular}{cccc}
    \toprule
    \multicolumn{4}{c}{\multirow{2}{*}{\textbf{DQN, CartPole, Reward=500}}}  \\
    \multicolumn{4}{c}{}
    \\ \toprule
    激活函数     & Tanh: 48.68    & ELU: 46.05     & ReLU: 5.26   \\
    隐含层数 & 2: 38.16       & 4: 36.18       & 3: 25.66     \\
    神经元数 & 256: 53.29     & 128: 32.89     & 64: 13.82    \\
    优化器      & RMSprop: 67.76 & Adam: 32.24    & NaN          \\
    学习率 & 0.001: 50.00   & 0.0005: 40.79  & 0.0001: 9.21 \\
    gamma          & 0.99: 36.18    & 0.95: 32.24    & 0.9: 31.58   \\ \toprule
    \multicolumn{4}{c}{\multirow{2}{*}{\textbf{PG, CartPole, Reward=500}}}   \\
    \multicolumn{4}{c}{} 
    \\ \toprule
    激活函数     & ELU: 39.77     & Tanh: 37.50    & ReLU: 22.73  \\
    隐含层数 & 2: 35.23       & 3: 34.09       & 4: 30.68     \\
    神经元数 & 256: 42.05     & 64: 29.55      & 128: 28.41   \\
    优化器      & Adam: 51.14    & RMSprop: 48.86 & NaN          \\
    学习率 & 0.001: 52.27   & 0.0005: 42.05  & 0.0001: 5.68 \\
    gamma          & 0.99: 94.32    & 0.95: 5.68     & NaN          \\ \toprule
    \multicolumn{4}{c}{\multirow{2}{*}{\textbf{PPO, CartPole, Reward=500}}}   \\
    \multicolumn{4}{c}{}
    \\ \toprule
    激活函数     & Tanh: 40.91    & ReLU: 33.33    & ELU: 25.76   \\
    隐含层数 & 4: 41.67       & 3: 36.36       & 2: 21.97     \\
    神经元数 & 256: 35.61     & 128: 34.09     & 64: 30.30    \\
    优化器      & RMSprop: 59.85 & Adam: 40.15    & NaN          \\
    学习率 & 0.001: 54.55   & 0.0005: 43.94  & 0.0001: 1.52 \\
    gamma          & 0.99: 71.21    & 0.95: 27.27    & 0.9: 1.52    \\ \bottomrule
    \end{tabular}
\end{table}

\mypara{实验设置}
在训练攻击模型以推断某个超参数时，其他超参数可能会对目标超参数产生影响。
为了进一步探究超参数两两之间的影响，我们将一个参数设置为目标超参数，选取另一个作为耦合超参数。
在每张子图的对角线，我们展示了不包含耦合超参数的推断结果。
如\autoref{Acrobot-reward0100-hyperparameter-ratio}和\autoref{cartpole-reward500-hyperparameter-ratio}所示，训练得到的DRL模型的超参数分布是不均匀的。
这意味着某些超参数设置下没有满足累计奖励标准的模型。

\mypara{实验结果}
从\autoref{fig:hyper_couple}中，我们可以得出以下结论。
1）超参数对模型行为偏好的影响程度不同。
例如，当优化器是目标超参数时（对应着x轴上的“opti”），推断精度一般是优于其他超参数作为目标参数的情况。
因此，优化器对 DRL 模型的行为影响比其他超参数更强。
此外，超参数 gamma（即x轴上的“gamm”）对三个 DRL 模型具有不同的影响特性。
与 DQN 模型相比，PG 和 PPO 模型的动作更多地暴露了有关 gamma 设置的信息。
回顾\autoref{sec:Deep Reinforcement Learning model}，DQN 模型是基于值的RL模型，这意味着 DQN 模型通过比较下一状态的期望累计奖励来选择动作。
而PG 和 PPO 模型直接利用期望累计奖励来更新策略网络，所以模型更新过程中更加依赖累计奖励的数值。
因此，gamma 设置对 PG 和 PPO 模型有更显著的影响。 
2）主动状态构造方法在一定程度上可以解耦不同超参数之间的影响，所以主动状态构造方法的性能比被动状态构造方法更优秀。
例如，当学习率是目标超参数时，由于其他耦合超参数的影响，使用被动状态构造方法的推断准确率约为60\%。
但是，使用主动状态构造方法时，相应的结果可以达到约75\%（甚至100\%）。





\subsection{消融实验}
\label{sec:ablation study}
\begin{figure*}[!t]
    \centering        
    \includegraphics[width=\hsize]{figure/drl_hypertheif/sequence_length/sequence_length-2.pdf}
    \caption{不同状态-动作序列长度下推断准确率的变化。
    每张图的x轴代表用于训练攻击模型的状态-动作序列的长度，y轴显示了相应的推断准确率。
    }
    \label{fig:ablation the length of state-action sequence}
    % \vspace{-0.3cm}
\end{figure*}
\begin{figure*}[!t]
    \centering
    \includegraphics[width=\hsize]{figure/drl_hypertheif/shadow_agent_amount/shadow_agent_amount-2.pdf}
    \caption{不同影子模型数量下推断准确率的变化。每张图的x轴代表用于训练攻击模型的影子模型数量，y轴显示了相应的窃取准确率。
    % \mc{The legend labels are not consistent with \autoref{fig:overall performence}.}
    }
    \label{fig:ablation the amount of shadow models}
\end{figure*}
\mypara{序列长度的影响}
\sysname 在序列长度从短（100步）到长（500步）条件下来进行推断攻击，并比较不同状态生成机制的推断效果。
根据\autoref{fig:ablation the length of state-action sequence}，我们得出以下观察结果。
1）较长的状态-动作序列有利于窃取攻击。
在\autoref{fig:ablation the length of state-action sequence}中，随着序列长度的增加，推测准确率往往会上升。
尤其是在CartPole环境中，对于PPO模型的激活函数的推测准确率从53\%上升到70\%。
两种攻击方法都使用DRL模型和环境交互过程的状态-动作数据。
因此更长的序列可能包含更多的行为分歧，有助于进行推断攻击。
2）状态-动作序列的长度对主动状态构造机制的影响较小。
主动状态构造机制的推断准确率几乎不随状态-动作序列长度的变化而变化。
由于优化后的状态序列中的每个状态更有可能使模型的动作不一致，因此主动状态构造机制的基础性能优于被动机制。
在主动状态构造机制中，使用100步状态-动作序列就足以取得和被动机制使用500步状态-动作序列相当的效果。

\mypara{影子模型数量的影响}
我们将训练攻击模型所使用的每个超参数设置下的影子模型数量从4个增加到16个。
根据\autoref{fig:ablation the amount of shadow models}，我们得出以下观察结果。
1）随着影子模型数量的增加，窃取准确率会增加。
在训练影子模型时，环境的初始状态和模型的探索过程会给模型的策略带来一定的随机性。
因此，攻击模型可以通过使用来自更加多样化的影子模型的状态-动作序列来降低随机性对于超参数推断的影响。
2）主动攻击方法更有效地利用了影子模型。
回顾\autoref{sec:attack state generation}，主动状态生成方法使用攻击模型的损失来优化状态序列，从而可以刺激DRL模型表现出更多的行为分歧。
即使在影子模型数量有限的情况下（例如只有4个影子模型），主动方法仍然可以比被动方法实现更高的超参数推断准确率。

\subsection{案例分析}
\label{sec:case study}
\subsubsection{对抗样本攻击的可迁移性}
泄漏的超参数信息使DRL模型在面对其他攻击时更加脆弱。
例如，模型提取攻击~\cite{TZJRR16}和对抗样本攻击~\cite{HPGDA17, BM17, LHLSLS17, KS17}。
% 对于模型提取攻击，Chen等人~\cite{CGZXL21}提出了第一种方法来窃取黑盒受害者DRL的功能，并确认了超参数和网络结构对识别算法族准确性的影响。
在本节中，我们展示了泄漏的超参数信息对于对抗样本攻击的可迁移性的增益效果。
实验设置与文献{\rm\parencite{CGZXL21}}保持一致，并利用GAIL~\cite{DBLP:conf/nips/HoE16}生成影子模型。
% 因此，以下实验可以用作比较\cite{CGZXL21}和\sysname之间转移增强的实验。

\mypara{实验设置}
我们随机选择了20个具有不同超参数设置的受害者模型，并分别生成了两组具有和不具有超参数信息的$20 \times 8$个影子模型。
对于每个白盒影子模型，我们生成一条长度为500步的对抗状态序列，并将对抗状态序列应用于受害者模型，并记录攻击成功率（对抗状态序列的可迁移性）。
在每个实验设置下，我们重复10次实验，并计算每个影子模型产生的对抗状态序列可迁移性的平均值。
我们展示出所有$20 \times 8$个影子模型中平均可迁移值超过90\%的影子智能体的百分比。
实验使用四种不同的对抗样本生成方法，即FGM、FGSM \cite{DBLP:journals/corr/GoodfellowSS14}、L2PGD和LinfPGD\cite{DBLP:conf/iclr/KurakinGB17a, DBLP:conf/iclr/MadryMSTV18}。

\begin{table}[!t]
    \centering
    \caption{影子模型集合中的平均可转移值超过90\%的模型比例，结果基于 PPO模型 和 CartPole 环境。
        % The percentage of shadow agents with average transferability values more than 90\%. 
    % The results are collected from PPO and the CartPole environment.
    % (\textbf{reward = 300}). 
    % In each table, the `W' and `W/O' illustrate the transferability experiment with or without the hyper-parameters information, and $\epsilon$ controls the strength of adversarial perturbation. 
    }
    \label{tab:adv-transferability-ppo-cartPole-reward300}
    % \resizebox{\textwidth}{5mm}{
    \resizebox{0.7\linewidth}{!}{
    \setlength{\tabcolsep}{0.8em}
    \begin{tabular}{c|cc|cc|cc|cc} 
    \toprule
      \multicolumn{9}{c}{\textbf{Reward=300}} \\
      \toprule
      \multicolumn{1}{c|}{$\epsilon$} &
      \multicolumn{2}{c|}{FGM} & \multicolumn{2}{c|}{L2PGD} & \multicolumn{2}{c|}{FGSM} & \multicolumn{2}{c}{LinfPGD}  \\ 
      & w. & w/o & w. & w/o & w. & w/o & w. & w/o \\
    \midrule
    1e-4 & 20.62 & 15.62            & 13.12 & 13.75              & 24.38 & 16.88       & 19.38 & 15.00                   \\
    1e-3 & 51.25 & 43.75            & 50.62 & 43.75              & 51.88 & 44.38             & 50.62 & 44.38                \\
    5e-3 & 51.88 & 44.38            & 51.25 & 44.38              & 51.88 & 44.38             & 51.25 & 44.38                \\
    1e-2 & 51.88 & 44.38            & 51.25 & 44.38              & 51.88 & 44.38             & 51.25 & 44.38                \\
    1e-1 & 51.88 & 44.38            & 46.88 & 41.88              & 51.88 & 44.38             & 50.00    & 42.50                 \\
    \toprule
      \multicolumn{9}{c}{\textbf{Reward=500}} \\
      \toprule
      \multicolumn{1}{c|}{$\epsilon$} &
      \multicolumn{2}{c|}{FGM} & \multicolumn{2}{c|}{L2PGD} & \multicolumn{2}{c|}{FGSM} & \multicolumn{2}{c}{LinfPGD}  \\ 
      & w. & w/o & w. & w/o & w. & w/o & w. & w/o \\
    \midrule
    1e-4 & 10.00    & 9.38             & 5.62  & 5.00                  & 11.88 & 11.25             & 9.38  & 8.75                 \\
    1e-3 & 58.13 & 37.50             & 56.25 & 36.88              & 60.00    & 38.12             & 58.75 & 38.12                \\
    5e-3 & 61.25 & 39.38            & 61.25 & 40.00                 & 61.25 & 39.38             & 61.25 & 39.38                \\
    1e-2 & 61.25 & 39.38            & 60.62 & 39.38              & 61.25 & 40.00                & 60.62 & 39.38                \\
    1e-1 & 61.25 & 40.00               & 58.75 & 37.50               & 61.25 & 40.00                & 58.75 & 37.50                 \\
    \bottomrule
    \end{tabular}
    }
\end{table}

\mypara{实验结果}
\autoref{tab:adv-transferability-ppo-cartPole-reward300} 显示了具有高对抗样本攻击可迁移性的阴影模型在整个影子模型中的比例。
我们设置了五个不同的 $\epsilon$ 值来控制对抗攻击的强度。
“W” 和 “W/O” 表示带或不带超参数信息的对抗样本攻击可迁移性实验。
从结果中，我们得到以下结论：
1）所提出的超参数窃取攻击在下游攻击中带来了显著的改进。当阴影模型与受害者模型使用相同的超参数设置时，它们的决策边界在训练过程中趋于封闭。因此，在大多数情况下，对抗性可转移性提高了超过15%。

2）与\autoref{tab:adv-transferability-ppo-cartPole-reward300}中的结果相比，在累计奖励标准较高的DRL模型中，可转移性增益更为明显，这与\autoref{sec:impact of model reward}中的分析一致。
我们推测，具有更高累计奖励标准的模型具有更趋近的决策边界，并且超参数对于其决策边界的形成具有更大的影响。

3）随着 $\epsilon$ 的增加，可转移性值会饱和。
与DRL模型交互的环境具有其物理约束。
例如，“CartPole”环境的状态由四个物理变量定义，即小车位置、小车速度、杆子角度和杆子角速度，范围从 $-[2.4, \infty, 12~\text{deg}, \infty]$ 到 $+[2.4, \infty, 12~\text{deg}, \infty]$。为满足物理约束，当对抗状态的某个数值超出环境约束时，对抗状态的生成立刻停止。
因此，对抗样本攻击可迁移性并不一直随着更大的 $\epsilon$ 值而上升。

\subsubsection{更复杂的环境}
在本节中，我们展示了\sysname 在一个更为复杂的环境下的有效性，即“LunarLander”环境。 
LunarLander 是一个经典的火箭轨迹优化问题，任务是使飞船在信标点之间平稳降落。
该环境的状态是一个 8 维向量，比 CartPole (4 维) 和 Acrobot (6 维) 更复杂，其动作空间也比前面的环境更大。
其他实验设置与\autoref{sec:evaluation}中的一致。本节主要展示了 PPO 模型的结果，根据实验结果和\autoref{sec:evaluation}中的观察结果，\sysname 在 DQN 和 PG 上的表现也类似。

从\autoref{fig:more complex environment}中，我们得出以下实验结果。
1）\sysname 的推断准确率均高于基线方法，这意味着 DRL 模型的行为确实可以暴露出超参数设置信息。
2）\sysname 在更为复杂的环境中也能够有效。激活函数、优化器和 gamma 的推断准确率均超过 80\%，这意味着即使环境的状态和动作空间增加，\sysname 仍然有效。

\begin{figure}[!ht]
    \centering
    \subfigure{\includegraphics[width=0.8\hsize]{figure/drl_hypertheif/overall_acc/ppo-lunarlander-reward90-20221008.pdf}}
    \\[-3ex]
    \subfigure{\includegraphics[width=0.6\textwidth]{figure/drl_hypertheif/overall/overall-legend-20220731.pdf}} 
    \caption{不同状态构造方法在 PPO 模型和 LunarLander 环境上的推断准确率。
        % The stealing accuracy of different methods on PPO and the LunarLander environment (Reward = 90).
        } 
    % \mc{I suggest using a shallow grid color to observe the different actions easily.}}
    \label{fig:more complex environment}
\end{figure}


\subsection{防御方法}
\label{sec:possible defense}
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\hsize]{figure/drl_hypertheif/defense/defense-1.pdf}
    \label{fig:PPO CartPole defense}
    \caption{不同方法在部署防御方法后推断准确性的变化。在每个子图中，“baseline”表示“majority\_vote”基准算法，“defence”意味着攻击模型仅能获得预测的动作，不能访问模型动作置信度。
    % The methods named with ``active" adopt the active state generation mechanisms. 
    % The x labels of each plot correspond in turn to the hyper-parameters activation function, hidden layers, neuron number, optimizer, learning rate, and gamma. 
    % The y labels show the stealing accuracy. 
    }
    \label{fig:possible defense}
\end{figure*}
在本节中，DRL模型仅输出其预测的具体动作而省略其动作置信度值（后验概率），以减少攻击模型可访问的信息。
\autoref{fig:possible defense} 展现了有无防御方法的对于\sysname 推断效果的影响。
当仅使用模型动作时，\sysname 的窃取准确率会降低，但是仍优于基线方法。
例如，对于PPO模型的优化器超参数推断中，主动状态生成方法推断准确率仍比基线高出15\%。

\subsection{结论}
\label{sec:takeaways}
通过归纳上述实验评估结果，我们回答\autoref{sec:evaluation}中的问题。
\begin{itemize}
    \item 在所有评估的场景中，\sysname 的推断准确性都高于基准方法，这表明DRL模型的行为确实可以暴露DRL模型超参数设置（见\autoref{sec:overall performance}）。
    \item 主动状态构造机制进一步利用了阴影模型的权重和梯度信息，从而获得了更好的窃取准确性（见\autoref{sec:evaluation}）。
    \item 具有获得更高累计奖励的DRL模型面临着更高的超参数泄漏风险，\sysname 也会更容易窃取成功（见\autoref{sec:impact of model reward}）。
    不同的超参数对DRL模型的行为有不同的影响强度（见\autoref{sec:impact of hyper-parameter coupling}）。
    \sysname 推断准确性受益于更长的状态-动作序列或更多的阴影模型（见\autoref{sec:ablation study}）。
    \item 仅能获取受害者模型的动作信息时，\sysname 的窃取准确性会降低，但仍然优于基准方法（见\autoref{sec:possible defense}）。
\end{itemize}

\section{本章小结}
在本节中，我们提出了一种新的超参数推断方法用来评估DRL模型的超参数泄漏风险。
具体来说，我们利用受害者模型的状态-动作数据对DRL模型的超参数进行推断攻击。
依据实验评估结果，\sysname 在不同的DRL模型和环境组合下实现了良好的超参数推断性能。
另外，通过分析不同实验设置下的算法性能，我们总结了几个重要的实验结论，以便在实践中部署 \sysname。
DRL模型性能和隐私风险之间的矛盾提醒RL从业者在追求高质量DRL模型时应注意超参数泄漏的风险。
最后，我们利用对抗样本的可转移性实验，佐证了所提方法对其他攻击方法的增益效果，并展示了在具有更大状态和环境动作空间的场景下，\sysname 仍然有效。


\chapter{总结与展望}
本章总结全文的主要工作和成果，并展望未来的研究工作。
\section{全文总结}
近年来，全球数字经济蓬勃发展，数据交易市场规模持续增长。
为了进一步推动公共数据、企业数据、个人数据合规高效流通使用，国务院于2022年底发布了《中共中央国务院关于构建数据基础制度更好发挥数据要素作用的意见》（以下简称“数据二十条”）。
另一方面，数据流通要以安全保护为前提。“数据二十条”明确提出，推动个人信息匿名化处理，保障使用个人信息数据时的信息安全和个人隐私。
，鼓励公共数据在保护个人隐私和确保公共安全的前提下，按照“原始数据不出域、数据可用不可见”的要求，以模型、核验等产品和服务等形式向社会提供。
本文针对数据流通场景，归纳出三种主要的数据存在形式，并分别进行数据安全流通技术的研究。
具体来说，对于用户侧产生、记录、相对分散的原始数据，本文主要关注个人数据的保隐私分析问题。
对于经过清洗、预处理后的高质量数据集，本文主要考虑数据集流通的产权审计问题。
对于经过分析、处理、加工后的数据模型，本文主要关注模型部署过程中的超参数泄露风险评估。
但是，现有研究工作中的高维数据的保隐私分析融合算法可用性比较低、高维场景下计算复杂度高；
标准化数据集产权保护方法往往需要向数据集中注入水印数据，影响数据集的可用性且易被检测；
模型超参数泄露风险评估工作主要考虑基于侧信道信息的手段，缺乏适用于可信执行环境的基于查询方法的技术研究；
另外，现有工作主要面向监督学习和非监督学习范式，缺乏对强化学习模型数据安全问题的研究。
本文针对现有工作中存在的不足进行了探索和改进，主要工作和贡献总结如下：
\begin{itemize}
    \item 研究了原始数据保隐私分析关键技术——高维数据范围查询——的聚合算法优化问题。本文提出的\myahead 方法，首先通过动态构建树结构，解决了现有的本地差分隐私方法的局限性，显著提高了聚合结果的准确性；接着，为了找到最优的数据域划分方式，\myahead 利用树的分支数 $B$ 和阈值 $\theta$ 进行数据域的分解，本文在严格满足LDP下推导这两个超参数的设置依据，并通过低维和高维数据集上的实验结果，证明了参数设置的合理性；最后通过对 \myahead 在各种隐私预算、域大小、用户规模、分布偏斜度、数据维度和属性相关性的深入分析，我们总结了一些有助于\myahead 在实际部署的观察结论，给未来改进方法提供依据。
    % 然后为了克服寻找最优分区的障碍\cite{muthukrishnan1999rectangular}，
    \item 研究了强化学习场景下的标准化数据集产权保护问题。本文提出的\sysnameo 是首个用于离线强化学习模型轨迹级数据集审计的方法。本文通过在不同实验设置下对 \sysnameo 进行全面分析，例如阴影模型数量、假设检验的显著性水平、轨迹大小以及对集成架构和动作扭曲的鲁棒性等，总结出一系列有用的观察结论，为实际中部署 \sysnameo 提供指导。另外，本文采用 \sysnameo 对来自Google和DeepMind的开源数据集训练的模型进行审计，所有真阳性和真阴性结果均优于95\%，证明 \sysnameo 在实际场景中具有较好的审计精度，是一种有效且高效的数据集产权审计方法。
    \item 研究了强化学习场景下的模型超参数泄露风险评估问题。本文提出的 \sysname 是首个基于查询的推断深度强化学习超参数设置的方法，揭示了当前深度强化学习模型的数据安全问题，并可以推动未来模型超参数保护机制的发展。本文为了实现更强的超参数推断能力，进一步提出利用攻击模型的损失来优化状态序列，优化后的状态序列在所有实验设置下推断性能都取得显著提升。最后，本文对\sysname 在不同实验因素下进行深入分析，总结了一系列辅助方法实际部署的观察结论。
    % 然后为了实现更高效的下游攻击\cite{CGZXL21}，
\end{itemize}


\section{研究展望}
本文针对流通场景中的数据安全问题进行了探索，并解决了现有研究工作中存在的不足。
但是，由于数据主体、采集数据类型、模型应用场景、模型架构等元素的多样性和复杂性，
本文内容无法涵盖所有研究面，仍存在一些重要问题需要后续工作解决，
我们列举了一些关键但具有挑战性的未来研究方向：
% 1. 无精度损失的保隐私数据分析方法
% 2. 完善数据集产权追溯机制
% 3. 模型内部信息防护机制
\begin{itemize}
    \item 无精度损失的保隐私数据分析方法：本文提出的保隐私数据分析方法基于本地差分隐私机制，适合大规模用户原始数据保隐私聚合分析场景。
    但是，本地差分隐私通过扰动方式实现保护用户敏感数据的目的，不可避免地对数据可用性造成破坏，特别是数据量或者是隐私预算较少的情况下。
    在一些对于数据可用性要求较高同时需要保护用户隐私的场景，例如医疗、交通、借贷等，需要实现无精度损失的保隐私数据分析，基于密码学方法可能更适用。
    因此，未来研究可以探索支持大规模用户且计算代价低的基于密码学的保隐私数据分析方法。
    \item 完善数据集产权追溯机制：本文考虑了一种基于数据集特征和模型行为联系的标准数据集产权审计方法。该方法通过提取可疑模型行为特征并于数据集特征进行比较，从而判定可疑模型是否侵犯了数据集产权。
    但是，在使用标准化数据集时，一些潜在的数据增强方法可能会改变数据集特征，从而降低产权审计方法的精度。另外，本文考虑的产权审计方法是一种事后检测方法，即可疑模型可能已经盗用了标准化数据集，不能预防产权侵犯行为。
    因此，未来研究可以结合现有商用软件产品密钥的思路，考虑在标准化数据产生的过程中提取数据集的“密钥”，数据集只有在结合对应密钥的情况下才可以被利用，从而实现通过授权密钥的方式分发数据集。
    % 1) 从\autoref{sec:impact of significance level}中可以看出，当显著性水平下降到0.001时，\sysnameo 的准确性会下降。因此，有意思的是在未来改进 \sysnameo，以满足更严格的审计需求。2) 基于单一距离度量的 \sysnameo 对于较大的扭曲可能不够鲁棒。根据\autoref{sec:perturbing models output}中的观察，将更多距离度量融入审计过程可能是进一步的有希望的方向。
    \item 模型内部信息防护机制：本文考虑了在可信执行环境下进行模型超参数泄露风险评估，揭露了模型行为可能暴露其超参数配置的事实，通过优化查询输入可以在相同条件下获得更高的超参数推断能力。但是，所提方法仅适用于模型部署前的风险评估，不能防止实际部署中攻击者通过查询获取模型内部信息。因此，未来研究可以考虑结合强化学习模型的序列决策特点，在模型性能（获得累计奖励）保持不变的情况下，通过适当调整模型输出扰乱攻击者获取的信息，从而抵御内部信息被攻击者推断。
    %  1) 对于一些超参数，如神经元数量和 gamma 值，与基准方法相比，\sysname 的准确性提升有限。由于\sysname 是一种基于查询的模型提取方法，即从查询输入和输出中窃取内部模型信息，提取的特征可能包含大量无用信息，例如，所有模型在某些状态下的行为相同。因此，当超参数的影响对模型行为间接时，基于侧信道的提取方法，即收集用于推断超参数的侧信道信息，更适用于这些超参数。
    % 2) 本工作主要关注由线性层构建的策略网络，这些线性层是复杂策略的基本组成部分。对于其他架构和优化超参数，例如卷积层、最大池化和批量大小，\sysname 的思想可能会被应用于这些超参数的扩展。因此，将\sysname 扩展到更多DRL策略网络的不同超参数是一个有趣的未来工作。
    % 3) 受害模型可能存在更多的超参数组合。可以保证高质量DRL模型的超参数的值范围是有限的。从\cite{MBMGLHSK16}可知，一种最先进的DRL模型——异步优势演员-评论家（A3C）模型，以学习速率约为$10^{-3}$的数值获得更好的性能和训练效率。在实践中，我们可以从更大的超参数组合范围开始，并寻找一组能够训练出与受害模型相似性能的超参数组合范围，从而缩小训练影子模型的可能超参数组合空间。此外，可以根据攻击模型的后验概率确定受害模型超参数是否在影子模型集合中，并缩小潜在超参数集合的范围。
    % 4) 状态-动作序列的长度和影子模型的数量都是攻击者设定的参数。由于主动方法的高效性，攻击者可以设计一种参数选择策略来降低成本。回顾\autoref{sec:ablation study}，由于主动方法的高效性，攻击者还可以设计参数选择策略来进一步降低成本。
\end{itemize}


% \begin{itemize}
%     \item \textbf{原始数据保隐私分析技术。} 1) 对于高维（>2）范围查询，\myahead 对用户记录的规模非常敏感。 \myahead 需要将用户分割两次，即按不同属性组合和2维树中的不同层次进行分区。因此，当用户组的数量很大时，\myahead 需要足够的用户记录以确保每个2维层的频率估计准确性。2) 与只使用两个网格（即更细粒度的1维和更稀疏的2维网格）的 \myHDG 相比，\myahead 生成具有不同粒度的2维间隔来分解整个域。因此，在实践中，与 \myHDG 相比，\myahead 需要更长的频率值搜索时间和更大的内存使用量。可以为每个2维 \myahead 树设计一种层融合策略以压缩树的高度。3) \myahead 是一个完全动态的框架，它使用阈值来确定每个子域的划分。在域大小较大的情况下，每个子域都需要与阈值进行比较，这不可避免地增加了计算开销。由于较高级别的节点表示相对较大的子域（其频率值通常大于阈值），可以潜在地采用“静态”和“动态”混合树结构。例如，顶部几层可以使用静态框架，而剩余的层可以根据动态框架利用阈值来分解子域。
%     \item \textbf{标准化数据集产权保护技术。} 1) 从\autoref{sec:impact of significance level}中可以看出，当显著性水平下降到0.001时，\sysnameo 的准确性会下降。因此，有意思的是在未来改进 \sysnameo，以满足更严格的审计需求。2) 基于单一距离度量的 \sysnameo 对于较大的扭曲可能不够鲁棒。根据\autoref{sec:perturbing models output}中的观察，将更多距离度量融入审计过程可能是进一步的有希望的方向。
%     \item \textbf{数据模型超参数泄露风险评估方法。} 1) 对于一些超参数，如神经元数量和 gamma 值，与基准方法相比，\sysname 的准确性提升有限。由于\sysname 是一种基于查询的模型提取方法，即从查询输入和输出中窃取内部模型信息，提取的特征可能包含大量无用信息，例如，所有模型在某些状态下的行为相同。因此，当超参数的影响对模型行为间接时，基于侧信道的提取方法，即收集用于推断超参数的侧信道信息，更适用于这些超参数。
%     2) 本工作主要关注由线性层构建的策略网络，这些线性层是复杂策略的基本组成部分。对于其他架构和优化超参数，例如卷积层、最大池化和批量大小，\sysname 的思想可能会被应用于这些超参数的扩展。因此，将\sysname 扩展到更多DRL策略网络的不同超参数是一个有趣的未来工作。
%     3) 受害模型可能存在更多的超参数组合。可以保证高质量DRL模型的超参数的值范围是有限的。从\cite{MBMGLHSK16}可知，一种最先进的DRL模型——异步优势演员-评论家（A3C）模型，以学习速率约为$10^{-3}$的数值获得更好的性能和训练效率。在实践中，我们可以从更大的超参数组合范围开始，并寻找一组能够训练出与受害模型相似性能的超参数组合范围，从而缩小训练影子模型的可能超参数组合空间。此外，可以根据攻击模型的后验概率确定受害模型超参数是否在影子模型集合中，并缩小潜在超参数集合的范围。
%     4) 状态-动作序列的长度和影子模型的数量都是攻击者设定的参数。由于主动方法的高效性，攻击者可以设计一种参数选择策略来降低成本。回顾\autoref{sec:ablation study}，由于主动方法的高效性，攻击者还可以设计参数选择策略来进一步降低成本。
% \end{itemize}

